# MÃ³dulo 02: Arquitectura del Cluster Kubernetes

## Tabla de Contenidos

1. [IntroducciÃ³n al MÃ³dulo](#introducciÃ³n-al-mÃ³dulo)
2. [VisiÃ³n General de la Arquitectura](#1-visiÃ³n-general-de-la-arquitectura)
3. [Control Plane - El Cerebro del Cluster](#2-control-plane---el-cerebro-del-cluster)
4. [Worker Nodes - Donde Corren las Aplicaciones](#3-worker-nodes---donde-corren-las-aplicaciones)
5. [ComunicaciÃ³n entre Componentes](#4-comunicaciÃ³n-entre-componentes)
6. [Alta Disponibilidad y Clustering](#5-alta-disponibilidad-y-clustering)
7. [Addons del Cluster](#6-addons-del-cluster)
8. [Conclusiones y PrÃ³ximos Pasos](#conclusiones-y-prÃ³ximos-pasos)

---

## IntroducciÃ³n al MÃ³dulo

En el MÃ³dulo 01 conociste quÃ© es Kubernetes y por quÃ© es fundamental en la infraestructura moderna. Ahora es momento de profundizar en **cÃ³mo funciona internamente** un cluster de Kubernetes.

### Objetivos de Aprendizaje

Al completar este mÃ³dulo, serÃ¡s capaz de:
- âœ… Identificar y explicar la funciÃ³n de cada componente del Control Plane
- âœ… Comprender el rol de los componentes en Worker Nodes
- âœ… Describir cÃ³mo se comunican los diferentes componentes
- âœ… Diagnosticar problemas bÃ¡sicos de arquitectura
- âœ… Entender los flujos de creaciÃ³n y gestiÃ³n de recursos
- âœ… Configurar componentes para alta disponibilidad

### Prerequisitos

Para este mÃ³dulo necesitas:
- Haber completado el MÃ³dulo 01 (IntroducciÃ³n a Kubernetes)
- **VM de Azure con Minikube instalado** (usando driver Docker)
- `kubectl` instalado y configurado
- Conocimientos bÃ¡sicos de redes y sistemas distribuidos

### Entorno de Trabajo

**IMPORTANTE**: En este curso trabajaremos exclusivamente con:
- âœ… **Minikube** como plataforma de Kubernetes
- âœ… **Driver Docker** para los contenedores
- âœ… **VM Ubuntu en Azure** como infraestructura base
- âŒ **NO** haremos instalaciÃ³n manual de clusters multi-nodo
- âŒ **NO** usaremos kubeadm o configuraciones bare-metal

**JustificaciÃ³n**: Minikube proporciona un entorno completo de Kubernetes ideal para aprendizaje, permitiÃ©ndonos explorar todos los componentes de la arquitectura sin la complejidad operativa de un cluster multi-nodo de producciÃ³n.

### DuraciÃ³n Estimada

- **Lectura teÃ³rica**: 45-60 minutos
- **Ejemplos prÃ¡cticos**: 30-45 minutos
- **Laboratorios**: 90-120 minutos

### Por QuÃ© es Importante Este MÃ³dulo

Entender la arquitectura de Kubernetes es fundamental porque:

1. **Troubleshooting efectivo**: Cuando algo falla, sabrÃ¡s exactamente dÃ³nde buscar
2. **OptimizaciÃ³n de recursos**: ComprenderÃ¡s cÃ³mo Kubernetes toma decisiones de scheduling
3. **Seguridad**: ConocerÃ¡s los puntos crÃ­ticos que necesitan protecciÃ³n
4. **Alta disponibilidad**: SabrÃ¡s cÃ³mo diseÃ±ar clusters resilientes
5. **Fundamento sÃ³lido**: Es la base para todos los mÃ³dulos siguientes

---

## 1. VisiÃ³n General de la Arquitectura

### El Modelo Cliente-Servidor Distribuido

Kubernetes sigue una arquitectura cliente-servidor distribuida donde mÃºltiples componentes trabajan coordinadamente para gestionar el estado del cluster. A diferencia de sistemas monolÃ­ticos, Kubernetes separa claramente las responsabilidades entre diferentes procesos especializados.

**Principio fundamental**: Kubernetes opera bajo el paradigma de **"Estado Deseado"** (Desired State). TÃº declaras cÃ³mo quieres que se vea tu aplicaciÃ³n, y Kubernetes trabaja continuamente para hacer que la realidad coincida con tu declaraciÃ³n.

### Los Dos Planos Principales

Un cluster de Kubernetes se divide en dos planos fundamentales:

**1. Control Plane (Plano de Control)** - El Cerebro
- Toma decisiones globales sobre el cluster
- Detecta y responde a eventos del cluster
- Generalmente corre en nodos dedicados (masters)
- Es el "quÃ©" y "cuÃ¡ndo" del cluster

**2. Data Plane / Worker Nodes (Plano de Datos)** - Los Ejecutores  
- Ejecuta las cargas de trabajo (aplicaciones)
- Mantiene los pods en ejecuciÃ³n
- Reporta estado al Control Plane
- Es el "dÃ³nde" y "cÃ³mo" del cluster

### Ejemplo prÃ¡ctico:

Arquitectura simplificada de un cluster de 3 nodos:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KUBERNETES CLUSTER                            â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚              CONTROL PLANE (Master Node)               â”‚     â”‚
â”‚  â”‚                                                         â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚     â”‚
â”‚  â”‚  â”‚   API    â”‚  â”‚ Schedulerâ”‚  â”‚  etcd    â”‚            â”‚     â”‚
â”‚  â”‚  â”‚  Server  â”‚  â”‚          â”‚  â”‚          â”‚            â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚     â”‚
â”‚  â”‚       â”‚             â”‚             â”‚                    â”‚     â”‚
â”‚  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚     â”‚
â”‚  â”‚                     â”‚                                   â”‚     â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚     â”‚
â”‚  â”‚         â”‚   Controller Manager â”‚                       â”‚     â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                        â”‚                                         â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚           â”‚            â”‚            â”‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ WORKER NODE 1 â”‚  â”‚WORKER N 2 â”‚  â”‚WORKER N 3 â”‚               â”‚
â”‚  â”‚               â”‚  â”‚           â”‚  â”‚           â”‚               â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚               â”‚
â”‚  â”‚ â”‚  kubelet  â”‚ â”‚  â”‚â”‚kubelet  â”‚â”‚  â”‚â”‚kubelet  â”‚â”‚               â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚               â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚               â”‚
â”‚  â”‚ â”‚kube-proxy â”‚ â”‚  â”‚â”‚kube-prxyâ”‚â”‚  â”‚â”‚kube-prxyâ”‚â”‚               â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚               â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚               â”‚
â”‚  â”‚ â”‚Container  â”‚ â”‚  â”‚â”‚Containerâ”‚â”‚  â”‚â”‚Containerâ”‚â”‚               â”‚
â”‚  â”‚ â”‚ Runtime   â”‚ â”‚  â”‚â”‚Runtime  â”‚â”‚  â”‚â”‚Runtime  â”‚â”‚               â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚               â”‚
â”‚  â”‚               â”‚  â”‚           â”‚  â”‚           â”‚               â”‚
â”‚  â”‚ [Pod] [Pod]   â”‚  â”‚[Pod][Pod] â”‚  â”‚[Pod][Pod] â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de Trabajo Fundamental

Cuando ejecutas un comando como `kubectl create deployment nginx --replicas=3`:

```
1. kubectl â†’ 2. API Server â†’ 3. etcd (guardar) â†’
4. Scheduler (asignar nodos) â†’ 5. Controller Manager (crear ReplicaSet) â†’
6. kubelet (ejecutar pods) â†’ 7. Container Runtime (iniciar contenedores)
```

Cada nÃºmero representa un componente que veremos en detalle. Este flujo es la esencia de cÃ³mo Kubernetes orquesta tus aplicaciones.

**ğŸ“ Ver diagrama completo:** [`ejemplos/01-arquitectura-general/diagrama-cluster.md`](./ejemplos/01-arquitectura-general/diagrama-cluster.md)

**ğŸ”¬ Laboratorio**: Explora la arquitectura de tu cluster en [`laboratorios/lab-01-exploracion-arquitectura.md`](./laboratorios/lab-01-exploracion-arquitectura.md)

---

## 2. Control Plane - El Cerebro del Cluster

El Control Plane es el conjunto de componentes que mantienen el estado del cluster. Toma decisiones globales (por ejemplo, dÃ³nde ejecutar un pod) y detecta y responde a eventos del cluster (por ejemplo, iniciar un nuevo pod cuando un deployment necesita mÃ¡s rÃ©plicas).

### CaracterÃ­sticas del Control Plane

**Stateless por diseÃ±o**: Los componentes del Control Plane son stateless. Todo el estado se almacena en etcd, lo que permite escalar y reemplazar componentes fÃ¡cilmente.

**API-Driven**: Todos los componentes se comunican exclusivamente a travÃ©s del API Server. No hay comunicaciÃ³n peer-to-peer entre componentes.

**Idempotente**: Puedes ejecutar la misma operaciÃ³n mÃºltiples veces sin cambiar el resultado mÃ¡s allÃ¡ de la aplicaciÃ³n inicial.

### Componentes del Control Plane

#### 2.1 API Server (kube-apiserver)

**DefiniciÃ³n**: El API Server es el frontend del Control Plane de Kubernetes. Expone la API de Kubernetes y es el Ãºnico componente que se comunica directamente con etcd.

**Rol principal**: ActÃºa como puerta de entrada para todas las operaciones administrativas en el cluster. Es el intermediario entre todos los componentes.

**Responsabilidades clave**:
- **AutenticaciÃ³n**: Verifica quiÃ©n eres (usuario, service account, certificado)
- **AutorizaciÃ³n**: Verifica quÃ© puedes hacer (RBAC, ABAC, Webhook)
- **ValidaciÃ³n**: Verifica que tus requests sean vÃ¡lidos
- **Admission Control**: Aplica polÃ­ticas y modifica objetos antes de persistirlos
- **RESTful API**: Provee endpoints HTTP para todos los recursos de Kubernetes

### Ejemplo prÃ¡ctico:

Cuando ejecutas un comando kubectl, asÃ­ interactÃºa con el API Server:

```bash
# Este comando simple...
kubectl get pods

# ...en realidad hace esto:
# 1. kubectl construye una request HTTP GET
GET /api/v1/namespaces/default/pods HTTP/1.1
Host: kubernetes-api-server:6443
Authorization: Bearer <token>

# 2. API Server procesa:
#    a) Autentica el token
#    b) Verifica permisos RBAC
#    c) Consulta etcd para obtener los pods
#    d) Devuelve la respuesta JSON

# 3. kubectl formatea y muestra los resultados
```

Veamos un ejemplo mÃ¡s complejo creando un pod:

```yaml
# Archivo: pod-nginx.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-example
  labels:
    app: web
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
```

```bash
# Al aplicar este archivo:
kubectl apply -f pod-nginx.yaml

# El API Server procesa asÃ­:
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 1. AUTENTICACIÃ“N                                â”‚
# â”‚    âœ“ Validar certificado cliente                â”‚
# â”‚    âœ“ Verificar identidad                        â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 2. AUTORIZACIÃ“N (RBAC)                          â”‚
# â”‚    âœ“ Â¿Puede este usuario crear pods?            â”‚
# â”‚    âœ“ Â¿En este namespace?                        â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 3. VALIDACIÃ“N                                   â”‚
# â”‚    âœ“ Schema correcto                            â”‚
# â”‚    âœ“ Campos requeridos presentes                â”‚
# â”‚    âœ“ Valores vÃ¡lidos                            â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 4. ADMISSION CONTROLLERS                        â”‚
# â”‚    â€¢ Mutating: Inyectar valores por defecto     â”‚
# â”‚    â€¢ Validating: Aplicar polÃ­ticas              â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 5. PERSISTENCIA                                 â”‚
# â”‚    âœ“ Guardar en etcd                            â”‚
# â”‚    âœ“ Generar eventos                            â”‚
# â”‚    âœ“ Notificar watchers                         â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Puertos importantes**:
- **6443**: Puerto HTTPS seguro (por defecto)
- **8080**: Puerto HTTP inseguro (deshabilitado en versiones recientes por seguridad)

**ğŸ“ Ver configuraciÃ³n completa:** [`ejemplos/02-control-plane/01-api-server-config.yaml`](./ejemplos/02-control-plane/01-api-server-config.yaml)

#### 2.2 etcd - El AlmacÃ©n de Estado

**DefiniciÃ³n**: etcd es una base de datos distribuida de tipo clave-valor que almacena todo el estado del cluster de Kubernetes.

**CaracterÃ­stica principal**: Utiliza el algoritmo de consenso **RAFT** para garantizar consistencia entre mÃºltiples nodos, asegurando que todos tengan la misma visiÃ³n del estado del cluster.

**Â¿Por quÃ© etcd?**:
- **Consistencia fuerte**: GarantÃ­as ACID para operaciones crÃ­ticas
- **Distribuido**: Tolera fallos de nodos individuales
- **Watch API**: Notificaciones en tiempo real de cambios
- **Snapshots**: Backups point-in-time automÃ¡ticos

### Ejemplo prÃ¡ctico:

Estructura de datos en etcd:

```bash
# etcd organiza los datos jerÃ¡rquicamente
# Todo bajo el prefijo /registry/

/registry/pods/default/nginx-example
{
  "apiVersion": "v1",
  "kind": "Pod",
  "metadata": {
    "name": "nginx-example",
    "namespace": "default",
    "uid": "abc-123-def-456",
    "resourceVersion": "12345"
  },
  "spec": { ... },
  "status": {
    "phase": "Running",
    "podIP": "10.244.1.5"
  }
}

/registry/services/default/my-service
{
  "apiVersion": "v1",
  "kind": "Service",
  ...
}

/registry/configmaps/kube-system/cluster-info
{
  "apiVersion": "v1",
  "kind": "ConfigMap",
  ...
}
```

**Datos almacenados en etcd**:
- Todos los objetos de Kubernetes (Pods, Services, Deployments, etc.)
- ConfiguraciÃ³n del cluster
- Secretos (encriptados en reposo desde Kubernetes 1.13+)
- Estado de nodos
- Network policies
- RBAC roles y bindings

### Ejemplo prÃ¡ctico de RAFT consensus:

```
Cluster etcd de 3 nodos:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   etcd-1    â”‚      â”‚   etcd-2    â”‚      â”‚   etcd-3    â”‚
â”‚   LEADER    â”‚â—„â”€â”€â”€â”€â–ºâ”‚  FOLLOWER   â”‚â—„â”€â”€â”€â”€â–ºâ”‚  FOLLOWER   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                     â”‚                     â”‚
      â”‚                     â”‚                     â”‚
      â–¼                     â–¼                     â–¼
  Write Op             Read Op              Read Op
  (Must go            (Can read            (Can read
   to Leader)          from any)            from any)

FLUJO DE ESCRITURA:
1. Client â†’ Leader (write request)
2. Leader â†’ Followers (replicate log entry)
3. Followers â†’ Leader (acknowledge)
4. Leader waits for majority (2 out of 3)
5. Leader commits and responds to client
6. Leader â†’ Followers (commit notification)

Si el Leader falla:
- Followers inician election (timeout detection)
- Follower con mÃ¡s logs recientes se vuelve candidato
- Necesita mayorÃ­a de votos (2 de 3)
- Nuevo Leader elegido en ~1 segundo
```

**Comandos Ãºtiles para interactuar con etcd**:

```bash
# Ver miembros del cluster etcd (en un pod de etcd)
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list

# Ver todos los pods en etcd
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  get /registry/pods --prefix --keys-only

# Crear snapshot (backup)
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /backup/etcd-snapshot-$(date +%Y%m%d-%H%M%S).db
```

**Consideraciones de seguridad**:
- etcd contiene **todos** los secretos del cluster
- Debe estar encriptado en reposo
- Acceso solo por API Server (nunca directamente desde pods)
- Backups regulares son **crÃ­ticos**

**ğŸ“ Ver configuraciÃ³n de etcd cluster:** [`ejemplos/02-control-plane/02-etcd-cluster-config.yaml`](./ejemplos/02-control-plane/02-etcd-cluster-config.yaml)

#### 2.3 Scheduler (kube-scheduler)

**DefiniciÃ³n**: El Scheduler es responsable de asignar pods a nodos (workers). Vigila los pods reciÃ©n creados que no tienen un nodo asignado y selecciona el mejor nodo para ejecutarlos.

**Proceso de decisiÃ³n en 2 fases**:
1. **Filtering (Filtrado)**: Encuentra nodos que cumplan los requisitos
2. **Scoring (PuntuaciÃ³n)**: Clasifica los nodos viables y elige el mejor

### Ejemplo prÃ¡ctico:

Proceso completo de scheduling:

```yaml
# Pod que necesita ser agendado
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        cpu: "500m"      # Necesita 0.5 CPU cores
        memory: "1Gi"    # Necesita 1GB RAM
  nodeSelector:
    disktype: "ssd"      # Solo nodos con SSD
```

```
FASE 1: FILTERING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Scheduler aplica predicados:

Node1: 
  CPU disponible: 0.3 cores âŒ DESCARTADO (insuficiente)
  Memoria: 2Gi âœ“
  disktype: ssd âœ“

Node2:
  CPU disponible: 1.5 cores âœ“
  Memoria: 512Mi âŒ DESCARTADO (insuficiente)
  disktype: ssd âœ“

Node3:
  CPU disponible: 2 cores âœ“
  Memoria: 4Gi âœ“
  disktype: hdd âŒ DESCARTADO (no tiene SSD)

Node4:
  CPU disponible: 1 core âœ“
  Memoria: 2Gi âœ“
  disktype: ssd âœ“ VIABLE

Node5:
  CPU disponible: 3 cores âœ“
  Memoria: 8Gi âœ“
  disktype: ssd âœ“ VIABLE

NODOS VIABLES: Node4, Node5
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

FASE 2: SCORING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Scheduler calcula puntuaciones:

Node4:
  â€¢ LeastRequestedPriority: 65/100
    (recursos utilizados moderadamente)
  â€¢ BalancedResourceAllocation: 70/100
    (CPU y memoria equilibradas)
  â€¢ NodeAffinityPriority: 100/100
    (cumple preferencias)
  â€¢ InterPodAffinityPriority: 50/100
    (no hay otros pods relacionados)
  TOTAL: 71/100

Node5:
  â€¢ LeastRequestedPriority: 90/100
    (muchos recursos libres)
  â€¢ BalancedResourceAllocation: 85/100
    (muy equilibrado)
  â€¢ NodeAffinityPriority: 100/100
    (cumple preferencias)
  â€¢ InterPodAffinityPriority: 80/100
    (tiene pods relacionados, mejor localidad)
  TOTAL: 89/100

DECISIÃ“N: Node5 (mayor puntuaciÃ³n)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

RESULTADO:
kubectl get pod webapp -o wide
NAME     READY   STATUS    NODE
webapp   1/1     Running   node5
```

**Predicados comunes (Filtering)**:
- `PodFitsResources`: Nodo tiene CPU/RAM suficiente
- `PodFitsHost`: Pod solicita un host especÃ­fico
- `PodFitsHostPorts`: Puertos solicitados estÃ¡n libres
- `PodMatchNodeSelector`: Cumple nodeSelector
- `CheckNodeDiskPressure`: Nodo no tiene presiÃ³n de disco
- `CheckNodeMemoryPressure`: Nodo no tiene presiÃ³n de memoria
- `CheckNodePIDPressure`: Nodo no tiene presiÃ³n de PIDs
- `PodToleratesNodeTaints`: Pod tolera los taints del nodo

**Prioridades comunes (Scoring)**:
- `LeastRequestedPriority`: Prefiere nodos con menos recursos utilizados
- `BalancedResourceAllocation`: Prefiere balance entre CPU y memoria
- `NodeAffinityPriority`: Cumplimiento de affinity preferences
- `InterPodAffinityPriority`: Co-localizaciÃ³n de pods relacionados
- `SelectorSpreadPriority`: Distribuye pods del mismo service
- `ImageLocalityPriority`: Prefiere nodos con la imagen ya descargada

**ğŸ“ Ver ejemplos de scheduling avanzado:** [`ejemplos/02-control-plane/03-scheduler-ejemplos.yaml`](./ejemplos/02-control-plane/03-scheduler-ejemplos.yaml)

#### 2.4 Controller Manager (kube-controller-manager)

**DefiniciÃ³n**: El Controller Manager ejecuta mÃºltiples controladores que regulan el estado del cluster. Cada controlador es un loop de control independiente que vigila el estado deseado vs el estado actual.

**PatrÃ³n fundamental - Control Loop**:
```
while true:
  desired_state = get_desired_state()
  current_state = get_current_state()
  
  if current_state != desired_state:
    make_changes_to_match(desired_state)
  
  sleep(reconcile_interval)
```

### Ejemplo prÃ¡ctico:

Veamos cÃ³mo funciona el **Deployment Controller**:

```yaml
# Deployment deseado
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3  # ESTADO DESEADO: 3 rÃ©plicas
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: nginx
        image: nginx
```

```
CONTROL LOOP DEL DEPLOYMENT CONTROLLER:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ITERACIÃ“N 1 (t=0s):
  Desired: 3 rÃ©plicas
  Current: 0 rÃ©plicas (deployment reciÃ©n creado)
  Action: Crear ReplicaSet para gestionar 3 pods
  
ITERACIÃ“N 2 (t=10s):
  Desired: 3 rÃ©plicas
  Current: 3 rÃ©plicas running âœ“
  Action: Ninguna (estado coincide)

EVENTO EXTERNO (t=30s): Un pod se crashea
  
ITERACIÃ“N 3 (t=40s):
  Desired: 3 rÃ©plicas
  Current: 2 rÃ©plicas (un pod muriÃ³)
  Action: ReplicaSet Controller crea nuevo pod
  
ITERACIÃ“N 4 (t=50s):
  Desired: 3 rÃ©plicas
  Current: 3 rÃ©plicas âœ“
  Action: Ninguna

CAMBIO DE USUARIO (t=60s):
  kubectl scale deployment webapp --replicas=5
  
ITERACIÃ“N 5 (t=70s):
  Desired: 5 rÃ©plicas (actualizado por usuario)
  Current: 3 rÃ©plicas
  Action: Escalar ReplicaSet a 5, crear 2 pods nuevos
  
ITERACIÃ“N 6 (t=80s):
  Desired: 5 rÃ©plicas
  Current: 5 rÃ©plicas âœ“
  Action: Ninguna
```

**Controladores principales incluidos**:

1. **Node Controller**: Monitorea salud de nodos
2. **Replication Controller**: Mantiene nÃºmero correcto de pods
3. **Endpoints Controller**: Conecta Services con Pods
4. **Service Account Controller**: Crea ServiceAccounts por defecto
5. **Namespace Controller**: Limpia recursos cuando se elimina un namespace
6. **PersistentVolume Controller**: Gestiona ciclo de vida de volumes
7. **Job Controller**: Ejecuta pods hasta completarse
8. **CronJob Controller**: Ejecuta jobs en horarios programados
9. **Deployment Controller**: Gestiona despliegues y actualizaciones
10. **StatefulSet Controller**: Gestiona aplicaciones con estado
11. **DaemonSet Controller**: Asegura que un pod corra en cada nodo

**Cada controlador vigila recursos especÃ­ficos mediante el Watch API del API Server**.

**ğŸ“ Ver controladores en detalle:** [`ejemplos/02-control-plane/04-controllers-explicados.yaml`](./ejemplos/02-control-plane/04-controllers-explicados.yaml)

**ğŸ”¬ Laboratorio**: Observa los controladores en acciÃ³n en [`laboratorios/lab-02-control-plane-practico.md`](./laboratorios/lab-02-control-plane-practico.md)

---

## 3. Worker Nodes - Donde Corren las Aplicaciones

Los Worker Nodes son las mÃ¡quinas donde realmente se ejecutan tus aplicaciones contenerizadas. A diferencia del Control Plane que toma decisiones, los Worker Nodes son los ejecutores que mantienen los pods corriendo y reportan su estado.

### CaracterÃ­sticas de los Worker Nodes

**Stateless por diseÃ±o**: Los workers no almacenan estado crÃ­tico del cluster. Si un worker falla, sus pods se recrean en otros nodos.

**Escalables horizontalmente**: Puedes agregar o quitar workers sin afectar el Control Plane. Es comÃºn tener docenas o cientos de workers en producciÃ³n.

**Especializados**: Puedes tener diferentes tipos de workers (CPU-optimized, GPU, memoria alta) y usar node selectors para dirigir workloads especÃ­ficos.

### Componentes de un Worker Node

#### 3.1 kubelet - El Agente del Nodo

**DefiniciÃ³n**: kubelet es el agente primario que corre en cada Worker Node. Es responsable de asegurar que los contenedores estÃ©n corriendo en un pod segÃºn lo especificado.

**Rol principal**: ActÃºa como el "capataz" del nodo, tomando instrucciones del API Server y asegurÃ¡ndose de que se ejecuten correctamente.

**Responsabilidades clave**:
- **Registrar el nodo** en el cluster al iniciar
- **Monitorear pods** asignados a su nodo
- **Iniciar contenedores** via Container Runtime Interface (CRI)
- **Ejecutar health checks** (liveness, readiness, startup probes)
- **Reportar estado** del nodo y pods al API Server
- **Gestionar volÃºmenes** montÃ¡ndolos en los pods

### Ejemplo prÃ¡ctico:

El ciclo de vida de kubelet con un pod:

```yaml
# Pod asignado a este nodo por el Scheduler
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  uid: abc-123-def-456
spec:
  nodeName: worker-node-1  # Scheduler ya asignÃ³ este nodo
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    livenessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 10
      periodSeconds: 5
```

```
FLUJO DE KUBELET:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. WATCH API SERVER (cada 10s)
   kubelet: "Â¿Hay pods nuevos para worker-node-1?"
   API Server: "SÃ­, pod 'webapp' (uid: abc-123-def-456)"

2. OBTENER ESPECIFICACIÃ“N
   kubelet descarga spec completa del pod
   - Imagen: nginx
   - Puerto: 80
   - Probes: liveness HTTP GET /

3. PREPARAR ENTORNO
   kubelet crea:
   - Directorio del pod: /var/lib/kubelet/pods/abc-123-def-456/
   - Directorio de volÃºmenes
   - ConfiguraciÃ³n de red

4. CREAR POD SANDBOX
   kubelet â†’ Container Runtime:
   "Crea sandbox para pod abc-123-def-456"
   
   Container Runtime:
   - Crea namespace de red
   - Asigna IP del pod: 10.244.1.5
   - Configura DNS

5. PULL IMAGEN
   kubelet â†’ Container Runtime:
   "Pull imagen nginx"
   
   Container Runtime:
   - Descarga nginx desde Docker Hub
   - Verifica checksum
   - Descomprime layers

6. CREAR Y INICIAR CONTENEDOR
   kubelet â†’ Container Runtime:
   "Crea contenedor 'nginx' en pod abc-123-def-456"
   
   Container Runtime:
   - Crea contenedor
   - Monta volÃºmenes
   - Configura env vars
   - Inicia proceso nginx

7. MONITOREAR HEALTH (cada 5s)
   kubelet ejecuta liveness probe:
   HTTP GET http://10.244.1.5:80/
   
   Respuesta: 200 OK âœ“
   Estado: Healthy

8. REPORTAR ESTADO AL API SERVER (cada 10s)
   kubelet â†’ API Server:
   "Pod 'webapp' en worker-node-1:"
   - Phase: Running
   - IP: 10.244.1.5
   - Container nginx: Running, Healthy
   - Started at: 2024-11-11T10:30:00Z

9. LOOP CONTINUO
   kubelet repite pasos 7-8 mientras el pod exista
```

**Si el contenedor falla**:

```
HEALTH CHECK FALLA:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

IteraciÃ³n 10: Liveness probe HTTP GET â†’ 503 Service Unavailable
IteraciÃ³n 11: Liveness probe HTTP GET â†’ Timeout
IteraciÃ³n 12: Liveness probe HTTP GET â†’ Connection Refused

kubelet detecta: 3 fallos consecutivos

ACCIÃ“N DE KUBELET:
1. Matar contenedor actual
2. Container Runtime â†’ stop nginx container
3. Incrementar restart count
4. Aplicar backoff (10s, 20s, 40s, ...)
5. Container Runtime â†’ start new nginx container
6. Reportar evento al API Server: "Container restarted"

Usuario ve:
$ kubectl get pod webapp
NAME     READY   STATUS    RESTARTS   AGE
webapp   1/1     Running   1          5m
                          â†‘ Restart count incrementado
```

**ConfiguraciÃ³n de kubelet**:

```bash
# Archivo de configuraciÃ³n: /var/lib/kubelet/config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
address: 0.0.0.0
port: 10250
authentication:
  webhook:
    enabled: true
  anonymous:
    enabled: false
authorization:
  mode: Webhook
cgroupDriver: systemd
clusterDomain: cluster.local
clusterDNS:
- 10.96.0.10
containerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock
maxPods: 110
podCIDR: 10.244.1.0/24
```

**ğŸ“ Ver configuraciÃ³n completa de kubelet:** [`ejemplos/03-worker-nodes/01-kubelet-config.yaml`](./ejemplos/03-worker-nodes/01-kubelet-config.yaml)

#### 3.2 kube-proxy - El Proxy de Red

**DefiniciÃ³n**: kube-proxy es un proxy de red que corre en cada nodo. Mantiene las reglas de red que permiten la comunicaciÃ³n de red hacia los pods desde dentro o fuera del cluster.

**Rol principal**: Implementa el concepto de **Service** de Kubernetes, proporcionando una IP virtual estable que balancea trÃ¡fico entre mÃºltiples pods.

**Modos de operaciÃ³n**:
1. **iptables** (por defecto): Usa reglas de firewall de Linux
2. **IPVS** (IP Virtual Server): MÃ¡s eficiente para muchos servicios
3. **userspace** (legacy): Modo antiguo, no recomendado

### Ejemplo prÃ¡ctico:

CÃ³mo kube-proxy implementa un Service:

```yaml
# Deployment con 3 rÃ©plicas
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80

---
# Service que expone el deployment
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp  # Selecciona pods con esta label
  ports:
  - protocol: TCP
    port: 80        # Puerto del Service
    targetPort: 80  # Puerto del contenedor
  type: ClusterIP
```

```
ESTADO DEL CLUSTER:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Service webapp-service:
  ClusterIP: 10.96.0.50
  Port: 80

Pods (endpoints):
  webapp-abc123 â†’ 10.244.1.5:80 (worker-node-1)
  webapp-def456 â†’ 10.244.2.8:80 (worker-node-2)
  webapp-ghi789 â†’ 10.244.3.3:80 (worker-node-3)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

KUBE-PROXY EN WORKER-NODE-1:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. WATCH API SERVER
   kube-proxy detecta:
   - Nuevo Service: webapp-service (10.96.0.50:80)
   - Endpoints: 3 pods con IPs reales

2. CREAR REGLAS IPTABLES
   kube-proxy configura cadenas de iptables:

   # Regla principal: Capturar trÃ¡fico al Service
   -A KUBE-SERVICES -d 10.96.0.50/32 -p tcp -m tcp --dport 80 \
     -j KUBE-SVC-WEBAPP

   # Balanceo de carga (round-robin)
   # 33.3% de trÃ¡fico al primer pod
   -A KUBE-SVC-WEBAPP -m statistic --mode random --probability 0.33333 \
     -j KUBE-SEP-POD1

   # 50% del trÃ¡fico restante al segundo pod
   -A KUBE-SVC-WEBAPP -m statistic --mode random --probability 0.5 \
     -j KUBE-SEP-POD2

   # El resto al tercer pod
   -A KUBE-SVC-WEBAPP -j KUBE-SEP-POD3

   # DNAT a cada pod especÃ­fico
   -A KUBE-SEP-POD1 -p tcp -j DNAT --to-destination 10.244.1.5:80
   -A KUBE-SEP-POD2 -p tcp -j DNAT --to-destination 10.244.2.8:80
   -A KUBE-SEP-POD3 -p tcp -j DNAT --to-destination 10.244.3.3:80

3. RESULTADO PRÃCTICO
   Cuando un pod hace:
   curl http://webapp-service:80

   Kernel Linux intercepta el paquete:
   - Destino original: 10.96.0.50:80
   - iptables aplica DNAT
   - Destino final: 10.244.1.5:80 (o uno de los otros pods)
   - TrÃ¡fico se enruta al pod seleccionado
```

**Ventajas de cada modo**:

```
MODO IPTABLES (por defecto):
âœ“ Bajo overhead
âœ“ Kernel-space (muy rÃ¡pido)
âœ“ Ampliamente probado
âœ— Performance degrada con 1000+ services
âœ— No soporta load balancing algorithms avanzados

MODO IPVS:
âœ“ Mejor performance con muchos services
âœ“ MÃºltiples algoritmos de balanceo (round-robin, least-connection, etc.)
âœ“ MÃ¡s eficiente para clusters grandes
âœ— Requiere kernel modules adicionales
âœ— MÃ¡s complejo de troubleshoot

CONFIGURACIÃ“N IPVS:
# kube-proxy config
mode: ipvs
ipvs:
  scheduler: "rr"  # round-robin, lc (least-connection), etc.
```

**Ver reglas de iptables en un nodo**:

```bash
# Ver todas las cadenas de Kubernetes
sudo iptables -t nat -L -n | grep KUBE

# Ver reglas especÃ­ficas de un service
sudo iptables -t nat -L KUBE-SERVICES -n | grep webapp-service

# Ver balanceo de carga
sudo iptables -t nat -L KUBE-SVC-WEBAPP -n -v
```

**ğŸ“ Ver configuraciÃ³n completa de kube-proxy:** [`ejemplos/03-worker-nodes/02-kube-proxy-config.yaml`](./ejemplos/03-worker-nodes/02-kube-proxy-config.yaml)

#### 3.3 Container Runtime - El Motor de Contenedores

**DefiniciÃ³n**: El Container Runtime es el software responsable de ejecutar contenedores. Kubernetes delega la ejecuciÃ³n real de contenedores a este componente.

**Rol principal**: Gestionar el ciclo de vida completo de contenedores: pull de imÃ¡genes, creaciÃ³n, inicio, parada y eliminaciÃ³n.

**Container Runtime Interface (CRI)**:
Kubernetes usa CRI como abstracciÃ³n, permitiendo mÃºltiples runtimes:

**Runtimes compatibles**:
1. **containerd** (recomendado): Ligero, mantenido por CNCF
2. **CRI-O**: DiseÃ±ado especÃ­ficamente para Kubernetes
3. **Docker** (via cri-dockerd): Requiere shim adicional desde Kubernetes 1.24+

### Ejemplo prÃ¡ctico:

Flujo completo de creaciÃ³n de contenedor:

```yaml
# Pod simple
apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - name: redis
    image: redis:alpine
    ports:
    - containerPort: 6379
    resources:
      requests:
        memory: "256Mi"
        cpu: "500m"
      limits:
        memory: "512Mi"
        cpu: "1000m"
```

```
INTERACCIÃ“N KUBELET â†” CONTAINER RUNTIME:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. KUBELET â†’ CONTAINER RUNTIME (via CRI)
   Request: "Crea pod sandbox para redis"
   
   Container Runtime (containerd):
   a) Crea pause container (infrastructure container)
      - Este contenedor mantiene el namespace de red del pod
      - IP asignada: 10.244.1.10
      - Network namespace: /var/run/netns/cni-abc123
   
   b) Configura networking (via CNI plugin)
      - Asigna IP del pod
      - Configura rutas
      - Configura DNS

2. KUBELET â†’ CONTAINER RUNTIME
   Request: "Pull imagen redis:alpine"
   
   Container Runtime:
   a) Contacta Docker Hub (registry.hub.docker.com)
   b) Descarga layers:
      - Layer 1: base alpine (5MB)
      - Layer 2: redis binaries (8MB)
      - Layer 3: config files (1KB)
   c) Verifica checksums SHA256
   d) Descomprime y almacena en:
      /var/lib/containerd/io.containerd.content.v1.content/
   
   Response: "Imagen lista"

3. KUBELET â†’ CONTAINER RUNTIME
   Request: "Crea contenedor 'redis' en pod sandbox"
   Params:
   - Imagen: redis:alpine
   - Comando: redis-server
   - Memory limit: 512Mi
   - CPU limit: 1 core
   - Network: compartir con pause container
   
   Container Runtime:
   a) Crea container spec (OCI runtime spec)
   b) Configura cgroups:
      - memory.limit_in_bytes = 512Mi
      - cpu.cfs_quota_us = 100000 (1 core)
   c) Configura namespaces:
      - Network: compartido con pause container
      - PID: aislado
      - Mount: aislado
      - IPC: compartido
   d) Prepara filesystem (overlay2):
      - Lower layers: imagen redis (read-only)
      - Upper layer: cambios del contenedor (read-write)
      - Merged: vista combinada
   
   Response: "Contenedor creado, ID: abc123def456"

4. KUBELET â†’ CONTAINER RUNTIME
   Request: "Inicia contenedor abc123def456"
   
   Container Runtime:
   a) Ejecuta runc (OCI runtime)
   b) Inicia proceso redis-server
   c) PID del proceso: 12345
   d) Estado: Running
   
   Response: "Contenedor iniciado exitosamente"

5. MONITORING CONTINUO
   kubelet â†’ Container Runtime (cada 10s):
   "Estado del contenedor abc123def456?"
   
   Container Runtime:
   - Estado: Running
   - Uso de memoria: 128Mi / 512Mi
   - Uso de CPU: 0.3 cores / 1 core
   - Uptime: 5m 30s
```

**Arquitectura de capas**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           kubelet                          â”‚
â”‚  (Kubernetes node agent)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ CRI gRPC API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Container Runtime                   â”‚
â”‚        (containerd / CRI-O)                â”‚
â”‚  - Image management                        â”‚
â”‚  - Container lifecycle                     â”‚
â”‚  - Pod sandbox management                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ OCI Runtime API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         OCI Runtime                        â”‚
â”‚         (runc)                             â”‚
â”‚  - Create container processes              â”‚
â”‚  - Configure namespaces/cgroups            â”‚
â”‚  - Execute container                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Verificar Container Runtime**:

```bash
# Ver quÃ© runtime usa el cluster
kubectl get nodes -o wide

# Conectarse al nodo y verificar containerd
sudo crictl info

# Listar pods corriendo (vista del runtime)
sudo crictl pods

# Listar contenedores
sudo crictl ps

# Ver logs de un contenedor
sudo crictl logs <container-id>

# Inspeccionar contenedor
sudo crictl inspect <container-id>
```

**ğŸ“ Ver configuraciÃ³n de Container Runtime:** [`ejemplos/03-worker-nodes/03-container-runtime-config.yaml`](./ejemplos/03-worker-nodes/03-container-runtime-config.yaml)

**ğŸ”¬ Laboratorio**: Explora Worker Nodes en profundidad en [`laboratorios/lab-03-worker-nodes.md`](./laboratorios/lab-03-worker-nodes.md)

---

## 4. ComunicaciÃ³n entre Componentes

Entender cÃ³mo se comunican los componentes es crucial para diagnosticar problemas y optimizar el cluster. Kubernetes usa mÃºltiples patrones de comunicaciÃ³n.

### Patrones de ComunicaciÃ³n

#### 4.1 API Server como Hub Central

**Principio fundamental**: TODOS los componentes se comunican a travÃ©s del API Server. No hay comunicaciÃ³n directa peer-to-peer entre componentes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API SERVER                          â”‚
â”‚                   (Hub Central)                         â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚     â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
    â–¼     â–¼      â–¼      â–¼      â–¼      â–¼      â–¼      â–¼
  etcd  Sched  Ctrl  kubectl kubelet proxy  CCM  Addons
```

**Ventajas de este diseÃ±o**:
- **Punto Ãºnico de autenticaciÃ³n y autorizaciÃ³n**
- **AuditorÃ­a centralizada** de todas las operaciones
- **FÃ¡cil de escalar** el Control Plane (mÃºltiples API servers)
- **Desacoplamiento** entre componentes
- **Consistencia** mediante etcd como Ãºnica fuente de verdad

### Ejemplo prÃ¡ctico:

Flujo completo de crear un deployment:

```bash
# Usuario ejecuta
kubectl create deployment nginx --image=nginx --replicas=3
```

```
COMUNICACIÃ“N PASO A PASO:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PASO 1: kubectl â†’ API Server
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
kubectl:
  POST /apis/apps/v1/namespaces/default/deployments
  Headers:
    Authorization: Bearer <token>
    Content-Type: application/json
  Body:
    {
      "apiVersion": "apps/v1",
      "kind": "Deployment",
      "metadata": {"name": "nginx"},
      "spec": {
        "replicas": 3,
        "selector": {"matchLabels": {"app": "nginx"}},
        "template": {...}
      }
    }

API Server:
  1. Autentica token âœ“
  2. Autoriza operaciÃ³n (RBAC) âœ“
  3. Valida JSON schema âœ“
  4. Ejecuta admission controllers âœ“
  5. Persiste en etcd âœ“
  
  Response: 201 Created

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASO 2: API Server â†’ etcd
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Server â†’ etcd:
  PUT /registry/deployments/default/nginx
  Body: <deployment JSON completo>

etcd:
  - Replica a followers via RAFT
  - Espera quorum (2/3 nodos)
  - Confirma write
  
  Response: Success

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASO 3: Deployment Controller detecta vÃ­a Watch
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Deployment Controller tiene open watch:
  GET /apis/apps/v1/deployments?watch=true
  (conexiÃ³n HTTP long-polling persistente)

API Server envÃ­a evento:
  {
    "type": "ADDED",
    "object": {
      "kind": "Deployment",
      "metadata": {"name": "nginx"},
      ...
    }
  }

Deployment Controller:
  - Detecta nuevo deployment
  - Reconcilia estado: necesita crear ReplicaSet

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASO 4: Deployment Controller â†’ API Server
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Deployment Controller:
  POST /apis/apps/v1/namespaces/default/replicasets
  Body: {
    "kind": "ReplicaSet",
    "spec": {"replicas": 3, ...}
  }

API Server:
  - Valida y persiste en etcd
  - Notifica watchers
  
  Response: 201 Created

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASO 5: ReplicaSet Controller detecta vÃ­a Watch
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ReplicaSet Controller:
  - Detecta nuevo ReplicaSet
  - Reconcilia: necesita 3 pods, tiene 0
  - Crea 3 pods

ReplicaSet Controller â†’ API Server:
  POST /api/v1/namespaces/default/pods (x3)
  Body: <pod spec>

API Server:
  - Valida y persiste cada pod en etcd
  - Pods creados con status: Pending
  - Notifica watchers

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASO 6: Scheduler detecta Pods Pending vÃ­a Watch
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Scheduler tiene watch:
  GET /api/v1/pods?watch=true&fieldSelector=spec.nodeName=""

API Server envÃ­a 3 eventos ADDED

Scheduler:
  - Ejecuta algoritmo de scheduling
  - Elige nodos: worker-1, worker-2, worker-3

Scheduler â†’ API Server (para cada pod):
  PATCH /api/v1/namespaces/default/pods/<pod-name>
  Body: {
    "spec": {"nodeName": "worker-1"}
  }

API Server:
  - Actualiza pod.spec.nodeName en etcd
  - Notifica watchers

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASO 7: kubelet detecta Pod asignado vÃ­a Watch
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
kubelet en worker-1 tiene watch:
  GET /api/v1/pods?watch=true&fieldSelector=spec.nodeName=worker-1

API Server envÃ­a evento MODIFIED

kubelet:
  - Detecta pod asignado a su nodo
  - Inicia proceso de creaciÃ³n

kubelet â†’ Container Runtime (CRI):
  1. CreatePodSandbox
  2. PullImage
  3. CreateContainer
  4. StartContainer

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASO 8: kubelet reporta estado â†’ API Server
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
kubelet â†’ API Server:
  PATCH /api/v1/namespaces/default/pods/<pod-name>/status
  Body: {
    "status": {
      "phase": "Running",
      "podIP": "10.244.1.5",
      "containerStatuses": [...]
    }
  }

API Server:
  - Actualiza pod.status en etcd
  - Notifica watchers

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASO 9: Endpoint Controller detecta Pod Ready vÃ­a Watch
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Endpoint Controller:
  - Detecta pod Running con label app=nginx
  - Si hay Service matching, actualiza Endpoints

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASO 10: kube-proxy detecta Endpoints vÃ­a Watch
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
kube-proxy:
  - Detecta nuevos endpoints
  - Actualiza reglas iptables/IPVS
  - TrÃ¡fico puede fluir al pod

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RESULTADO FINAL:
  3 pods corriendo en 3 nodos diferentes
  Tiempo total: ~10-15 segundos
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### 4.2 Watch API - Eficiencia en Tiempo Real

**Problema**: Si cada componente hace polling (consultar repetidamente) al API Server, generarÃ­a trÃ¡fico masivo.

**SoluciÃ³n**: Kubernetes usa **Watch API** - conexiones HTTP long-polling donde el API Server envÃ­a eventos solo cuando hay cambios.

```
SIN WATCH (Polling):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Controller â†’ API Server: "Â¿Hay nuevos deployments?" (cada 1s)
API Server: "No"
Controller â†’ API Server: "Â¿Hay nuevos deployments?"
API Server: "No"
Controller â†’ API Server: "Â¿Hay nuevos deployments?"
API Server: "No"
...
(1000s de requests vacÃ­os)

CON WATCH:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Controller â†’ API Server: GET /deployments?watch=true
  (conexiÃ³n queda abierta)

... silencio (sin trÃ¡fico) ...

(Usuario crea deployment)

API Server â†’ Controller:
  {
    "type": "ADDED",
    "object": <deployment>
  }

Controller procesa evento

... silencio ...

(Solo trÃ¡fico cuando hay cambios reales)
```

**ImplementaciÃ³n de Watch**:

```go
// Ejemplo conceptual en Go (usado internamente por componentes)
watcher, err := clientset.AppsV1().Deployments("default").Watch(
    context.TODO(),
    metav1.ListOptions{},
)

for event := range watcher.ResultChan() {
    switch event.Type {
    case watch.Added:
        // Nuevo deployment creado
        handleAdd(event.Object)
    case watch.Modified:
        // Deployment actualizado
        handleUpdate(event.Object)
    case watch.Deleted:
        // Deployment eliminado
        handleDelete(event.Object)
    }
}
```

#### 4.3 Networking entre Pods

**Modelo de red de Kubernetes**:

1. **Cada Pod tiene una IP Ãºnica** en el cluster
2. **Pods pueden comunicarse sin NAT** directamente con sus IPs
3. **Nodes pueden comunicarse con todos los Pods** sin NAT
4. **La IP que un Pod ve para sÃ­ mismo** es la misma que otros ven

### Ejemplo prÃ¡ctico:

```yaml
# Pod Frontend
apiVersion: v1
kind: Pod
metadata:
  name: frontend
  labels:
    app: frontend
spec:
  containers:
  - name: web
    image: nginx
    ports:
    - containerPort: 80

---
# Pod Backend
apiVersion: v1
kind: Pod
metadata:
  name: backend
  labels:
    app: backend
spec:
  containers:
  - name: api
    image: my-api
    ports:
    - containerPort: 8080
```

```
COMUNICACIÃ“N POD A POD:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Escenario: frontend necesita llamar a backend

OPCIÃ“N 1: ComunicaciÃ³n directa (NO recomendado)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
frontend (10.244.1.5) â†’ backend (10.244.2.8:8080)

Problemas:
âœ— IP del pod cambia si se recrea
âœ— Sin balanceo de carga si hay mÃºltiples backends
âœ— Sin service discovery

OPCIÃ“N 2: Via Service (RECOMENDADO)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Crear Service para backend:

apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  selector:
    app: backend
  ports:
  - port: 80
    targetPort: 8080

Service obtiene ClusterIP: 10.96.0.100

2. Frontend usa DNS del cluster:

frontend container:
  $ curl http://backend-service:80/api

3. Flujo de resoluciÃ³n:

   a) DNS Query
      frontend â†’ CoreDNS: "Â¿IP de backend-service?"
      CoreDNS â†’ frontend: "10.96.0.100"
   
   b) Packet routing
      frontend envÃ­a: 10.244.1.5 â†’ 10.96.0.100:80
      
   c) kube-proxy intercept
      iptables/IPVS reescribe destino:
      10.96.0.100:80 â†’ 10.244.2.8:8080
      
   d) Entrega
      Packet llega a backend pod
      backend responde: 10.244.2.8 â†’ 10.244.1.5

Ventajas:
âœ“ IP estable (Service ClusterIP)
âœ“ DNS name (backend-service)
âœ“ Balanceo de carga automÃ¡tico
âœ“ Service discovery built-in
```

**ğŸ“ Ver ejemplos de networking:** [`ejemplos/04-networking/comunicacion-pods.yaml`](./ejemplos/04-networking/comunicacion-pods.yaml)

**ğŸ”¬ Laboratorio**: Practica troubleshooting de comunicaciÃ³n en [`laboratorios/lab-04-troubleshooting-networking.md`](./laboratorios/lab-04-troubleshooting-networking.md)

---

## 5. Alta Disponibilidad y Conceptos de Clustering

### ğŸ“ Nota sobre el Entorno del Curso

**IMPORTANTE**: En este mÃ³dulo exploraremos los conceptos de Alta Disponibilidad (HA) desde una perspectiva **teÃ³rica y arquitectÃ³nica**. 

- âœ… **Comprenderemos** cÃ³mo funcionan los clusters HA en producciÃ³n
- âœ… **Analizaremos** la arquitectura de mÃºltiples masters y etcd clustering
- âœ… **Exploraremos** los componentes del Control Plane en nuestro Minikube
- âŒ **NO implementaremos** un cluster multi-nodo real (usamos Minikube)
- âŒ **NO configuraremos** Load Balancers o etcd externo manualmente

**JustificaciÃ³n**: Minikube simula un cluster completo en un solo nodo, pero nos permite inspeccionar y entender todos los componentes que en producciÃ³n estarÃ­an distribuidos. Los conceptos de HA son fundamentales para entender la arquitectura, aunque su implementaciÃ³n prÃ¡ctica queda fuera del alcance de este curso introductorio.

---

### 5.1 Conceptos de Alta Disponibilidad

La Alta Disponibilidad en Kubernetes garantiza que el cluster continÃºe operando incluso cuando algunos componentes fallen. Estos conceptos son crÃ­ticos para entornos de producciÃ³n donde el downtime no es aceptable.

**Conceptos clave que entenderemos**:
- **Control Plane HA**: MÃºltiples rÃ©plicas de API Server, Scheduler, Controller Manager
- **etcd HA**: Cluster de etcd con quorum (3, 5, o 7 nodos)
- **Worker Node redundancy**: MÃºltiples workers para distribuir carga
- **Load balancing**: DistribuciÃ³n de trÃ¡fico entre componentes replicados

### 5.2 Arquitectura HA en ProducciÃ³n (Conceptual)

Aunque en Minikube todo corre en un solo nodo, es importante entender cÃ³mo se ve un cluster de producciÃ³n con alta disponibilidad.

**Arquitectura HA tÃ­pica** (referencia conceptual):

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      LOAD BALANCER                 â”‚
                    â”‚   (HAProxy / nginx / cloud LB)     â”‚
                    â”‚   VIP: 192.168.1.100:6443          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                      â”‚                      â”‚
        â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MASTER 1     â”‚      â”‚  MASTER 2     â”‚      â”‚  MASTER 3     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ API Server    â”‚      â”‚ API Server    â”‚      â”‚ API Server    â”‚
â”‚   :6443       â”‚      â”‚   :6443       â”‚      â”‚   :6443       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Scheduler     â”‚      â”‚ Scheduler     â”‚      â”‚ Scheduler     â”‚
â”‚ (standby)     â”‚      â”‚ (ACTIVE)      â”‚      â”‚ (standby)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ctrl Manager  â”‚      â”‚ Ctrl Manager  â”‚      â”‚ Ctrl Manager  â”‚
â”‚ (standby)     â”‚      â”‚ (ACTIVE)      â”‚      â”‚ (standby)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   etcd CLUSTER         â”‚
                    â”‚   3 o 5 nodos          â”‚
                    â”‚   RAFT consensus       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ejemplo conceptual de configuraciÃ³n HA** (referencia - NO para implementar):

```yaml
# EJEMPLO TEÃ“RICO: ConfiguraciÃ³n de cluster HA con kubeadm
# Este archivo es SOLO para comprensiÃ³n arquitectÃ³nica
# NO lo usaremos en Minikube
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
controlPlaneEndpoint: "192.168.1.100:6443"  # VIP del Load Balancer
etcd:
  external:
    endpoints:
    - https://192.168.1.10:2379  # etcd-1
    - https://192.168.1.11:2379  # etcd-2
    - https://192.168.1.12:2379  # etcd-3
    caFile: /etc/kubernetes/pki/etcd/ca.crt
    certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
    keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
networking:
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/12"
```

**CaracterÃ­sticas importantes**:

1. **API Server**: MÃºltiples instancias activas (active-active)
   - Todas responden a requests simultÃ¡neamente
   - Load Balancer distribuye trÃ¡fico
   - Sin coordinaciÃ³n entre ellas (stateless)

2. **Scheduler y Controller Manager**: Leader Election (active-passive)
   
   ```bash
   # CONCEPTO: Solo UNA instancia es lÃ­der y trabaja activamente
   # Las demÃ¡s estÃ¡n en standby esperando
   
   # En un cluster real, verÃ­as quiÃ©n es el lÃ­der:
   # $ kubectl get endpoints kube-scheduler -n kube-system -o yaml
   # holderIdentity: master-2_abc123...
   # leaseDurationSeconds: 15
   # renewTime: "2024-11-11T10:45:30Z"
   ```

3. **etcd**: Cluster con RAFT consensus
   - NÃºmero impar de nodos (3, 5, 7)
   - Quorum: (N+1)/2
   - 3 nodos: tolera 1 fallo
   - 5 nodos: tolera 2 fallos

**CÃ³mo explorar estos conceptos en Minikube**:

```bash
# Aunque Minikube es single-node, podemos ver cÃ³mo estÃ¡n los componentes

# Ver pods del Control Plane
$ kubectl get pods -n kube-system

# En Minikube verÃ¡s:
# - etcd-minikube (solo 1 instancia)
# - kube-apiserver-minikube
# - kube-controller-manager-minikube
# - kube-scheduler-minikube

# Inspeccionar el API Server
$ kubectl get pod kube-apiserver-minikube -n kube-system -o yaml

# Ver logs del scheduler
$ kubectl logs kube-scheduler-minikube -n kube-system

# En un cluster HA real tendrÃ­as mÃºltiples instancias de cada uno
```

**Referencia conceptual**: Pasos que se seguirÃ­an en un cluster HA real (SOLO para conocimiento):

```bash
# EJEMPLO TEÃ“RICO - NO EJECUTAR EN MINIKUBE
# Este es el proceso que usarÃ­as con kubeadm en un entorno real

# 1. Configurar Load Balancer (HAProxy/nginx)
# frontend k8s-api
#     bind 192.168.1.100:6443
#     backend: master-1, master-2, master-3

# 2. Inicializar primer master
# $ kubeadm init --config=kubeadm-config.yaml --upload-certs

# 3. Unir masters adicionales
# $ kubeadm join 192.168.1.100:6443 \
#     --control-plane \
#     --certificate-key <cert-key>

# 4. Resultado: mÃºltiples masters
# $ kubectl get nodes
# master-1   Ready    control-plane   10m
# master-2   Ready    control-plane   5m
# master-3   Ready    control-plane   2m
```

**ğŸ“ Ver referencia de configuraciÃ³n HA:** [`ejemplos/04-alta-disponibilidad/ha-cluster-setup.yaml`](./ejemplos/04-alta-disponibilidad/ha-cluster-setup.yaml)

### 5.3 etcd: El AlmacÃ©n de Estado (Conceptos)

etcd es el componente MÃS CRÃTICO del cluster (almacena TODO el estado). En clusters de producciÃ³n, etcd puede ejecutarse en HA.

**TopologÃ­as de etcd** (referencia conceptual):

1. **Stacked etcd** (mismo nodo que control plane):
   ```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Master Node    â”‚
   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
   â”‚ â”‚ API Server   â”‚ â”‚
   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
   â”‚ â”‚ etcd         â”‚â—„â”¼â”€â”€â” Cluster
   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ de etcd
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                         â”‚
   âœ“ MÃ¡s simple           â”‚
   âœ“ Menos recursos       â”‚
   âœ— Menos resiliente     â”‚
   
   (Minikube usa esta topologÃ­a)
   ```

2. **External etcd** (nodos dedicados - solo producciÃ³n):
   ```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Master Node  â”‚       â”‚ etcd Node    â”‚
   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
   â”‚ â”‚API Serverâ”œâ”€â”¼â”€â”€â”€â”€â”€â”€â–ºâ”‚ â”‚  etcd    â”‚ â”‚
   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   âœ“ MÃ¡s resiliente
   âœ“ Performance aislada
   âœ— MÃ¡s complejo
   âœ— MÃ¡s recursos
   ```

**Explorando etcd en Minikube**:

```bash
# Acceder al pod de etcd en Minikube
$ kubectl exec -it etcd-minikube -n kube-system -- sh

# Dentro del pod, ver datos almacenados
$ export ETCDCTL_API=3
$ etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/var/lib/minikube/certs/etcd/ca.crt \
  --cert=/var/lib/minikube/certs/etcd/server.crt \
  --key=/var/lib/minikube/certs/etcd/server.key \
  get / --prefix --keys-only | head -20

# VerÃ¡s keys como:
# /registry/pods/default/my-pod
# /registry/services/default/kubernetes
# /registry/deployments/default/my-app

# Ver estadÃ­sticas
$ etcdctl endpoint status --write-out=table
```

**REFERENCIA: CÃ³mo se verÃ­a etcd cluster en producciÃ³n** (solo conceptual):

```bash
# EJEMPLO TEÃ“RICO - Cluster de 3 nodos etcd

# Ver miembros del cluster
# $ ETCDCTL_API=3 etcdctl member list
# etcd-1: https://192.168.1.10:2379 (LEADER)
# etcd-2: https://192.168.1.11:2379 (FOLLOWER)
# etcd-3: https://192.168.1.12:2379 (FOLLOWER)

# Ver salud
# $ ETCDCTL_API=3 etcdctl endpoint health --cluster
# etcd-1 is healthy
# etcd-2 is healthy
# etcd-3 is healthy
```

**CÃ¡lculo de quorum** (importante para entender resilencia):

```
Quorum = (N + 1) / 2

â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Nodos â”‚ Quorum â”‚ Fallos OK    â”‚ RecomendaciÃ³n   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   1   â”‚   1    â”‚ 0 (sin HA)   â”‚ Minikube/Dev    â”‚
â”‚   3   â”‚   2    â”‚ 1 nodo       â”‚ âœ“ ProducciÃ³n    â”‚
â”‚   5   â”‚   3    â”‚ 2 nodos      â”‚ âœ“ Alta crÃ­tica  â”‚
â”‚   7   â”‚   4    â”‚ 3 nodos      â”‚ Casos extremos  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âš ï¸ IMPORTANTE: MÃ¡s nodos NO siempre es mejor
- MÃ¡s latencia (consensus mÃ¡s lento)
- MÃ¡s ancho de banda (replicaciÃ³n)
- NÃºmero impar SIEMPRE (evitar split-brain)
```

**ğŸ“ Ver referencia de etcd cluster:** [`ejemplos/04-alta-disponibilidad/etcd-ha-cluster.yaml`](./ejemplos/04-alta-disponibilidad/etcd-ha-cluster.yaml)

### 5.4 Backup y Restore de etcd en Minikube

Aunque no tenemos un cluster multi-nodo, podemos practicar backup y restore de etcd en Minikube.

```bash
# Hacer snapshot del etcd de Minikube
$ minikube ssh

# Dentro de Minikube
$ docker exec -it $(docker ps -qf "name=etcd") sh

# Crear backup
$ ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-backup.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/var/lib/minikube/certs/etcd/ca.crt \
  --cert=/var/lib/minikube/certs/etcd/server.crt \
  --key=/var/lib/minikube/certs/etcd/server.key

# Verificar snapshot
$ ETCDCTL_API=3 etcdctl snapshot status /tmp/etcd-backup.db --write-out=table
```

### 5.5 Conceptos de HA para ProducciÃ³n (Referencia)

**NOTA**: Esta secciÃ³n es SOLO para referencia conceptual. En producciÃ³n real con managed Kubernetes (AKS, EKS, GKE), el proveedor de nube gestiona automÃ¡ticamente la HA del Control Plane.

**Checklist conceptual de HA**:

```yaml
# REFERENCIA: ConfiguraciÃ³n tÃ­pica de producciÃ³n
# (NO aplicable a Minikube)

Control Plane:
  masters: 3  # NÃºmero impar
  etcd_nodes: 3  # Puede ser external o stacked
  load_balancer: "Cloud LB (Azure LB, AWS ELB, GCP LB)"
  
Worker Nodes:
  min_workers: 3+
  distribution: "MÃºltiples availability zones"
  auto_scaling: true
  
Networking:
  cni_plugin: "Calico / Cilium / Azure CNI"
  dns_replicas: 2
  
Storage:
  persistent_volumes: "Con replicaciÃ³n"
  backup_strategy: "Automated etcd snapshots"
  
Monitoring:
  metrics_server: true
  prometheus: true
  alerting: "Critical components down"
```

**Patrones de fallo y recuperaciÃ³n**:

| Escenario | Impacto | RecuperaciÃ³n |
|-----------|---------|--------------|
| 1 API Server cae | âœ“ Sin impacto (LB redirige) | AutomÃ¡tico |
| 1 etcd nodo cae (de 3) | âœ“ Sin impacto (quorum 2/3) | Manual: reemplazar nodo |
| 2 etcd nodos caen | âœ— Cluster read-only | Urgente: restaurar nodos |
| Scheduler cae | âœ“ Leader election (1-2s) | AutomÃ¡tico |
| Controller Mgr cae | âœ“ Leader election (1-2s) | AutomÃ¡tico |
| Worker node cae | âœ“ Pods migran a otros nodes | AutomÃ¡tico (60s-5min) |

**ğŸ“ Ver estrategias de backup/restore:** [`ejemplos/04-alta-disponibilidad/backup-restore.yaml`](./ejemplos/04-alta-disponibilidad/backup-restore.yaml)

---

## 6. Cluster Addons

Los Addons son componentes opcionales que extienden la funcionalidad del cluster. Aunque no son parte del core de Kubernetes, son casi imprescindibles en producciÃ³n.

**Addons comunes**:
- **DNS (CoreDNS)**: ResoluciÃ³n de nombres de Services
- **Metrics Server**: MÃ©tricas de recursos (CPU, RAM)
- **Dashboard**: UI web para gestiÃ³n
- **Ingress Controller**: Routing HTTP/HTTPS
- **CNI Plugin**: Networking entre pods

### 6.1 CoreDNS - Servicio DNS del Cluster

CoreDNS proporciona resoluciÃ³n DNS para todos los Services y Pods en el cluster.

**Â¿CÃ³mo funciona?**

```
POD solicita: backend-service.default.svc.cluster.local
                                   â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  /etc/resolv.conf del Pod          â”‚
            â”‚  nameserver 10.96.0.10  â† CoreDNS  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ DNS Query
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  CoreDNS Pod                        â”‚
            â”‚  - Lee Service resources vÃ­a API    â”‚
            â”‚  - Retorna ClusterIP: 10.96.100.50  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ejemplo de configuraciÃ³n CoreDNS inline**:

```yaml
# CoreDNS se despliega como un Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
spec:
  replicas: 2  # HA: mÃºltiples rÃ©plicas
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      containers:
      - name: coredns
        image: registry.k8s.io/coredns/coredns:v1.10.1
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
      volumes:
      - name: config-volume
        configMap:
          name: coredns
---
# Service para exponer CoreDNS
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
spec:
  clusterIP: 10.96.0.10  # IP fija conocida
  selector:
    k8s-app: kube-dns
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
```

**ConfiguraciÃ³n de CoreDNS (Corefile)**:

```bash
# ConfigMap: coredns
.:53 {
    errors
    health {
       lameduck 5s
    }
    ready
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
    prometheus :9153
    forward . /etc/resolv.conf {
       max_concurrent 1000
    }
    cache 30
    loop
    reload
    loadbalance
}
```

**Prueba de DNS**:

```bash
# Desde un pod, verificar DNS
$ kubectl run test-dns --rm -it --image=busybox -- sh

/ # nslookup kubernetes
Server:    10.96.0.10
Address:   10.96.0.10:53

Name:      kubernetes.default.svc.cluster.local
Address:   10.96.0.1

/ # nslookup backend-service
Server:    10.96.0.10
Address:   10.96.0.10:53

Name:      backend-service.default.svc.cluster.local
Address:   10.96.100.50
```

**Formatos DNS vÃ¡lidos**:

```
<service-name>                             â†’ Mismo namespace
<service-name>.<namespace>                 â†’ Namespace especÃ­fico
<service-name>.<namespace>.svc             â†’ Forma corta
<service-name>.<namespace>.svc.cluster.local â†’ FQDN completo
```

**ğŸ“ Ver configuraciÃ³n completa de CoreDNS:** [`ejemplos/05-addons/coredns-config.yaml`](./ejemplos/05-addons/coredns-config.yaml)

### 6.2 Metrics Server - MÃ©tricas de Recursos

Metrics Server recolecta mÃ©tricas de recursos (CPU, memoria) de kubelet y las expone vÃ­a API.

**Â¿Para quÃ© sirve?**
- `kubectl top nodes` / `kubectl top pods`
- Horizontal Pod Autoscaler (HPA)
- Vertical Pod Autoscaler (VPA)

**Arquitectura**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  kubectl top pods                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ GET /apis/metrics.k8s.io/v1beta1/pods
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Server (agrega API metrics.k8s.io)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Proxy request
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Metrics Server                                          â”‚
â”‚  - Agrega mÃ©tricas de todos los kubelets                 â”‚
â”‚  - No almacena histÃ³rico (solo Ãºltimo valor)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ HTTPS GET /stats/summary
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  kubelet (cada Worker Node)                              â”‚
â”‚  - Lee cgroups del kernel                                â”‚
â”‚  - MÃ©tricas en tiempo real                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ejemplo de instalaciÃ³n inline**:

```bash
# Instalar Metrics Server
$ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Verificar instalaciÃ³n
$ kubectl get deployment metrics-server -n kube-system
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
metrics-server   1/1     1            1           2m

# Esperar a que estÃ© listo
$ kubectl wait --for=condition=available --timeout=300s \
  deployment/metrics-server -n kube-system

# Usar mÃ©tricas
$ kubectl top nodes
NAME         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
master-1     250m         12%    2048Mi          25%
worker-1     150m         7%     1536Mi          19%
worker-2     180m         9%     1792Mi          22%

$ kubectl top pods
NAME                    CPU(cores)   MEMORY(bytes)
nginx-deployment-abc    10m          64Mi
backend-service-xyz     25m          128Mi
```

**ConfiguraciÃ³n de Metrics Server**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-server
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
  template:
    metadata:
      labels:
        k8s-app: metrics-server
    spec:
      containers:
      - name: metrics-server
        image: registry.k8s.io/metrics-server/metrics-server:v0.6.4
        args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP
        - --kubelet-use-node-status-port
        - --metric-resolution=15s  # Frecuencia de scraping
```

**Uso con HPA (Horizontal Pod Autoscaler)**:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Escalar si CPU > 70%
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # Escalar si RAM > 80%
```

**ğŸ“ Ver configuraciÃ³n de Metrics Server y HPA:** [`ejemplos/05-addons/metrics-server.yaml`](./ejemplos/05-addons/metrics-server.yaml)

### 6.3 Kubernetes Dashboard - UI Web

Dashboard proporciona una interfaz grÃ¡fica para gestionar recursos del cluster.

**InstalaciÃ³n**:

```bash
# Instalar Dashboard
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

# Crear usuario admin
$ kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard
$ kubectl create clusterrolebinding dashboard-admin \
  --clusterrole=cluster-admin \
  --serviceaccount=kubernetes-dashboard:dashboard-admin

# Obtener token
$ kubectl create token dashboard-admin -n kubernetes-dashboard
eyJhbGciOiJSUzI1NiIsImtpZCI6IjRyV...

# Acceder via proxy
$ kubectl proxy
Starting to serve on 127.0.0.1:8001

# URL: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
```

**Funcionalidades del Dashboard**:
- Ver/editar Deployments, Pods, Services
- Ver logs de contenedores
- Ejecutar shell en pods
- Ver eventos del cluster
- Monitorear recursos (si Metrics Server estÃ¡ instalado)

**âš ï¸ Consideraciones de seguridad**:
```yaml
# NO usar cluster-admin en producciÃ³n
# Crear ServiceAccount con permisos limitados

apiVersion: v1
kind: ServiceAccount
metadata:
  name: dashboard-viewer
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dashboard-viewer
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view  # Solo lectura
subjects:
- kind: ServiceAccount
  name: dashboard-viewer
  namespace: kubernetes-dashboard
```

**ğŸ“ Ver configuraciÃ³n segura de Dashboard:** [`ejemplos/05-addons/dashboard-secure.yaml`](./ejemplos/05-addons/dashboard-secure.yaml)

### 6.4 CNI Plugins - Networking

Container Network Interface (CNI) plugins proporcionan networking entre pods.

**Opciones populares**:

| Plugin | CaracterÃ­sticas | Mejor para |
|--------|----------------|------------|
| **Calico** | Network policies, BGP routing | ProducciÃ³n, seguridad |
| **Flannel** | Simple, overlay VXLAN | Dev, clusters pequeÃ±os |
| **Cilium** | eBPF, observabilidad avanzada | Performance, seguridad |
| **Weave Net** | Mesh encryption | Multi-cloud |

**Ejemplo: InstalaciÃ³n de Calico**:

```bash
# Instalar Calico
$ kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

# Verificar pods de calico
$ kubectl get pods -n kube-system -l k8s-app=calico-node
NAME                READY   STATUS    RESTARTS   AGE
calico-node-abc     1/1     Running   0          2m
calico-node-xyz     1/1     Running   0          2m
calico-node-def     1/1     Running   0          2m

# Verificar networking
$ kubectl get pods -o wide
NAME         READY   STATUS    IP            NODE
frontend     1/1     Running   10.244.1.5    worker-1
backend      1/1     Running   10.244.2.8    worker-2

# Desde frontend, hacer ping a backend
$ kubectl exec frontend -- ping -c 2 10.244.2.8
PING 10.244.2.8 (10.244.2.8): 56 data bytes
64 bytes from 10.244.2.8: seq=0 ttl=62 time=0.5 ms
64 bytes from 10.244.2.8: seq=1 ttl=62 time=0.4 ms
```

**Network Policies con Calico**:

```yaml
# Ejemplo: Denegar todo el trÃ¡fico excepto desde frontend
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-policy
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080
```

**ğŸ“ Ver comparativa de CNI plugins:** [`ejemplos/05-addons/cni-plugins.yaml`](./ejemplos/05-addons/cni-plugins.yaml)

---

## ğŸ¯ Conclusiones y Resumen del MÃ³dulo

### ğŸ“š Conceptos Clave Dominados

Has completado un recorrido exhaustivo por la arquitectura de Kubernetes. Estos son los conceptos fundamentales que ahora dominas:

#### 1. **Arquitectura del Cluster**
- SeparaciÃ³n clara entre **Control Plane** (gestiÃ³n) y **Worker Nodes** (ejecuciÃ³n)
- Arquitectura distribuida y resiliente
- ComunicaciÃ³n centralizada a travÃ©s del API Server

#### 2. **Control Plane Components**
- **API Server**: Gateway Ãºnico para todas las operaciones
- **etcd**: Base de datos distribuida con consistencia RAFT
- **Scheduler**: Algoritmo inteligente de placement
- **Controller Manager**: Reconciliation loops para estado deseado

#### 3. **Worker Node Components**
- **kubelet**: Agente que asegura que contenedores estÃ¡n corriendo
- **kube-proxy**: Networking y balanceo de carga
- **Container Runtime**: Interfaz CRI con containerd/CRI-O

#### 4. **High Availability**
- Multi-master setup con etcd clustering
- Load Balancing de Control Plane
- Backup y restore procedures
- Tolerancia a fallos con quorum

#### 5. **Cluster Addons**
- **CoreDNS**: Service discovery interno
- **Metrics Server**: Autoscaling y monitoreo
- **CNI Plugins**: Networking flexible
- **Dashboard**: GestiÃ³n visual

---

### âœ… Checklist de VerificaciÃ³n

Antes de pasar al siguiente mÃ³dulo, asegÃºrate de poder responder **SÃ** a todas estas preguntas:

#### Arquitectura General
- [ ] Â¿Puedo explicar la diferencia entre Control Plane y Data Plane?
- [ ] Â¿Entiendo por quÃ© el API Server es el componente central?
- [ ] Â¿Conozco el flujo completo desde `kubectl apply` hasta la ejecuciÃ³n?

#### Control Plane
- [ ] Â¿SÃ© cÃ³mo el Scheduler selecciona nodos para los pods?
- [ ] Â¿Entiendo quÃ© es etcd y por quÃ© es crÃ­tico?
- [ ] Â¿Puedo explicar quÃ© es un reconciliation loop?
- [ ] Â¿SÃ© hacer backup y restore de etcd?

#### Worker Nodes
- [ ] Â¿Entiendo cÃ³mo kubelet gestiona el ciclo de vida de pods?
- [ ] Â¿Conozco las diferencias entre iptables e IPVS en kube-proxy?
- [ ] Â¿SÃ© usar crictl para debugging de contenedores?
- [ ] Â¿Puedo diagnosticar problemas de networking en nodos?

#### High Availability
- [ ] Â¿SÃ© configurar un cluster multi-master?
- [ ] Â¿Entiendo cÃ³mo funciona el quorum de etcd?
- [ ] Â¿Puedo explicar el proceso de elecciÃ³n de lÃ­der?
- [ ] Â¿Conozco estrategias de disaster recovery?

#### Addons y Networking
- [ ] Â¿Entiendo cÃ³mo funciona DNS en Kubernetes?
- [ ] Â¿SÃ© troubleshootear Services que no responden?
- [ ] Â¿Puedo configurar el Metrics Server para HPA?
- [ ] Â¿Conozco las diferencias entre CNI plugins?

---

### ğŸ› ï¸ Habilidades PrÃ¡cticas Adquiridas

DespuÃ©s de completar los laboratorios, ahora puedes:

1. **Explorar un Cluster**
   - Inspeccionar componentes del Control Plane
   - Analizar estado de Worker Nodes
   - Verificar comunicaciÃ³n entre componentes

2. **Operar el Control Plane**
   - Interactuar con el API Server vÃ­a REST
   - Realizar backup y restore de etcd
   - Analizar decisiones del Scheduler
   - Debuggear reconciliation loops

3. **Gestionar Worker Nodes**
   - Configurar kubelet para diferentes escenarios
   - Cambiar entre modos iptables e IPVS
   - Usar crictl para inspeccionar contenedores
   - Analizar cgroups y namespaces

4. **Troubleshooting Avanzado**
   - Diagnosticar Services que no responden
   - Resolver problemas de DNS
   - Usar tcpdump para anÃ¡lisis de trÃ¡fico
   - Debuggear NetworkPolicies

---

### ğŸ“Š Mapa Mental de la Arquitectura

```
Kubernetes Cluster
â”‚
â”œâ”€â”€ Control Plane (Master Nodes)
â”‚   â”‚
â”‚   â”œâ”€â”€ API Server (:6443)
â”‚   â”‚   â”œâ”€â”€ REST API
â”‚   â”‚   â”œâ”€â”€ Watch API
â”‚   â”‚   â””â”€â”€ Authentication/Authorization
â”‚   â”‚
â”‚   â”œâ”€â”€ etcd (RAFT)
â”‚   â”‚   â”œâ”€â”€ Key-Value Store
â”‚   â”‚   â”œâ”€â”€ Distributed Consensus
â”‚   â”‚   â””â”€â”€ Backup/Restore
â”‚   â”‚
â”‚   â”œâ”€â”€ Scheduler
â”‚   â”‚   â”œâ”€â”€ Node Selection
â”‚   â”‚   â”œâ”€â”€ Resource Awareness
â”‚   â”‚   â””â”€â”€ Affinity/Taints
â”‚   â”‚
â”‚   â””â”€â”€ Controller Manager
â”‚       â”œâ”€â”€ ReplicaSet Controller
â”‚       â”œâ”€â”€ Deployment Controller
â”‚       â”œâ”€â”€ Node Controller
â”‚       â””â”€â”€ ... (50+ controllers)
â”‚
â””â”€â”€ Worker Nodes
    â”‚
    â”œâ”€â”€ kubelet
    â”‚   â”œâ”€â”€ Pod Lifecycle
    â”‚   â”œâ”€â”€ Health Probes
    â”‚   â””â”€â”€ CRI Interface
    â”‚
    â”œâ”€â”€ kube-proxy
    â”‚   â”œâ”€â”€ Service Abstraction
    â”‚   â”œâ”€â”€ Load Balancing
    â”‚   â””â”€â”€ iptables/IPVS
    â”‚
    â”œâ”€â”€ Container Runtime
    â”‚   â”œâ”€â”€ containerd/CRI-O
    â”‚   â”œâ”€â”€ runc (OCI)
    â”‚   â””â”€â”€ Namespaces/Cgroups
    â”‚
    â””â”€â”€ Addons
        â”œâ”€â”€ CoreDNS (DNS)
        â”œâ”€â”€ CNI Plugin (Networking)
        â””â”€â”€ Metrics Server
```

---

### ğŸ” Comandos Esenciales para Recordar

```bash
# Control Plane
kubectl get componentstatuses
kubectl get --raw /api/v1
etcdctl snapshot save backup.db
kubectl describe node <node> | grep -A 5 "Allocated resources"

# Worker Nodes
kubectl get nodes -o wide
kubectl describe node <node>
crictl ps
crictl images

# Networking
kubectl get svc
kubectl get endpoints
kubectl exec -it <pod> -- nslookup kubernetes
tcpdump -i any -n port 80

# Addons
kubectl get pods -n kube-system
kubectl top nodes
kubectl top pods
kubectl logs -n kube-system <coredns-pod>

# Troubleshooting
kubectl get events --sort-by='.lastTimestamp'
kubectl logs <pod> --previous
kubectl debug node/<node> -it --image=ubuntu
kubectl run netshoot --rm -it --image=nicolaka/netshoot
```

---

### ğŸ“– Recursos de Referencia RÃ¡pida

#### DocumentaciÃ³n Oficial
- [Kubernetes Components](https://kubernetes.io/docs/concepts/overview/components/)
- [Cluster Architecture](https://kubernetes.io/docs/concepts/architecture/)
- [etcd Documentation](https://etcd.io/docs/)
- [Kubernetes API Reference](https://kubernetes.io/docs/reference/kubernetes-api/)

#### GuÃ­as de Troubleshooting
- [Debug Services](https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/)
- [Debug Pods](https://kubernetes.io/docs/tasks/debug/debug-application/debug-pods/)
- [Troubleshoot Clusters](https://kubernetes.io/docs/tasks/debug/debug-cluster/)

#### Herramientas
- **crictl**: Container Runtime CLI
- **etcdctl**: etcd command-line tool
- **kubectl debug**: Ephemeral containers
- **netshoot**: Network troubleshooting container

---

### ğŸš¨ Problemas Comunes y Soluciones (En Minikube)

#### "Pods no arrancan en mi cluster"
```bash
# 1. Verificar scheduler
kubectl get pods -n kube-system | grep scheduler

# 2. Ver eventos
kubectl get events --sort-by='.lastTimestamp' | tail

# 3. Verificar recursos del nodo Minikube
kubectl describe node minikube | grep -A 5 "Allocated"

# 4. Ver logs del scheduler
kubectl logs -n kube-system kube-scheduler-minikube
```

#### "Services no responden"
```bash
# 1. Verificar endpoints
kubectl get endpoints <service>

# 2. Comparar labels
kubectl get svc <service> -o jsonpath='{.spec.selector}'
kubectl get pods -l <selector> -o jsonpath='{.items[0].metadata.labels}'

# 3. Probar conectividad directa al pod
kubectl exec -it <test-pod> -- curl http://<pod-ip>:<port>

# 4. Verificar desde dentro de Minikube
minikube ssh
curl <service-cluster-ip>:<port>
```

#### "DNS no funciona"
```bash
# 1. Verificar CoreDNS en Minikube
kubectl get pods -n kube-system -l k8s-app=kube-dns

# 2. Verificar Service
kubectl get svc -n kube-system kube-dns

# 3. Test directo
kubectl run test --rm -it --image=busybox -- nslookup kubernetes

# 4. Reiniciar CoreDNS si es necesario
kubectl rollout restart deployment coredns -n kube-system
```

#### "Minikube no inicia o se queda colgado"
```bash
# Ver logs de Minikube
minikube logs

# Reiniciar completamente
minikube stop
minikube delete
minikube start --driver=docker

# Verificar recursos de la VM
minikube ssh
df -h  # Espacio en disco
free -h  # Memoria
```

---

### ğŸ“ PreparaciÃ³n para el Siguiente MÃ³dulo

**MÃ³dulo 03: OperaciÃ³n y Seguridad** se enfocarÃ¡ en:

1. **RBAC (Role-Based Access Control)**
   - Users, Groups, ServiceAccounts
   - Roles y ClusterRoles
   - RoleBindings y ClusterRoleBindings

2. **Security Best Practices**
   - Pod Security Standards
   - Network Policies
   - Secrets Management
   - Security Contexts

3. **Resource Management**
   - Resource Quotas
   - LimitRanges
   - Priority Classes

4. **Operaciones con Minikube**
   - Minikube Addons
   - Persistent Volumes en Minikube
   - Acceso a Services desde el host
   - Disaster Recovery

**Pre-requisitos para MÃ³dulo 03:**
- âœ… ComprensiÃ³n sÃ³lida de la arquitectura (este mÃ³dulo)
- âœ… Capacidad de usar kubectl con confianza
- âœ… Minikube funcionando correctamente
- âœ… Familiaridad con pods, services y deployments bÃ¡sicos

---

### ğŸ’¡ Mejores PrÃ¡cticas Aprendidas

#### Trabajando con Minikube
- âœ“ **Usa driver Docker** para mejor compatibilidad
- âœ“ **Asigna recursos adecuados**: `minikube start --cpus=2 --memory=4096`
- âœ“ **Habilita addons necesarios**: `minikube addons enable metrics-server`
- âœ“ **Usa `minikube ssh`** para debugging avanzado dentro del nodo

#### OperaciÃ³n del Cluster
- âœ“ **Monitorea recursos**: `kubectl top nodes` y `kubectl top pods`
- âœ“ **Establece Resource Limits en todos los pods**
- âœ“ **Usa Health Probes** (liveness, readiness, startup)
- âœ“ **Consulta eventos regularmente**: `kubectl get events --sort-by='.lastTimestamp'`

#### Troubleshooting
- âœ“ **Empieza siempre por los eventos**: `kubectl get events`
- âœ“ **Verifica endpoints antes de culpar a DNS**
- âœ“ **Usa `kubectl describe`** para ver detalles completos
- âœ“ **Revisa logs de componentes del sistema**: `kubectl logs -n kube-system`

#### Conceptos de ProducciÃ³n (para el futuro)
- âœ“ **En producciÃ³n, usa managed Kubernetes** (AKS, EKS, GKE)
- âœ“ **Nunca expongas API Server sin autenticaciÃ³n**
- âœ“ **Usa RBAC para todos los usuarios**
- âœ“ **Implementa Network Policies restrictivas**
- âœ“ **Automatiza backups de estado crÃ­tico**

---

### ğŸ† Logros Desbloqueados

âœ… **Arquitecto de Kubernetes**: Comprendes todos los componentes del cluster

âœ… **Operador del Control Plane**: Puedes gestionar API Server, etcd, Scheduler

âœ… **Especialista en Worker Nodes**: Dominas kubelet, kube-proxy, container runtime

âœ… **Troubleshooter Experto**: Sabes diagnosticar problemas de networking y Services

âœ… **HA Master**: Puedes diseÃ±ar e implementar clusters de alta disponibilidad

---

### ğŸ“ Ejercicio Final de AutoevaluaciÃ³n

Antes de continuar, intenta responder sin consultar:

1. Â¿QuÃ© sucede cuando ejecutas `kubectl apply -f deployment.yaml`? (describe cada componente involucrado)

2. Â¿CÃ³mo decide el Scheduler en quÃ© nodo colocar un pod?

3. Â¿QuÃ© diferencia hay entre iptables mode e IPVS mode en kube-proxy?

4. Â¿CÃ³mo aseguras que un cluster de 5 nodos etcd puede tolerar 2 fallos?

5. Â¿Por quÃ© un Service puede tener ClusterIP pero no endpoints?

**Respuestas esperadas:**
- Detalladas, con referencia a componentes especÃ­ficos
- MenciÃ³n de flujos de comunicaciÃ³n
- Consideraciones de HA y troubleshooting

---

### ğŸ¯ PrÃ³ximos Pasos Recomendados

1. **PrÃ¡ctica Continua en Minikube**
   - Repite los laboratorios en tu VM de Azure
   - Experimenta creando y eliminando recursos
   - Practica troubleshooting intencionalmente (elimina pods, simula fallos)

2. **ProfundizaciÃ³n**
   - Explora la [documentaciÃ³n oficial de Kubernetes](https://kubernetes.io/docs/)
   - Sigue el blog de Kubernetes para novedades
   - Ãšnete a comunidades en espaÃ±ol de Kubernetes

3. **PreparaciÃ³n para ProducciÃ³n (futuro)**
   - Considera preparar **CKA** (Certified Kubernetes Administrator)
   - Explora managed Kubernetes (AKS en Azure)
   - Aprende sobre GitOps (ArgoCD, Flux)

4. **Comunidad**
   - Ãšnete a [Kubernetes Slack](https://slack.k8s.io/)
   - Participa en meetups locales de Cloud Native
   - Comparte tu aprendizaje en LinkedIn/Twitter

---

## ğŸ™ Agradecimientos

Has completado el **MÃ³dulo 02: Arquitectura del Cluster de Kubernetes**.

Este mÃ³dulo te ha proporcionado las bases sÃ³lidas necesarias para:
- Entender cÃ³mo funciona Kubernetes internamente
- Trabajar con confianza en Minikube
- Diagnosticar y resolver problemas bÃ¡sicos
- Prepararte para conceptos avanzados de operaciÃ³n y seguridad
- Comprender la diferencia entre entornos de aprendizaje y producciÃ³n

**Entorno de Trabajo**:
- âœ… Minikube con driver Docker en VM Ubuntu (Azure)
- âœ… Cluster single-node ideal para aprendizaje
- âœ… Todos los componentes del Control Plane accesibles para inspecciÃ³n
- âœ… Conceptos de HA y clustering comprendidos (sin implementaciÃ³n prÃ¡ctica)

**Â¡Felicitaciones por tu dedicaciÃ³n y esfuerzo! ğŸ‰**

---

**Siguiente:** [MÃ³dulo 03 - OperaciÃ³n y Seguridad](../area-3-operacion-seguridad/README.md)

---

*Ãšltima actualizaciÃ³n: Noviembre 2025*  
*Curso: Kubernetes de Fundamentos a ProducciÃ³n*  
*Ãrea 2: Arquitectura de Kubernetes*  
*Entorno: Minikube + Docker en Azure VM*
