# ============================================================================
# KUBERNETES ALTA DISPONIBILIDAD - CONFIGURACIÃ“N COMPLETA
# ============================================================================
# Setup de cluster HA con 3 masters, etcd externo, y Load Balancer
# ============================================================================

# ğŸ”§ IMPORTANTE - PRODUCCIÃ“N / CONCEPTUAL EN MINIKUBE:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Este archivo es para APRENDIZAJE de conceptos de Alta Disponibilidad.
# 
# âŒ NO SE PUEDE EJECUTAR EN MINIKUBE porque:
# â€¢ Minikube es single-node (solo 1 nodo)
# â€¢ No tiene mÃºltiples masters
# â€¢ No usa HAProxy como Load Balancer
# â€¢ etcd corre integrado, no como cluster externo
# 
# âœ… USO EN MINIKUBE:
# â€¢ Estudiar los conceptos de HA
# â€¢ Entender la arquitectura de producciÃ³n
# â€¢ Prepararse para clusters reales (AKS, EKS, GKE)
# 
# Para ver cÃ³mo Minikube gestiona el Control Plane:
# $ kubectl get pods -n kube-system
# $ kubectl get nodes
# 
# Para simular HA en desarrollo, considera:
# â€¢ kind (Kubernetes in Docker) - soporta multi-node
# â€¢ kubeadm en VMs separadas
# â€¢ Clusters gestionados en cloud (AKS, EKS, GKE)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

---
# ============================================================================
# PASO 1: CONFIGURAR LOAD BALANCER (HAProxy)
# ============================================================================
# 
# NOTA: HAProxy usa su propio formato de configuraciÃ³n
# El archivo se encuentra en: /etc/haproxy/haproxy.cfg
#
# ConfiguraciÃ³n mÃ­nima necesaria:
#
# Frontend para API Server (puerto 6443):
#   - Bind en *:6443
#   - Mode TCP
#   - Backend k8s-api-backend
#
# Backend con 3 masters:
#   - server master-1 192.168.1.10:6443 check
#   - server master-2 192.168.1.11:6443 check
#   - server master-3 192.168.1.12:6443 check
#   - Balance roundrobin
#   - Health checks habilitados
#
# Stats (opcional):
#   - Bind *:9000
#   - URI /stats
#
# Comandos importantes:
# $ sudo systemctl restart haproxy
# $ sudo systemctl enable haproxy
# $ sudo systemctl status haproxy
# $ curl http://localhost:9000/stats

---
# ============================================================================
# PASO 2: CONFIGURACIÃ“N DE KUBEADM PARA HA
# ============================================================================
# Archivo: kubeadm-config.yaml

apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONTROL PLANE ENDPOINT (Load Balancer VIP)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
controlPlaneEndpoint: "192.168.1.100:6443"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ETCD EXTERNO (3 nodos dedicados)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
etcd:
  external:
    endpoints:
    - https://192.168.1.20:2379
    - https://192.168.1.21:2379
    - https://192.168.1.22:2379
    caFile: /etc/kubernetes/pki/etcd/ca.crt
    certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
    keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NETWORKING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/12"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API SERVER CONFIGURACIÃ“N
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
apiServer:
  extraArgs:
    authorization-mode: "Node,RBAC"
    enable-admission-plugins: "NodeRestriction,PodSecurityPolicy"
    audit-log-path: "/var/log/kubernetes/audit.log"
    audit-log-maxage: "30"
    audit-log-maxbackup: "10"
  certSANs:
  - "192.168.1.100"  # Load Balancer VIP
  - "kubernetes"
  - "kubernetes.default"
  - "kubernetes.default.svc"
  - "kubernetes.default.svc.cluster.local"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SCHEDULER Y CONTROLLER MANAGER (Leader Election)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
scheduler:
  extraArgs:
    leader-elect: "true"
    leader-elect-lease-duration: "15s"
    leader-elect-renew-deadline: "10s"
    leader-elect-retry-period: "2s"

controllerManager:
  extraArgs:
    leader-elect: "true"
    leader-elect-lease-duration: "15s"
    leader-elect-renew-deadline: "10s"
    leader-elect-retry-period: "2s"

---
# ============================================================================
# PASO 3: INICIALIZAR PRIMER MASTER
# ============================================================================

# En master-1 (192.168.1.10)
# $ sudo kubeadm init --config=kubeadm-config.yaml --upload-certs

# Output esperado:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Your Kubernetes control-plane has initialized successfully!
# 
# To start using your cluster, you need to run the following:
#   mkdir -p $HOME/.kube
#   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
#   sudo chown $(id -u):$(id -g) $HOME/.kube/config
# 
# You can now join any number of control-plane nodes by running:
#   kubeadm join 192.168.1.100:6443 --token abc123.xyz789 \
#     --discovery-token-ca-cert-hash sha256:1234567890abcdef... \
#     --control-plane --certificate-key fedcba0987654321...
# 
# Join worker nodes:
#   kubeadm join 192.168.1.100:6443 --token abc123.xyz789 \
#     --discovery-token-ca-cert-hash sha256:1234567890abcdef...
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Configurar kubectl en master-1
# $ mkdir -p $HOME/.kube
# $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
# $ sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Instalar CNI (Calico ejemplo)
# $ kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

---
# ============================================================================
# PASO 4: UNIR MASTERS ADICIONALES
# ============================================================================

# En master-2 (192.168.1.11)
# $ sudo kubeadm join 192.168.1.100:6443 \
#     --token abc123.xyz789 \
#     --discovery-token-ca-cert-hash sha256:1234567890abcdef... \
#     --control-plane \
#     --certificate-key fedcba0987654321...

# En master-3 (192.168.1.12)
# $ sudo kubeadm join 192.168.1.100:6443 \
#     --token abc123.xyz789 \
#     --discovery-token-ca-cert-hash sha256:1234567890abcdef... \
#     --control-plane \
#     --certificate-key fedcba0987654321...

# âš ï¸ NOTA: certificate-key expira en 2 horas
# Si expira, regenerar:
# $ sudo kubeadm init phase upload-certs --upload-certs

---
# ============================================================================
# PASO 5: VERIFICAR CLUSTER HA
# ============================================================================

# Ver todos los nodos control plane
# $ kubectl get nodes
# NAME       STATUS   ROLES           AGE   VERSION
# master-1   Ready    control-plane   10m   v1.28.0
# master-2   Ready    control-plane   5m    v1.28.0
# master-3   Ready    control-plane   2m    v1.28.0

# Ver componentes del control plane
# $ kubectl get pods -n kube-system -o wide

# Verificar quiÃ©n es el lÃ­der del Scheduler
# $ kubectl get endpoints kube-scheduler -n kube-system -o yaml
# metadata:
#   annotations:
#     control-plane.alpha.kubernetes.io/leader: |
#       {"holderIdentity":"master-2_abc123...","leaseDurationSeconds":15,...}
#                          â†‘ master-2 es el lÃ­der actual

# Verificar quiÃ©n es el lÃ­der del Controller Manager
# $ kubectl get endpoints kube-controller-manager -n kube-system -o yaml

# Ver eventos de leader election
# $ kubectl get events -n kube-system --sort-by='.lastTimestamp' | grep -i leader

---
# ============================================================================
# PASO 6: PRUEBAS DE ALTA DISPONIBILIDAD
# ============================================================================

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PRUEBA 1: Apagar un API Server
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# En master-1:
# $ sudo systemctl stop kubelet

# Desde otro nodo:
# $ kubectl get nodes
# (DeberÃ­a funcionar - Load Balancer redirige a master-2 o master-3)

# Ver logs de HAProxy
# $ sudo tail -f /var/log/haproxy.log
# Server k8s-api-backend/master-1 is DOWN (health check failed)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PRUEBA 2: Simular fallo del lÃ­der del Scheduler
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Identificar lÃ­der actual
# $ kubectl get endpoints kube-scheduler -n kube-system -o jsonpath='{.metadata.annotations.control-plane\.alpha\.kubernetes\.io/leader}'

# Apagar ese nodo (ejemplo: master-2)
# $ ssh master-2
# $ sudo systemctl stop kubelet

# Verificar nuevo lÃ­der (deberÃ­a cambiar en ~15 segundos)
# $ kubectl get endpoints kube-scheduler -n kube-system -o jsonpath='{.metadata.annotations.control-plane\.alpha\.kubernetes\.io/leader}'
# holderIdentity ahora es master-1 o master-3

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PRUEBA 3: Deployment continÃºa funcionando
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Con master-1 apagado, crear un Deployment
# $ kubectl create deployment nginx --image=nginx --replicas=3

# Verificar que se crea correctamente
# $ kubectl get pods -o wide
# (Pods se crean en workers disponibles)

---
# ============================================================================
# TOPOLOGÃA COMPLETA
# ============================================================================

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚                     LOAD BALANCER                          â”‚
# â”‚                 192.168.1.100:6443                         â”‚
# â”‚                    (HAProxy)                               â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                â”‚              â”‚              â”‚
#     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
#     â”‚   MASTER-1     â”‚ â”‚  MASTER-2  â”‚ â”‚  MASTER-3  â”‚
#     â”‚ 192.168.1.10   â”‚ â”‚192.168.1.11â”‚ â”‚192.168.1.12â”‚
#     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#     â”‚ API Server     â”‚ â”‚API Server  â”‚ â”‚API Server  â”‚
#     â”‚ Scheduler      â”‚ â”‚Scheduler   â”‚ â”‚Scheduler   â”‚
#     â”‚ Ctrl Manager   â”‚ â”‚Ctrl Managerâ”‚ â”‚Ctrl Managerâ”‚
#     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
#              â”‚               â”‚             â”‚
#              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                              â”‚
#              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#              â”‚        etcd CLUSTER           â”‚
#              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#              â”‚ etcd-1: 192.168.1.20:2379     â”‚
#              â”‚ etcd-2: 192.168.1.21:2379     â”‚
#              â”‚ etcd-3: 192.168.1.22:2379     â”‚
#              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
#     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#     â”‚  WORKER-1   â”‚  â”‚  WORKER-2   â”‚  â”‚  WORKER-3   â”‚
#     â”‚192.168.1.30 â”‚  â”‚192.168.1.31 â”‚  â”‚192.168.1.32 â”‚
#     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---
# ============================================================================
# NOTAS IMPORTANTES
# ============================================================================
# 
# âœ“ Load Balancer es CRÃTICO (single point of failure si no estÃ¡ en HA)
# âœ“ etcd externo permite escalar control plane independientemente
# âœ“ Scheduler y Controller Manager usan leader election (solo uno activo)
# âœ“ API Server es stateless (todas las instancias activas)
# âœ“ certificate-key solo vÃ¡lido 2 horas (regenerar si expira)
# âœ“ MÃ­nimo 3 masters para verdadera HA (tolera 1 fallo)
# âœ“ NÃºmero IMPAR de masters (evita split-brain)
# âœ“ controlPlaneEndpoint DEBE apuntar al Load Balancer (no a un master)
# âœ“ Todos los masters deben poder comunicarse con etcd
# âœ“ Workers se conectan SOLO al Load Balancer (no directo a masters)
# 
# RECOVERY:
# â€¢ Si master cae: automatico (LB redirige)
# â€¢ Si lÃ­der cae: leader election (~15s)
# â€¢ Si LB cae: cluster DOWN (usar LB en HA tambiÃ©n)
# â€¢ Si etcd pierde quorum: cluster read-only (restaurar urgente)
# 
# ============================================================================
