# Laboratorio 02: Control Plane PrÃ¡ctico en Minikube

## Objetivos

Al finalizar este laboratorio, serÃ¡s capaz de:
- âœ“ Interactuar directamente con el API Server de Minikube
- âœ“ Realizar backup y restore de etcd en Minikube
- âœ“ Analizar el funcionamiento del Scheduler
- âœ“ Entender el reconciliation loop del Controller Manager
- âœ“ Troubleshooting de componentes del Control Plane

## DuraciÃ³n Estimada

â±ï¸ 90-120 minutos

## Pre-requisitos

- **Minikube** instalado y corriendo con driver Docker
- **VM Ubuntu en Azure** funcionando
- `jq` instalado para parsing de JSON: `sudo apt install jq`
- `kubectl` configurado

## âš ï¸ Nota sobre el Entorno

Este laboratorio explora el Control Plane de Minikube:
- Todos los componentes corren como **contenedores Docker** dentro del nodo Minikube
- El acceso a etcd se hace desde **dentro del contenedor** etcd
- No necesitas acceso SSH a mÃºltiples nodos (todo estÃ¡ en Minikube)
- Los conceptos son idÃ©nticos a un cluster real, solo cambia el mÃ©todo de acceso

---

## Parte 1: InteracciÃ³n con el API Server (30 minutos)

### ğŸ“ Ejercicio 1.1: API Server sin kubectl

**Objetivo:** Entender que `kubectl` es solo un cliente HTTP del API Server.

**Paso 1:** ObtÃ©n el token de autenticaciÃ³n

```bash
# Crear un token temporal para el ServiceAccount default
TOKEN=$(kubectl create token default)

echo $TOKEN
```

**Paso 2:** ObtÃ©n la URL del API Server de Minikube

```bash
# En Minikube, el API Server estÃ¡ en https://192.168.49.2:8443 (o similar)
API_SERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')
echo $API_SERVER

# DeberÃ­a mostrar algo como: https://192.168.49.2:8443
```

**Paso 3:** Realiza una peticiÃ³n HTTP directa

```bash
# Listar namespaces (equivalente a: kubectl get namespaces)
curl -k -H "Authorization: Bearer $TOKEN" \
  $API_SERVER/api/v1/namespaces | jq '.items[].metadata.name'
```

**Pregunta:** Â¿QuÃ© namespaces existen en tu cluster Minikube?

<details>
<summary>ğŸ’¡ Respuesta esperada</summary>
DeberÃ­as ver al menos:
- default
- kube-system
- kube-public
- kube-node-lease
</details>

---

### ğŸ“ Ejercicio 1.2: Explorar la API

**Paso 1:** Lista todas las API versions disponibles

```bash
curl -k -H "Authorization: Bearer $TOKEN" \
  $API_SERVER/apis | jq '.groups[].name'
```

**Paso 2:** Lista recursos en la API core

```bash
curl -k -H "Authorization: Bearer $TOKEN" \
  $API_SERVER/api/v1 | jq '.resources[] | select(.namespaced==true) | .name' | head -10
```

**Paso 3:** ObtÃ©n un pod especÃ­fico vÃ­a API REST

```bash
# Primero crea un pod de prueba
kubectl run api-test --image=nginx

# Espera a que estÃ© listo
kubectl wait --for=condition=Ready pod/api-test --timeout=60s

# ObtÃ©n el pod via API REST
curl -k -H "Authorization: Bearer $TOKEN" \
  $API_SERVER/api/v1/namespaces/default/pods/api-test | jq '.status.phase'
```

**Pregunta:** Â¿QuÃ© fase (phase) estÃ¡ el pod?

<details>
<summary>ğŸ’¡ Respuesta esperada</summary>
DeberÃ­a mostrar: "Running"
</details>

---

### ğŸ“ Ejercicio 1.3: Watch API en AcciÃ³n

**Paso 1:** Abre dos terminales en tu VM de Azure

**Terminal 1:** Inicia un watch de pods

```bash
TOKEN=$(kubectl create token default)
API_SERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')

curl -k -H "Authorization: Bearer $TOKEN" \
  "$API_SERVER/api/v1/namespaces/default/pods?watch=true"
```

**Deja esto corriendo...**

**Terminal 2:** Crea y elimina pods

```bash
kubectl run watch-test-1 --image=nginx
sleep 5
kubectl delete pod watch-test-1

kubectl run watch-test-2 --image=redis
sleep 5
kubectl delete pod watch-test-2
```

**Observa en Terminal 1:** DeberÃ­as ver eventos `ADDED`, `MODIFIED`, `DELETED` en tiempo real.

**Pregunta:** Â¿CuÃ¡ntos eventos ves por cada pod creado? Â¿Por quÃ© hay mÃºltiples eventos `MODIFIED`?

---

### ğŸ“ Ejercicio 1.4: Crear Recurso via API REST

**Paso 1:** Crea un pod usando POST directo

```bash
# Define el pod en JSON
cat > pod-via-api.json <<EOF
{
  "apiVersion": "v1",
  "kind": "Pod",
  "metadata": {
    "name": "created-via-api",
    "labels": {
      "method": "rest-api"
    }
  },
  "spec": {
    "containers": [
      {
        "name": "nginx",
        "image": "nginx",
        "ports": [
          {
            "containerPort": 80
          }
        ]
      }
    ]
  }
}
EOF

# POST al API Server
TOKEN=$(kubectl create token default)
API_SERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')

curl -k -X POST \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d @pod-via-api.json \
  $API_SERVER/api/v1/namespaces/default/pods | jq '.metadata.name'
```

**Paso 2:** Verifica que el pod existe

```bash
kubectl get pod created-via-api
```

**Pregunta:** Â¿QuÃ© ventajas tiene usar `kubectl` sobre llamadas REST directas?

---

## Parte 2: Backup y Restore de etcd en Minikube (35 minutos)

### ğŸ“ Ejercicio 2.1: Snapshot de etcd

**âš ï¸ IMPORTANTE:** En Minikube accederemos a etcd desde dentro del contenedor Docker.

**Paso 1:** Verifica la salud de etcd

```bash
# Acceder al contenedor etcd en Minikube
minikube ssh

# Dentro de Minikube, acceder al contenedor etcd
docker exec -it $(docker ps -qf "name=etcd") sh

# Dentro del contenedor etcd, verificar salud
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/var/lib/minikube/certs/etcd/ca.crt \
  --cert=/var/lib/minikube/certs/etcd/server.crt \
  --key=/var/lib/minikube/certs/etcd/server.key \
  endpoint health

# Salir del contenedor
exit

# Salir de Minikube
exit
```

**Paso 2:** Crea datos de prueba en el cluster

```bash
# Desde tu VM de Azure (fuera de Minikube SSH)
# Crea un namespace con recursos
kubectl create namespace backup-test
kubectl create deployment nginx --image=nginx --replicas=3 -n backup-test
kubectl create configmap test-config --from-literal=key1=value1 -n backup-test
kubectl create secret generic test-secret --from-literal=password=secret123 -n backup-test

# Espera a que estÃ©n listos
kubectl wait --for=condition=Available deployment/nginx -n backup-test --timeout=60s

# Verifica
kubectl get all,configmap,secret -n backup-test
```

**Paso 3:** Toma un snapshot de etcd

```bash
# Acceder a Minikube
minikube ssh

# Dentro de Minikube, acceder al contenedor etcd
docker exec -it $(docker ps -qf "name=etcd") sh

# Dentro del contenedor etcd, crear backup
ETCDCTL_API=3 etcdctl snapshot save /tmp/etcd-backup.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/var/lib/minikube/certs/etcd/ca.crt \
  --cert=/var/lib/minikube/certs/etcd/server.crt \
  --key=/var/lib/minikube/certs/etcd/server.key

# Verifica el snapshot
ETCDCTL_API=3 etcdctl snapshot status /tmp/etcd-backup.db --write-out=table
```

**Anota el tamaÃ±o del snapshot:** _______ MB

**Pregunta:** Â¿CuÃ¡ntas keys (llaves) hay en etcd?

---

### ğŸ“ Ejercicio 2.2: Simular PÃ©rdida de Datos

**Paso 1:** Elimina el namespace de prueba (simula pÃ©rdida de datos)

```bash
# Desde tu VM de Azure
kubectl delete namespace backup-test

# Verifica que se eliminÃ³
kubectl get namespace backup-test
# DeberÃ­a dar error: "not found"
```

**Paso 2:** Crea otro recurso que NO queremos conservar

```bash
kubectl create namespace temporal
kubectl run unwanted-pod --image=nginx -n temporal

# Verifica que existe
kubectl get pod -n temporal
```

---

### ğŸ“ Ejercicio 2.3: Restore desde Snapshot (Conceptual)

**âš ï¸ IMPORTANTE:** El restore completo de etcd en Minikube requiere reiniciar todo el cluster y puede ser complejo. 

**Concepto clave**: En producciÃ³n, el proceso serÃ­a:
1. Detener API Server
2. Restore del snapshot de etcd
3. Reiniciar componentes

**Para Minikube**, en lugar de hacer un restore real (que podrÃ­a romper el cluster), vamos a:

**Paso 1:** Copiar el snapshot fuera de Minikube (para prÃ¡ctica)

```bash
# Desde Minikube SSH (dentro del contenedor etcd)
# Ya tenemos el snapshot en /tmp/etcd-backup.db

# Salir del contenedor y de Minikube
exit  # Sale del contenedor etcd
exit  # Sale de Minikube SSH

# Desde la VM de Azure, copiar el snapshot
minikube cp minikube:/tmp/etcd-backup.db ./etcd-backup-minikube.db

# Verificar
ls -lh etcd-backup-minikube.db
```

**Paso 2:** Entender el proceso de restore (REFERENCIA - NO ejecutar)

```bash
# EJEMPLO TEÃ“RICO - SOLO PARA COMPRENSIÃ“N
# En un cluster real harÃ­as:

# 1. Detener API Server y etcd
# sudo mv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/
# sudo mv /etc/kubernetes/manifests/etcd.yaml /tmp/

# 2. Backup del directorio actual
# sudo mv /var/lib/etcd /var/lib/etcd.backup

# 3. Restore desde snapshot
# sudo ETCDCTL_API=3 etcdctl snapshot restore /tmp/etcd-backup.db \
#   --data-dir=/var/lib/etcd

# 4. Reiniciar componentes
# sudo mv /tmp/etcd.yaml /etc/kubernetes/manifests/
# sudo mv /tmp/kube-apiserver.yaml /etc/kubernetes/manifests/
```

**Paso 3:** VerificaciÃ³n de conceptos

**Pregunta:** Â¿Por quÃ© NO hacemos el restore real en Minikube?

<details>
<summary>ğŸ’¡ Respuesta</summary>
- Minikube gestiona su propia infraestructura de forma especial
- Un restore manual podrÃ­a romper el cluster
- En producciÃ³n usarÃ­as managed Kubernetes (AKS) con backups automÃ¡ticos
- El objetivo es entender el CONCEPTO, no romper nuestro entorno de prÃ¡ctica
</details>

**Paso 4:** Restaura el namespace manualmente (simulaciÃ³n)

```bash
# Como no hicimos restore real, volvemos a crear los recursos
# En producciÃ³n, esto vendrÃ­a del backup de etcd automÃ¡ticamente

kubectl create namespace backup-test
kubectl create deployment nginx --image=nginx --replicas=3 -n backup-test
kubectl create configmap test-config --from-literal=key1=value1 -n backup-test
kubectl create secret generic test-secret --from-literal=password=secret123 -n backup-test

# Verifica
kubectl get all,configmap,secret -n backup-test
# DeberÃ­a dar error "not found" (no existÃ­a en el snapshot)
```

**Pregunta:** Â¿Por quÃ© el namespace `temporal` no existe despuÃ©s del restore?

<details>
<summary>ğŸ’¡ Respuesta</summary>
Porque el snapshot se tomÃ³ ANTES de crear el namespace temporal. El restore volviÃ³ el cluster al estado exacto de cuando se tomÃ³ el snapshot.
</details>

---

## Parte 3: Scheduler en AcciÃ³n (25 minutos)

### ğŸ“ Ejercicio 3.1: Observar Decisiones del Scheduler

**Paso 1:** Crea un pod sin especificar nodo

```bash
# Desde tu VM de Azure
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: scheduler-test
spec:
  containers:
  - name: nginx
    image: nginx
EOF
```

**Paso 2:** Observa el evento de scheduling

```bash
kubectl get events --field-selector involvedObject.name=scheduler-test

# Busca el evento: "Successfully assigned default/scheduler-test to minikube"
```

**Paso 3:** Verifica la asignaciÃ³n

```bash
kubectl get pod scheduler-test -o wide

# Observa la columna NODE - deberÃ­a mostrar "minikube"
```

**Pregunta:** Â¿A quÃ© nodo asignÃ³ el pod? Â¿Por quÃ© siempre es el mismo nodo?

<details>
<summary>ğŸ’¡ Respuesta</summary>
En Minikube (single-node), todos los pods se asignan al nodo "minikube" porque es el Ãºnico disponible. En un cluster multi-nodo, el Scheduler elegirÃ­a basÃ¡ndose en recursos disponibles, taints, tolerations, affinity, etc.
</details>

---

### ğŸ“ Ejercicio 3.2: Node Selector

**Paso 1:** Etiqueta el nodo

```bash
# En Minikube solo tenemos un nodo, pero podemos practicar el concepto
kubectl label node minikube disktype=ssd

# Verifica la etiqueta
kubectl get nodes --show-labels | grep disktype
```

**Paso 2:** Crea un pod con nodeSelector

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: ssd-pod
spec:
  nodeSelector:
    disktype: ssd
  containers:
  - name: nginx
    image: nginx
EOF
```

**Paso 3:** Verifica que se asignÃ³ correctamente

```bash
kubectl get pod ssd-pod -o wide

# Verifica el campo NODE
kubectl describe pod ssd-pod | grep "Node:"
kubectl describe pod ssd-pod | grep "Node-Selectors:"
```

**Pregunta:** Â¿QuÃ© pasarÃ­a si crearas un nodeSelector con una etiqueta que no existe?

<details>
<summary>ğŸ’¡ PruÃ©balo</summary>

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: unschedulable-pod
spec:
  nodeSelector:
    disktype: nvme  # Etiqueta que NO existe
  containers:
  - name: nginx
    image: nginx
EOF

# Observa el estado
kubectl get pod unschedulable-pod
kubectl describe pod unschedulable-pod | grep -A 3 "Events:"
# DeberÃ­a mostrar: "0/1 nodes are available: 1 node(s) didn't match Pod's node affinity/selector"
```
</details>

---

### ğŸ“ Ejercicio 3.3: Pod con Recursos Grandes (LÃ­mites del Scheduler)

**Paso 1:** Verifica los recursos disponibles en Minikube

```bash
# Desde tu VM de Azure
kubectl describe node minikube | grep -A 5 "Allocatable:"

# Anota los valores de cpu y memory disponibles
```

**Paso 2:** Crea un pod que requiere recursos imposibles

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: huge-pod
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        cpu: "1000"  # 1000 cores - imposible en Minikube!
        memory: "1000Gi"
EOF
```

**Paso 3:** Observa por quÃ© no se puede programar

```bash
kubectl get pod huge-pod

# DeberÃ­a mostrar estado "Pending"

kubectl describe pod huge-pod | grep -A 10 "Events:"
```

**Pregunta:** Â¿QuÃ© mensaje de error ves del Scheduler?

<details>
<summary>ğŸ’¡ Respuesta esperada</summary>
DeberÃ­as ver algo como:
```
Warning  FailedScheduling  ... 0/1 nodes are available: 1 Insufficient cpu, 1 Insufficient memory
```
El Scheduler no puede encontrar un nodo con suficientes recursos.
</details>

**Paso 4:** Limpieza

```bash
kubectl delete pod huge-pod
```

---

### ğŸ“ Ejercicio 3.4: Manual Scheduling (Sin usar el Scheduler)

**Paso 1:** Crea un pod SIN scheduler

```bash
# Desde tu VM de Azure
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: manual-schedule
spec:
  schedulerName: manual-scheduler  # Scheduler que NO existe
  containers:
  - name: nginx
    image: nginx
EOF
```

**Paso 2:** Verifica que estÃ¡ en estado Pending

```bash
kubectl get pod manual-schedule

# STATUS deberÃ­a ser: Pending

kubectl describe pod manual-schedule | grep "Events:"
# DeberÃ­a ver: "FailedScheduling" porque "manual-scheduler" no existe
```

**Paso 3:** Asigna manualmente el pod al nodo

```bash
# Asignar directamente al nodo minikube
kubectl patch pod manual-schedule -p '{"spec":{"nodeName":"minikube"}}'
```

**Paso 4:** Verifica que ahora corre

```bash
kubectl get pod manual-schedule -o wide

# Ahora deberÃ­a estar Running en el nodo minikube
```

**Pregunta:** Â¿Por quÃ© el pod no requiere al Scheduler cuando le asignas `nodeName`?

<details>
<summary>ğŸ’¡ Respuesta</summary>
El campo `nodeName` es el resultado final del proceso de scheduling. Cuando lo asignas manualmente, estÃ¡s "haciendo el trabajo del Scheduler" - le dices a Kubernetes exactamente dÃ³nde correr el pod, saltÃ¡ndote la lÃ³gica de selecciÃ³n automÃ¡tica.
</details>

**Paso 5:** Limpieza

```bash
kubectl delete pod manual-schedule scheduler-test ssd-pod unschedulable-pod --ignore-not-found=true
```

---

## Parte 4: Controller Manager (20 minutos)

### ğŸ“ Ejercicio 4.1: Reconciliation Loop del ReplicaSet Controller

**Paso 1:** Crea un Deployment

```bash
# Desde tu VM de Azure
kubectl create deployment test-reconcile --image=nginx --replicas=3

# Espera que se creen los pods
kubectl wait --for=condition=ready pod -l app=test-reconcile --timeout=60s
```

**Paso 2:** Verifica los pods

```bash
kubectl get pods -l app=test-reconcile

# DeberÃ­as ver 3 pods en estado Running
```

**Paso 3:** Elimina un pod manualmente (simula falla)

```bash
POD=$(kubectl get pods -l app=test-reconcile -o jsonpath='{.items[0].metadata.name}')
echo "Eliminando pod: $POD"

kubectl delete pod $POD
```

**Paso 4:** Observa la reconciliaciÃ³n en tiempo real

```bash
# Ejecuta rÃ¡pidamente despuÃ©s de eliminar
kubectl get pods -l app=test-reconcile -w

# Presiona Ctrl+C despuÃ©s de unos segundos
```

**Pregunta:** Â¿QuÃ© sucede? Â¿CuÃ¡nto tiempo tarda en aparecer un nuevo pod?

<details>
<summary>ğŸ’¡ ExplicaciÃ³n</summary>
El ReplicaSet Controller detecta que hay solo 2 pods (desired: 3, actual: 2) y crea uno nuevo inmediatamente (generalmente en menos de 5 segundos). Esto es el "reconciliation loop" en acciÃ³n - el controller continuamente compara el estado deseado (3 rÃ©plicas) con el estado actual y toma acciones correctivas.
</details>

**Paso 5:** Verifica el evento de creaciÃ³n

```bash
kubectl get events --field-selector reason=SuccessfulCreate | tail -5
```

---

### ğŸ“ Ejercicio 4.2: Node Controller (Conceptual en Minikube)

**âš ï¸ NOTA:** Este ejercicio es conceptual porque en Minikube solo tenemos un nodo. Detenerlo romperÃ¡ todo el cluster.

**Concepto a entender:**

En un cluster multi-nodo, el Node Controller:
1. Monitorea el estado de cada nodo via heartbeats
2. Si un nodo no responde por 40 segundos â†’ marca como "NotReady"
3. Si estÃ¡ NotReady por 5 minutos â†’ evict pods (los elimina y los recrea en otros nodos)

**Paso 1:** Observa el estado del nodo

```bash
# Desde tu VM de Azure
kubectl get nodes

# DeberÃ­a mostrar:
# NAME       STATUS   ROLES           AGE   VERSION
# minikube   Ready    control-plane   Xd    vX.XX.X
```

**Paso 2:** Inspecciona las condiciones del nodo

```bash
kubectl describe node minikube | grep -A 10 "Conditions:"

# Observa las condiciones:
# - MemoryPressure: False
# - DiskPressure: False
# - PIDPressure: False
# - Ready: True
```

**Pregunta:** Â¿QuÃ© pasarÃ­a si "Ready" cambiara a "False"?

<details>
<summary>ğŸ’¡ Respuesta (TeorÃ­a)</summary>
Si el nodo pasa a NotReady:
1. Pods ya existentes siguen corriendo (el container runtime aÃºn funciona)
2. NO se programan nuevos pods en ese nodo
3. DespuÃ©s de 5 minutos, el Node Controller marca los pods para eviction
4. Los pods se recrean en otros nodos (si los hay)

En Minikube (single-node): Si el nodo se cae, todo el cluster se detiene.
</details>

**Paso 3:** Verifica los heartbeats del kubelet

```bash
# ConÃ©ctate a Minikube
minikube ssh

# Dentro de Minikube, verifica el kubelet
sudo systemctl status kubelet | grep "Active:"

# DeberÃ­a mostrar "active (running)"

# Sal de Minikube
exit
```

**REFERENCIA - NO ejecutar:** En un cluster real para simular falla

```bash
# âš ï¸ NO EJECUTAR EN MINIKUBE - SOLO REFERENCIA
# sudo systemctl stop kubelet
# 
# Esto harÃ­a que el nodo pase a NotReady en ~40 segundos
```

---

### ğŸ“ Ejercicio 4.3: Endpoint Controller

**Paso 1:** Crea un Service sin pods

```bash
# Desde tu VM de Azure
kubectl create service clusterip test-endpoints --tcp=80:80
```

**Paso 2:** Verifica los endpoints (deberÃ­an estar vacÃ­os)

```bash
kubectl get endpoints test-endpoints

# DeberÃ­a mostrar:
# NAME              ENDPOINTS   AGE
# test-endpoints    <none>      10s
```

**Paso 3:** Crea pods que coincidan con el selector del Service

```bash
# El Service busca app=test-endpoints por defecto
kubectl run pod1 --image=nginx --labels=app=test-endpoints
kubectl run pod2 --image=nginx --labels=app=test-endpoints
kubectl run pod3 --image=nginx --labels=app=test-endpoints

# Espera que los pods estÃ©n listos
kubectl wait --for=condition=ready pod -l app=test-endpoints --timeout=60s
```

**Paso 4:** Verifica que el Endpoint Controller actualizÃ³ los endpoints

```bash
kubectl get endpoints test-endpoints

# Ahora deberÃ­a mostrar 3 IPs
kubectl get endpoints test-endpoints -o yaml | grep "ip:"
```

**Paso 5:** Compara con las IPs de los pods

```bash
kubectl get pods -l app=test-endpoints -o wide

# Las IPs deberÃ­an coincidir exactamente
```

**Pregunta:** Â¿CuÃ¡ntas IPs ves en los endpoints? Â¿Coinciden con las IPs de los pods?

**Paso 6:** Elimina un pod y observa

```bash
kubectl delete pod pod1

# Verifica inmediatamente
kubectl get endpoints test-endpoints

# DeberÃ­a mostrar solo 2 IPs ahora
```

**Pregunta:** Â¿CuÃ¡nto tiempo tardÃ³ en actualizarse el endpoint?

<details>
<summary>ğŸ’¡ ExplicaciÃ³n</summary>
El Endpoint Controller monitorea continuamente:
- Servicios que necesitan endpoints
- Pods que coinciden con los selectores del Service
- Estado de los pods (Ready/NotReady)

Cuando un pod se crea/elimina o cambia su estado, actualiza los endpoints en segundos.
</details>

**Paso 7:** Limpieza

```bash
kubectl delete svc test-endpoints
kubectl delete deployment test-reconcile
kubectl delete pod pod2 pod3 --ignore-not-found=true
```

---

## Parte 5: Troubleshooting del Control Plane (15 minutos)

### ğŸ“ Ejercicio 5.1: Logs de Componentes

**Paso 1:** Ver logs del API Server

```bash
# Desde tu VM de Azure
kubectl logs -n kube-system kube-apiserver-minikube --tail=50

# Si quieres buscar errores especÃ­ficos:
kubectl logs -n kube-system kube-apiserver-minikube --tail=200 | grep -i error
```

**Paso 2:** Ver logs del Scheduler

```bash
kubectl logs -n kube-system kube-scheduler-minikube --tail=50

# Buscar decisiones de scheduling:
kubectl logs -n kube-system kube-scheduler-minikube --tail=200 | grep -i "successfully assigned"
```

**Paso 3:** Ver logs del Controller Manager

```bash
kubectl logs -n kube-system kube-controller-manager-minikube --tail=50

# Buscar eventos de reconciliaciÃ³n:
kubectl logs -n kube-system kube-controller-manager-minikube --tail=200 | grep -i "scaled"
```

**Paso 4:** Ver logs de etcd

```bash
kubectl logs -n kube-system etcd-minikube --tail=50
```

**Tarea:** Busca en los logs algÃºn mensaje de WARNING o ERROR. Â¿QuÃ© dicen?

<details>
<summary>ğŸ’¡ Tip de bÃºsqueda</summary>

```bash
# Buscar todos los warnings/errors en componentes del Control Plane
for component in kube-apiserver kube-scheduler kube-controller-manager etcd; do
  echo "=== $component ==="
  kubectl logs -n kube-system ${component}-minikube --tail=100 | grep -iE "error|warn"
done
```
</details>

---

### ğŸ“ Ejercicio 5.2: Health Checks

**Paso 1:** Verifica el health del API Server

```bash
# Desde tu VM de Azure
# OpciÃ³n 1: Usando el endpoint de healthz
kubectl get --raw /healthz

# DeberÃ­a retornar: ok

# OpciÃ³n 2: Usando curl
APISERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')
echo "API Server: $APISERVER"

curl -k $APISERVER/healthz
# DeberÃ­a retornar: ok
```

**Paso 2:** Verifica endpoints especÃ­ficos de salud

```bash
# Livez (liveness)
kubectl get --raw /livez?verbose

# Readyz (readiness)
kubectl get --raw /readyz?verbose
```

**Paso 3:** Verifica el health de etcd desde dentro del contenedor

```bash
# ConÃ©ctate a Minikube
minikube ssh

# Ejecuta el comando dentro del contenedor etcd
docker exec -it $(docker ps -q -f "name=k8s_etcd_etcd") sh -c '
  ETCDCTL_API=3 etcdctl \
    --endpoints=https://127.0.0.1:2379 \
    --cacert=/var/lib/minikube/certs/etcd/ca.crt \
    --cert=/var/lib/minikube/certs/etcd/server.crt \
    --key=/var/lib/minikube/certs/etcd/server.key \
    endpoint health
'

# DeberÃ­a mostrar: "127.0.0.1:2379 is healthy"

# Sal de Minikube
exit
```

**Paso 4:** Verifica componentes desde `kubectl`

```bash
# Desde tu VM de Azure
kubectl get componentstatuses 2>/dev/null || echo "componentstatuses deprecated - use podmapping"

# MÃ©todo alternativo: Verificar todos los pods del sistema
kubectl get pods -n kube-system

# Todos deberÃ­an estar Running/Completed
```

---

### ğŸ“ Ejercicio 5.3: Simular y Resolver un Problema

**Paso 1:** Crea un pod problemÃ¡tico

```bash
# Pod que intenta usar una imagen inexistente
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: broken-pod
spec:
  containers:
  - name: app
    image: nginx:nonexistent-tag-12345
EOF
```

**Paso 2:** Diagnostica el problema

```bash
# Verifica el estado
kubectl get pod broken-pod

# Revisa los eventos
kubectl describe pod broken-pod | grep -A 10 "Events:"

# Revisa los logs (probablemente no habrÃ¡ porque no arrancÃ³)
kubectl logs broken-pod 2>&1
```

**Pregunta:** Â¿QuÃ© evento indica el problema? Â¿En quÃ© estado estÃ¡ el pod?

<details>
<summary>ğŸ’¡ Respuesta esperada</summary>
- Estado: `ImagePullBackOff` o `ErrImagePull`
- Evento: `Failed to pull image "nginx:nonexistent-tag-12345": ... not found`
- El Scheduler asignÃ³ el pod, pero el kubelet no puede descargar la imagen
</details>

**Paso 3:** Corrige el problema

```bash
# Elimina el pod roto
kubectl delete pod broken-pod

# Crea uno con imagen correcta
kubectl run fixed-pod --image=nginx

# Verifica
kubectl get pod fixed-pod
```

---

## VerificaciÃ³n Final

### âœ… Checklist de Conocimientos

Verifica que puedas responder SÃ a cada pregunta:

- [ ] Â¿Puedo hacer peticiones REST al API Server sin kubectl?
- [ ] Â¿Entiendo el formato de la API de Kubernetes (apiVersion, kind, metadata, spec)?
- [ ] Â¿SÃ© cÃ³mo tomar un snapshot de etcd en Minikube?
- [ ] Â¿Entiendo los conceptos de restore de etcd (aunque no lo ejecute en Minikube)?
- [ ] Â¿Entiendo cÃ³mo el Scheduler asigna pods a nodos?
- [ ] Â¿Puedo forzar un pod a un nodo especÃ­fico con `nodeName` y `nodeSelector`?
- [ ] Â¿Entiendo el reconciliation loop de los Controllers?
- [ ] Â¿SÃ© cÃ³mo verificar el health de componentes del Control Plane?
- [ ] Â¿Puedo troubleshootear problemas del Control Plane con logs y describe?
- [ ] Â¿Entiendo las diferencias entre Minikube y un cluster de producciÃ³n?

---

## Limpieza

```bash
# Desde tu VM de Azure
# Eliminar recursos de prueba
kubectl delete pod api-test created-via-api scheduler-test ssd-pod huge-pod manual-schedule broken-pod fixed-pod --ignore-not-found=true
kubectl delete deployment test-reconcile --ignore-not-found=true
kubectl delete service test-endpoints --ignore-not-found=true
kubectl delete pod pod1 pod2 pod3 --ignore-not-found=true
kubectl delete namespace backup-test temporal --ignore-not-found=true

# Limpiar etiquetas del nodo
kubectl label node minikube disktype-

# Eliminar archivos temporales
rm -f pod-via-api.json etcd-backup-minikube.db

# Verificar limpieza
kubectl get all
# Solo deberÃ­a mostrar el service "kubernetes"
```

---

## ğŸ“ Resumen del Laboratorio

En este laboratorio prÃ¡ctico has:

1. **API Server**: Interactuado directamente con la API REST, creado recursos vÃ­a curl
2. **etcd**: Realizado backup del datastore (conceptualmente aprendido restore)
3. **Scheduler**: Observado decisiones de scheduling, usado nodeSelector y scheduling manual
4. **Controller Manager**: Visto reconciliation loops en acciÃ³n (ReplicaSet, Endpoint Controllers)
5. **Troubleshooting**: Diagnosticado problemas usando logs, events, y health checks

### ğŸ”‘ Conceptos Clave

- El Control Plane es el "cerebro" de Kubernetes
- Cada componente tiene una responsabilidad especÃ­fica
- Los Controllers implementan el patrÃ³n de "reconciliation loop"
- En Minikube todo corre en un solo nodo, pero los conceptos aplican a producciÃ³n
- El troubleshooting efectivo combina: logs + events + describe + health checks

### ğŸ“š PrÃ³ximos Pasos

- **Lab 03**: Worker Nodes (kubelet, kube-proxy, container runtime)
- **Lab 04**: Troubleshooting y Networking avanzado

---

**â±ï¸ Tiempo completado:** ~90 minutos  
**ğŸ“Š Progreso del mÃ³dulo:** 66% (Lab 2/3)

# Si hiciste el restore, puedes limpiar
sudo rm -f /tmp/etcd-backup.db
sudo rm -rf /var/lib/etcd.backup  # Cuidado con este comando
```

---

## PrÃ³ximo Laboratorio

â¡ï¸ **Laboratorio 03**: Worker Nodes - kubelet, kube-proxy y Container Runtime en profundidad

---

**Â¿Completaste el laboratorio?** âœ…
