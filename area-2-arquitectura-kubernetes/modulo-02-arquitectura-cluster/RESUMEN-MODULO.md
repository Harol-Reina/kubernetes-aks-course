# ğŸ“ RESUMEN: Arquitectura de Cluster Kubernetes

> **GuÃ­a de Estudio RÃ¡pida** - Componentes tÃ©cnicos, comunicaciÃ³n, y troubleshooting del cluster

---

## ğŸ¯ Conceptos Clave en 5 Minutos

### Â¿QuÃ© es la Arquitectura de K8s?
Kubernetes es un **sistema distribuido** compuesto por mÃºltiples componentes que trabajan juntos. Se divide en dos partes principales:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLUSTER KUBERNETES                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CONTROL PLANE (Cerebro)    â”‚    WORKERS (MÃºsculo)      â”‚
â”‚  â”œâ”€ API Server (6443)       â”‚    â”œâ”€ kubelet             â”‚
â”‚  â”œâ”€ etcd (2379)             â”‚    â”œâ”€ kube-proxy          â”‚
â”‚  â”œâ”€ Scheduler               â”‚    â””â”€ Container Runtime   â”‚
â”‚  â””â”€ Controller Manager      â”‚                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### AnalogÃ­a Simple
**Kubernetes = Hospital**
- **Control Plane** = AdministraciÃ³n (recepciÃ³n, gerencia, archivos)
- **Workers** = Ãreas de atenciÃ³n (consultorios donde trabajan los mÃ©dicos)
- **API Server** = Recepcionista (punto Ãºnico de contacto)
- **etcd** = Sistema de archivos mÃ©dicos (base de datos)
- **Scheduler** = Gerente asignando pacientes a consultorios
- **kubelet** = Enfermera en cada consultorio (ejecuta Ã³rdenes)
- **Pods** = Pacientes siendo atendidos

---

## ğŸ“Š Componentes del Control Plane

### 1. kube-apiserver (El NÃºcleo)

**FunciÃ³n**: Punto de entrada Ãºnico para TODAS las operaciones del cluster.

**CaracterÃ­sticas**:
- âœ… Expone API REST en puerto **6443** (HTTPS)
- âœ… AutenticaciÃ³n, autorizaciÃ³n, validaciÃ³n
- âœ… **ÃšNICO** componente que habla con etcd
- âœ… Stateless (puede escalar horizontalmente)

**Flujo tÃ­pico**:
```
kubectl create pod â†’ API Server â†’ Valida YAML â†’ Guarda en etcd â†’ Responde OK
```

**Comandos Ãºtiles**:
```bash
# Ver logs del API Server
kubectl logs -n kube-system kube-apiserver-<master-node>

# Verificar puerto del API Server
netstat -tlnp | grep 6443

# Probar conectividad
curl -k https://<master-ip>:6443/version
```

---

### 2. etcd (La Memoria del Cluster)

**FunciÃ³n**: Base de datos clave-valor distribuida que almacena TODO el estado del cluster.

**CaracterÃ­sticas**:
- âœ… Almacena configuraciÃ³n, secrets, estados de recursos
- âœ… Distribuido con algoritmo **Raft** (consenso)
- âœ… Puerto **2379** (cliente), **2380** (peers)
- âœ… Requiere **quorum** para funcionalidad (ej: 3 nodos = mÃ­nimo 2 activos)

**Datos que almacena**:
```
/registry/
â”œâ”€â”€ pods/              # Estado de todos los pods
â”œâ”€â”€ services/          # Definiciones de services
â”œâ”€â”€ deployments/       # Configuraciones de deployments
â”œâ”€â”€ secrets/           # Datos sensibles (encriptados)
â””â”€â”€ configmaps/        # Variables de configuraciÃ³n
```

**Comandos Ãºtiles**:
```bash
# Ver miembros del cluster etcd (desde pod etcd)
ETCDCTL_API=3 etcdctl member list

# Backup de etcd
ETCDCTL_API=3 etcdctl snapshot save backup.db

# Ver todas las keys
ETCDCTL_API=3 etcdctl get / --prefix --keys-only
```

**âš ï¸ CRÃTICO**: Si pierdes etcd, pierdes TODO el cluster. Siempre hacer backups.

---

### 3. kube-scheduler (El Asignador)

**FunciÃ³n**: Decide en quÃ© Worker Node debe ejecutarse cada nuevo Pod.

**Proceso de decisiÃ³n**:
```
1. FILTRADO (Filtering)
   â”œâ”€ Elimina nodos sin recursos suficientes
   â”œâ”€ Elimina nodos con taints incompatibles
   â””â”€ Elimina nodos que no cumplen nodeSelector

2. SCORING (PuntuaciÃ³n)
   â”œâ”€ Balance de recursos (CPU, RAM)
   â”œâ”€ Anti-afinidad (no poner todos en un nodo)
   â””â”€ TopologÃ­a (spread across zones)

3. BINDING
   â””â”€ Actualiza etcd: "Pod X va al Nodo Y"
```

**CaracterÃ­sticas**:
- âœ… Solo **asigna**, no ejecuta pods
- âœ… Lee continuamente etcd buscando pods sin nodo
- âœ… Considera: recursos, afinidad, taints, tolerations

**Ejemplo visual**:
```
Pod Nuevo â†’ Scheduler busca nodo â†’ Scoring:
  Nodo A: 85 puntos (70% RAM libre)
  Nodo B: 60 puntos (40% RAM libre)
  Nodo C: 30 puntos (10% RAM libre)
â†’ Scheduler asigna al Nodo A
```

**Comandos Ãºtiles**:
```bash
# Ver eventos del scheduler
kubectl get events --sort-by='.metadata.creationTimestamp'

# Ver logs del scheduler
kubectl logs -n kube-system kube-scheduler-<master-node>
```

---

### 4. kube-controller-manager (El Vigilante)

**FunciÃ³n**: Ejecuta mÃºltiples "controllers" que vigilan el estado deseado vs real.

**Controllers principales**:

| Controller | FunciÃ³n | Ejemplo |
|-----------|---------|---------|
| **Node Controller** | Detecta nodos caÃ­dos | Si nodo no responde 5 min â†’ marca pods como terminados |
| **Replication Controller** | Mantiene rÃ©plicas correctas | Si hay 2/3 pods â†’ crea 1 mÃ¡s |
| **Endpoints Controller** | Actualiza endpoints de Services | Si pod nuevo â†’ aÃ±ade IP a Service |
| **ServiceAccount Controller** | Crea ServiceAccounts para namespaces | Namespace nuevo â†’ crea SA "default" |

**Loop de control**:
```
1. Lee estado DESEADO de etcd (ej: Deployment con 3 rÃ©plicas)
2. Lee estado REAL del cluster (ej: solo 2 pods running)
3. ACTÃšA para reconciliar (crea 1 pod mÃ¡s)
4. Espera 5-10 segundos
5. Repite infinitamente
```

**Comandos Ãºtiles**:
```bash
# Ver logs de controllers
kubectl logs -n kube-system kube-controller-manager-<master-node>

# Ver quÃ© controllers estÃ¡n activos
kubectl get componentstatuses
```

---

## ğŸ–¥ï¸ Componentes de Worker Nodes

### 1. kubelet (El Ejecutor)

**FunciÃ³n**: Agente en cada Worker que ejecuta y supervisa los Pods.

**Responsabilidades**:
- âœ… Registra el nodo en el cluster
- âœ… Lee PodSpecs asignados a su nodo (desde API Server)
- âœ… Ejecuta contenedores usando el container runtime
- âœ… Monitorea salud de pods (health checks)
- âœ… Reporta estado al API Server

**Flujo de trabajo**:
```
1. kubelet consulta API Server cada 10s: "Â¿Hay pods para mÃ­?"
2. API Server responde: "SÃ­, ejecuta pod X con imagen nginx:1.21"
3. kubelet descarga imagen (si no existe)
4. kubelet dice al runtime: "Crea contenedor con esta spec"
5. kubelet monitorea contenedor
6. kubelet reporta estado a API Server
```

**CaracterÃ­sticas**:
- âœ… Corre como **systemd service** (no como pod)
- âœ… Puerto **10250** (API del kubelet)
- âœ… Ejecuta health checks (liveness, readiness, startup)

**Comandos Ãºtiles**:
```bash
# Ver estado del kubelet
systemctl status kubelet

# Ver logs del kubelet
journalctl -u kubelet -f

# Ver configuraciÃ³n del kubelet
kubectl get --raw /api/v1/nodes/<node-name>/proxy/configz
```

---

### 2. kube-proxy (El Enrutador)

**FunciÃ³n**: Implementa las reglas de red para que los Services funcionen.

**CÃ³mo funciona**:
```
Service "mi-app" = ClusterIP 10.96.0.50:80
Pods backend:
  - Pod A: 192.168.1.10:8080
  - Pod B: 192.168.1.11:8080
  - Pod C: 192.168.1.12:8080

kube-proxy crea reglas iptables:
  "Si alguien intenta conectar a 10.96.0.50:80 
   â†’ redirige a uno de los pods aleatoriamente"
```

**Modos de operaciÃ³n**:

| Modo | DescripciÃ³n | Performance |
|------|-------------|-------------|
| **iptables** | Reglas de firewall (default) | â­â­â­ Bueno |
| **IPVS** | Balanceo avanzado | â­â­â­â­â­ Excelente |
| **userspace** | Proxy en espacio de usuario | â­ Lento (legacy) |

**Comandos Ãºtiles**:
```bash
# Ver reglas iptables creadas por kube-proxy
sudo iptables-save | grep <service-name>

# Ver logs de kube-proxy
kubectl logs -n kube-system kube-proxy-<pod-id>

# Ver modo de kube-proxy
kubectl logs -n kube-system kube-proxy-<pod> | grep "Using"
```

---

### 3. Container Runtime (El Motor)

**FunciÃ³n**: Software que ejecuta contenedores (Docker, containerd, CRI-O).

**EvoluciÃ³n histÃ³rica**:
```
2014-2020: Docker (runtime + builder + registry)
      â†“
2020+: containerd (solo runtime, mÃ¡s ligero)
      â†“
Alternativas: CRI-O (Red Hat), gVisor (Google)
```

**Interface CRI** (Container Runtime Interface):
- âœ… EstÃ¡ndar para que kubelet hable con cualquier runtime
- âœ… Operaciones: PullImage, CreateContainer, StartContainer, StopContainer

**Comandos Ãºtiles**:
```bash
# Ver runtime configurado
kubectl get nodes -o wide
# Columna CONTAINER-RUNTIME

# Con Docker
docker ps

# Con containerd
crictl ps

# Ver imÃ¡genes descargadas
crictl images
```

---

## ğŸ”„ Flujo de ComunicaciÃ³n Completo

### Ejemplo: `kubectl create deployment nginx --image=nginx:1.21 --replicas=3`

```
PASO 1: kubectl â†’ API Server (puerto 6443)
  â”œâ”€ kubectl autentica con certificado
  â”œâ”€ API Server valida YAML
  â””â”€ API Server guarda Deployment en etcd

PASO 2: Controller Manager detecta cambio
  â”œâ”€ Deployment Controller lee: "Necesito 3 rÃ©plicas"
  â”œâ”€ Crea 3 PodSpecs
  â””â”€ API Server guarda Pods en etcd (estado: Pending)

PASO 3: Scheduler asigna Pods a Nodos
  â”œâ”€ Lee pods con estado "Pending"
  â”œâ”€ Scoring de nodos (recursos disponibles)
  â”œâ”€ Asigna: Pod1â†’NodeA, Pod2â†’NodeB, Pod3â†’NodeC
  â””â”€ API Server actualiza etcd

PASO 4: kubelet en cada nodo ejecuta
  â”œâ”€ NodeA: kubelet detecta Pod1 asignado
  â”œâ”€ Descarga imagen nginx:1.21
  â”œâ”€ Dice a containerd: "Crea contenedor"
  â””â”€ Reporta a API Server: "Pod1 Running"

PASO 5: kube-proxy configura networking
  â”œâ”€ Detecta nuevo pod con label app=nginx
  â”œâ”€ Actualiza reglas iptables
  â””â”€ Service puede enviar trÃ¡fico al pod

RESULTADO: 3 pods nginx ejecutÃ¡ndose en 3 nodos diferentes
```

---

## ğŸ¢ Alta Disponibilidad (HA)

### Arquitectura Multi-Master

**Problema**: Si el Control Plane falla, el cluster queda inoperable.

**SoluciÃ³n**: MÃºltiples Control Planes con Load Balancer.

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚Load Balancer â”‚ (puerto 6443)
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚               â”‚               â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
      â”‚ Master1 â”‚     â”‚ Master2 â”‚     â”‚ Master3 â”‚
      â”‚ API:6443â”‚     â”‚ API:6443â”‚     â”‚ API:6443â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
           â”‚               â”‚               â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ etcd Cluster â”‚ (quorum: 2/3)
                    â”‚ â”Œâ”€â” â”Œâ”€â” â”Œâ”€â”  â”‚
                    â”‚ â”‚1â”‚ â”‚2â”‚ â”‚3â”‚  â”‚
                    â”‚ â””â”€â”˜ â””â”€â”˜ â””â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ConfiguraciÃ³n tÃ­pica**:

| Componente | Instancias | RazÃ³n |
|-----------|------------|-------|
| **API Server** | 3+ | Load balancer distribuye carga |
| **etcd** | 3, 5, 7 (impar) | Quorum requiere mayorÃ­a |
| **Scheduler** | 3+ | Leader election (solo 1 activo) |
| **Controller Manager** | 3+ | Leader election (solo 1 activo) |

**Leader Election**:
- Scheduler y Controller Manager usan algoritmo de elecciÃ³n
- Solo UNO estÃ¡ activo (lÃ­der)
- Si lÃ­der falla â†’ otro toma el liderazgo en ~15s

**Comandos Ãºtiles**:
```bash
# Ver cuÃ¡l es el lÃ­der actual
kubectl get endpoints kube-scheduler -n kube-system -o yaml

# Ver miembros de etcd
kubectl exec -n kube-system etcd-master1 -- etcdctl member list
```

---

## ğŸ§© Addons Esenciales

### CoreDNS (DNS Interno)

**FunciÃ³n**: Resuelve nombres de Services a IPs dentro del cluster.

**Ejemplo**:
```yaml
# Service llamado "mi-app" en namespace "produccion"
# Se puede acceder como:
mi-app.produccion.svc.cluster.local â†’ 10.96.0.50
```

**Comandos Ãºtiles**:
```bash
# Ver pods de CoreDNS
kubectl get pods -n kube-system -l k8s-app=kube-dns

# Probar resoluciÃ³n DNS desde pod
kubectl run -it --rm debug --image=busybox --restart=Never -- nslookup kubernetes
```

---

### Metrics Server (MÃ©tricas)

**FunciÃ³n**: Recolecta mÃ©tricas de CPU/RAM de pods y nodos.

**Habilita comandos**:
```bash
kubectl top nodes       # Uso de CPU/RAM por nodo
kubectl top pods        # Uso de CPU/RAM por pod
```

**InstalaciÃ³n**:
```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

---

### Dashboard (UI Web)

**FunciÃ³n**: Interfaz grÃ¡fica para gestionar cluster (opcional).

**Acceso**:
```bash
kubectl proxy
# Abrir: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
```

---

## ğŸ› ï¸ Comandos de DiagnÃ³stico Esencial

### Verificar Salud del Cluster

```bash
# Estado de componentes (deprecado pero Ãºtil)
kubectl get componentstatuses
# NAME                 STATUS    MESSAGE             ERROR
# scheduler            Healthy   ok
# controller-manager   Healthy   ok
# etcd-0               Healthy   {"health":"true"}

# Ver nodos y su estado
kubectl get nodes
# NAME      STATUS   ROLES           AGE   VERSION
# master1   Ready    control-plane   10d   v1.28.0
# worker1   Ready    <none>          10d   v1.28.0
# worker2   Ready    <none>          10d   v1.28.0

# Ver pods del sistema
kubectl get pods -n kube-system
# Buscar: Running (todos deberÃ­an estar running)

# Ver eventos del cluster (errores recientes)
kubectl get events --all-namespaces --sort-by='.lastTimestamp'
```

---

### Diagnosticar Control Plane

```bash
# Logs del API Server
kubectl logs -n kube-system kube-apiserver-<master-node> --tail=100

# Logs del Scheduler
kubectl logs -n kube-system kube-scheduler-<master-node> --tail=100

# Logs del Controller Manager
kubectl logs -n kube-system kube-controller-manager-<master-node> --tail=100

# Verificar etcd (desde pod de etcd)
kubectl exec -n kube-system etcd-master1 -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint health
```

---

### Diagnosticar Worker Nodes

```bash
# Ver detalles de un nodo (condiciones, capacidad, info)
kubectl describe node <node-name>

# Logs del kubelet (en el nodo directamente)
ssh <node-name>
journalctl -u kubelet -f

# Ver recursos usados en nodo
kubectl top node <node-name>

# Ver quÃ© pods estÃ¡n en un nodo
kubectl get pods --all-namespaces --field-selector spec.nodeName=<node-name>
```

---

## ğŸ“‹ Checklist de Conceptos Clave

### Control Plane
- [ ] SÃ© que API Server es el ÃšNICO punto de entrada
- [ ] Entiendo que etcd almacena TODO el estado del cluster
- [ ] Puedo explicar quÃ© hace el Scheduler (asignar pods a nodos)
- [ ] Comprendo el loop de control de Controller Manager
- [ ] SÃ© que mÃºltiples masters requieren Load Balancer

### Worker Nodes
- [ ] Entiendo que kubelet ejecuta pods en cada nodo
- [ ] SÃ© que kube-proxy maneja las reglas de red
- [ ] Conozco la diferencia entre Docker y containerd
- [ ] Puedo verificar el estado de kubelet con `systemctl status kubelet`

### ComunicaciÃ³n
- [ ] Puedo trazar el flujo de `kubectl create deployment`
- [ ] Entiendo cÃ³mo interactÃºan todos los componentes
- [ ] SÃ© que SOLO API Server habla con etcd

### Alta Disponibilidad
- [ ] Comprendo por quÃ© etcd necesita quorum (mayorÃ­a)
- [ ] Entiendo leader election en Scheduler y Controllers
- [ ] SÃ© que API Server puede escalar horizontalmente

### Troubleshooting
- [ ] Puedo verificar salud con `kubectl get nodes`
- [ ] SÃ© cÃ³mo ver logs de componentes del Control Plane
- [ ] Puedo diagnosticar problemas de kubelet con `journalctl`

---

## â“ Preguntas de Repaso

### Conceptuales

1. **Â¿Por quÃ© API Server es el Ãºnico componente que habla con etcd?**
   <details>
   <summary>Ver respuesta</summary>
   
   - **Seguridad**: Un Ãºnico punto de acceso es mÃ¡s fÃ¡cil de proteger
   - **Consistencia**: API Server valida y serializa todos los cambios
   - **AuditorÃ­a**: Todos los cambios pasan por un punto central
   - **EncriptaciÃ³n**: API Server puede encriptar datos antes de guardarlos
   </details>

2. **Â¿QuÃ© sucede si etcd falla completamente?**
   <details>
   <summary>Ver respuesta</summary>
   
   - **Pods existentes siguen ejecutÃ¡ndose** (kubelet los mantiene)
   - **NO puedes crear/modificar recursos** (API Server no puede guardar)
   - **NO puedes ver estado del cluster** (kubectl get no funciona)
   - **Scheduler y Controllers se detienen** (no pueden leer/escribir estado)
   - **SOLUCIÃ“N**: Restaurar etcd desde backup
   </details>

3. **Â¿Por quÃ© se recomienda un nÃºmero impar de nodos etcd?**
   <details>
   <summary>Ver respuesta</summary>
   
   - **Quorum**: Necesitas mayorÃ­a para tomar decisiones
   - Con 3 nodos: toleras 1 fallo (2/3 = mayorÃ­a)
   - Con 4 nodos: toleras 1 fallo (3/4 = mayorÃ­a) â† mismo que con 3
   - Con 5 nodos: toleras 2 fallos (3/5 = mayorÃ­a)
   - Con 6 nodos: toleras 2 fallos (4/6 = mayorÃ­a) â† mismo que con 5
   - **ConclusiÃ³n**: 4 y 6 no aportan ventaja sobre 3 y 5
   </details>

---

### TÃ©cnicas

4. **Â¿CÃ³mo verificas que el Scheduler estÃ¡ funcionando?**
   <details>
   <summary>Ver respuesta</summary>
   
   ```bash
   # OpciÃ³n 1: Ver estado de componentes
   kubectl get componentstatuses
   
   # OpciÃ³n 2: Ver logs del scheduler
   kubectl logs -n kube-system kube-scheduler-<master-node>
   
   # OpciÃ³n 3: Crear pod y ver si se asigna
   kubectl run test-pod --image=nginx
   kubectl get pod test-pod -o wide
   # Si tiene NODO asignado â†’ Scheduler funciona
   
   # OpciÃ³n 4: Ver eventos
   kubectl get events --sort-by='.metadata.creationTimestamp'
   # Buscar: "Successfully assigned..."
   ```
   </details>

5. **Â¿CÃ³mo determinas quÃ© container runtime estÃ¡ usando tu cluster?**
   <details>
   <summary>Ver respuesta</summary>
   
   ```bash
   # OpciÃ³n 1: Ver en informaciÃ³n de nodos
   kubectl get nodes -o wide
   # Columna CONTAINER-RUNTIME
   
   # OpciÃ³n 2: Describe del nodo
   kubectl describe node <node-name> | grep "Container Runtime"
   
   # OpciÃ³n 3: Desde el nodo directamente
   ssh <node>
   crictl version  # Si usa containerd/CRI-O
   docker version  # Si usa Docker
   ```
   </details>

6. **Â¿CÃ³mo verificas que kube-proxy estÃ¡ creando reglas correctamente?**
   <details>
   <summary>Ver respuesta</summary>
   
   ```bash
   # Ver logs de kube-proxy
   kubectl logs -n kube-system kube-proxy-<pod-id>
   
   # Desde un worker node, ver reglas iptables
   ssh <node>
   sudo iptables-save | grep <service-name>
   
   # Ver modo de operaciÃ³n
   kubectl logs -n kube-system kube-proxy-<pod> | grep "Using"
   # "Using iptables Proxier" o "Using ipvs Proxier"
   
   # Probar conectividad a un Service
   kubectl run test --image=busybox -it --rm --restart=Never -- wget -O- http://<service-name>
   ```
   </details>

---

### Troubleshooting

7. **Un nodo aparece como "NotReady". Â¿CÃ³mo diagnosticas?**
   <details>
   <summary>Ver respuesta</summary>
   
   ```bash
   # Paso 1: Ver detalles del nodo
   kubectl describe node <node-name>
   # Buscar secciÃ³n "Conditions" â†’ razÃ³n del NotReady
   
   # Paso 2: Verificar kubelet en el nodo
   ssh <node-name>
   systemctl status kubelet
   journalctl -u kubelet -f
   
   # Paso 3: Verificar recursos del nodo
   df -h          # Espacio en disco
   free -h        # Memoria
   top            # CPU
   
   # Paso 4: Verificar conectividad con API Server
   telnet <master-ip> 6443
   
   # Paso 5: Reiniciar kubelet
   sudo systemctl restart kubelet
   ```
   </details>

8. **Creaste un Deployment pero los pods no se ejecutan. Â¿QuÃ© revisas?**
   <details>
   <summary>Ver respuesta</summary>
   
   ```bash
   # Paso 1: Ver estado de pods
   kubectl get pods
   # Estados posibles: Pending, ImagePullBackOff, CrashLoopBackOff, etc.
   
   # Paso 2: Describe del pod
   kubectl describe pod <pod-name>
   # Buscar "Events" al final
   
   # Paso 3: Si estÃ¡ Pending
   kubectl get events --sort-by='.metadata.creationTimestamp'
   # Puede ser: sin recursos, sin nodos, taints
   
   # Paso 4: Ver recursos disponibles
   kubectl top nodes
   kubectl describe nodes | grep -A 5 "Allocated resources"
   
   # Paso 5: Ver logs del Scheduler
   kubectl logs -n kube-system kube-scheduler-<master-node>
   ```
   </details>

9. **Â¿CÃ³mo sabes si etcd estÃ¡ saludable en un cluster HA?**
   <details>
   <summary>Ver respuesta</summary>
   
   ```bash
   # Paso 1: Ver pods de etcd
   kubectl get pods -n kube-system -l component=etcd
   # Todos deberÃ­an estar "Running"
   
   # Paso 2: Verificar salud de endpoints
   kubectl exec -n kube-system etcd-master1 -- etcdctl \
     --endpoints=https://127.0.0.1:2379 \
     --cacert=/etc/kubernetes/pki/etcd/ca.crt \
     --cert=/etc/kubernetes/pki/etcd/server.crt \
     --key=/etc/kubernetes/pki/etcd/server.key \
     endpoint health
   
   # Paso 3: Ver miembros del cluster
   kubectl exec -n kube-system etcd-master1 -- etcdctl member list
   # Todos deberÃ­an aparecer
   
   # Paso 4: Verificar que hay quorum
   # Con 3 nodos, mÃ­nimo 2 deben estar "healthy"
   ```
   </details>

---

### Profesionales

10. **Â¿CuÃ¡ndo necesitas realmente un cluster HA?**
    <details>
    <summary>Ver respuesta</summary>
    
    **SÃ necesitas HA**:
    - âœ… ProducciÃ³n con SLA crÃ­tico (99.9%+)
    - âœ… Aplicaciones 24/7 sin downtime permitido
    - âœ… MÃºltiples equipos dependiendo del cluster
    - âœ… Regulaciones de compliance (finanzas, salud)
    
    **NO necesitas HA**:
    - âŒ Entorno de desarrollo/testing
    - âŒ Demos o PoCs
    - âŒ Minikube/K3s para aprendizaje
    - âŒ Clusters efÃ­meros (recreables fÃ¡cilmente)
    
    **Trade-offs**:
    - Costo: 3-5x mÃ¡s infraestructura
    - Complejidad: MÃ¡s difÃ­cil de mantener
    - Networking: Load balancers adicionales
    </details>

11. **Â¿QuÃ© componentes puedes escalar horizontalmente?**
    <details>
    <summary>Ver respuesta</summary>
    
    | Componente | Escalable | Notas |
    |-----------|-----------|-------|
    | **API Server** | âœ… SÃ | Stateless, usa Load Balancer |
    | **etcd** | âš ï¸ SÃ | Quorum, solo nÃºmeros impares |
    | **Scheduler** | âš ï¸ Parcial | Leader election, solo 1 activo |
    | **Controller Mgr** | âš ï¸ Parcial | Leader election, solo 1 activo |
    | **kubelet** | âŒ NO | 1 por nodo (no aplica) |
    | **kube-proxy** | âŒ NO | 1 por nodo (no aplica) |
    | **Worker Nodes** | âœ… SÃ | AÃ±ade cuantos necesites |
    </details>

12. **Â¿CÃ³mo decides el tamaÃ±o del Control Plane?**
    <details>
    <summary>Ver respuesta</summary>
    
    **Reglas generales**:
    
    | TamaÃ±o Cluster | Control Plane | etcd | RazÃ³n |
    |---------------|---------------|------|-------|
    | < 10 nodos | 1 master | 1 nodo | Dev/testing |
    | 10-100 nodos | 3 masters | 3 nodos | ProducciÃ³n tÃ­pica |
    | 100-1000 nodos | 5 masters | 5 nodos | Alta escala |
    | 1000+ nodos | 7+ masters | 7 nodos | Enterprise |
    
    **Recursos mÃ­nimos por master**:
    - CPU: 2-4 cores
    - RAM: 4-8 GB
    - Disco: 50-100 GB SSD (para etcd)
    - Red: 1 Gbps
    
    **Factores a considerar**:
    - NÃºmero de objetos (pods, services, etc.)
    - Frecuencia de cambios (deployments por minuto)
    - Uso de admission webhooks (aumentan carga en API)
    </details>

---

## ğŸ“ Para Certificaciones

### CKA (Certified Kubernetes Administrator)

**Temas de este mÃ³dulo en el examen**:
- âœ… Arquitectura de cluster (10-15% del examen)
- âœ… InstalaciÃ³n y configuraciÃ³n de componentes
- âœ… Backup y restore de etcd
- âœ… Troubleshooting de cluster

**Comandos que DEBES saber**:
```bash
# Backup de etcd
ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# Restore de etcd
ETCDCTL_API=3 etcdctl snapshot restore /backup/etcd-snapshot.db \
  --data-dir=/var/lib/etcd-restore

# Ver componentes
kubectl get componentstatuses
kubectl get nodes
kubectl get pods -n kube-system

# Diagnosticar nodo NotReady
kubectl describe node <node-name>
journalctl -u kubelet -f
```

---

### CKAD (Certified Kubernetes Application Developer)

**Relevancia para CKAD**: Baja directa, alta contextual

- No te preguntarÃ¡n arquitectura detallada
- Pero ayuda entender:
  - Por quÃ© tu pod no se ejecuta (Scheduler)
  - CÃ³mo funcionan Services (kube-proxy)
  - Por quÃ© necesitas crear resources en namespaces (API Server)

**EnfÃ³cate en**: MÃ³dulos 04-18 (aplicaciones, no infraestructura)

---

## ğŸ“š Recursos Adicionales

### DocumentaciÃ³n Oficial
- [Kubernetes Architecture](https://kubernetes.io/docs/concepts/architecture/)
- [Control Plane Components](https://kubernetes.io/docs/concepts/overview/components/#control-plane-components)
- [Node Components](https://kubernetes.io/docs/concepts/overview/components/#node-components)

### Diagramas Interactivos
- [Kubernetes Architecture Diagram](https://kubernetes.io/docs/concepts/architecture/)
- [Picturing Kubernetes](https://azure.microsoft.com/en-us/resources/videos/picturing-kubernetes/)

### Herramientas de VisualizaciÃ³n
- **K9s**: Terminal UI para clusters
- **Lens**: IDE grÃ¡fico para Kubernetes
- **kube-ops-view**: Vista en tiempo real del cluster

---

## ğŸ¯ Siguiente Paso

Ahora que entiendes CÃ“MO funciona Kubernetes internamente:

â¡ï¸ **MÃ³dulo 03: InstalaciÃ³n de Minikube** - VerÃ¡s estos componentes en acciÃ³n

AprenderÃ¡s a:
- Instalar Minikube (cluster local)
- Verificar componentes del Control Plane
- Interactuar con el cluster vÃ­a kubectl
- Crear tus primeros recursos

**ConexiÃ³n**: MÃ³dulo 02 (teorÃ­a) + MÃ³dulo 03 (prÃ¡ctica) = Base sÃ³lida para el resto del curso.

---

**ğŸ“Š EstadÃ­sticas de este mÃ³dulo**:
- Componentes Control Plane: 4 principales
- Componentes Worker: 3 principales
- Addons esenciales: 3 (CoreDNS, Metrics Server, Dashboard)
- Puertos clave: 6443 (API), 2379/2380 (etcd), 10250 (kubelet)
- Comandos de diagnÃ³stico: 15+ cubiertos

**âœ… Checklist**: Â¿Puedes explicar el flujo completo de `kubectl create deployment` sin mirar las notas? Si sÃ­, estÃ¡s listo para continuar.
