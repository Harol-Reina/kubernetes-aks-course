# Setup - Lab 03: Node Drain & Cordon

Este documento describe los prerequisitos y configuraciÃ³n necesarios para el laboratorio de mantenimiento de nodos.

---

## ğŸ“‹ Tabla de Contenidos

1. [Requisitos del Sistema](#requisitos-del-sistema)
2. [Prerequisitos de Software](#prerequisitos-de-software)
3. [VerificaciÃ³n de Prerequisitos](#verificaciÃ³n-de-prerequisitos)
4. [ConfiguraciÃ³n del Entorno](#configuraciÃ³n-del-entorno)
5. [Pre-lab Validation](#pre-lab-validation)
6. [Troubleshooting de Setup](#troubleshooting-de-setup)

---

## ğŸ–¥ï¸ Requisitos del Sistema

### Arquitectura MÃ­nima del Cluster

Este laboratorio requiere **al menos 2 worker nodes** para demostrar la migraciÃ³n de pods:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       CONTROL PLANE NODE            â”‚
â”‚  - Ubuntu 20.04/22.04               â”‚
â”‚  - 2 CPU, 4GB RAM                   â”‚
â”‚  - Kubernetes v1.27+ o v1.28+       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”
â”‚Worker-1â”‚         â”‚Worker-2â”‚
â”‚        â”‚         â”‚        â”‚
â”‚2CPU/4GBâ”‚         â”‚2CPU/4GBâ”‚
â”‚Ready   â”‚         â”‚Ready   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hardware MÃ­nimo

| Componente | Control Plane | Worker Nodes (mÃ­nimo 2) |
|------------|---------------|-------------------------|
| **CPU** | 2 cores | 2 cores cada uno |
| **RAM** | 4 GB | 4 GB cada uno |
| **Disco** | 20 GB | 20 GB cada uno |
| **Red** | 1 Gbps | 1 Gbps |

### Software Base

- **OS**: Ubuntu 20.04 LTS o 22.04 LTS
- **Kubernetes**: v1.27.0+ o v1.28.0+
- **Container Runtime**: containerd v1.6+ o CRI-O
- **CNI**: Calico, Flannel, Weave, o similar

---

## ğŸ“¦ Prerequisitos de Software

### 1. Cluster Kubernetes Funcional

Debes tener un cluster con:

- âœ… Al menos **2 worker nodes** en estado `Ready`
- âœ… kubectl configurado con acceso admin
- âœ… CNI plugin instalado y funcional
- âœ… CoreDNS operacional

### 2. Acceso y Permisos

- âœ… kubectl con permisos de cluster-admin
- âœ… Capacidad de crear/eliminar namespaces
- âœ… Capacidad de crear PodDisruptionBudgets

### 3. Recursos Disponibles

El cluster debe tener capacidad para:

- âœ… Correr 10-15 pods simultÃ¡neos
- âœ… Mover pods entre nodos (recursos suficientes)
- âœ… Crear deployments con mÃºltiples rÃ©plicas

---

## âœ… VerificaciÃ³n de Prerequisitos

### Paso 1: Verificar NÃºmero de Nodos

```bash
# Listar todos los nodos
kubectl get nodes

# Output esperado (MÃNIMO):
# NAME                STATUS   ROLES           AGE   VERSION
# k8s-control-plane   Ready    control-plane   30d   v1.28.0
# k8s-worker-01       Ready    <none>          30d   v1.28.0
# k8s-worker-02       Ready    <none>          30d   v1.28.0
```

âœ… **PASS**: Al menos 2 nodos worker en estado `Ready`  
âŒ **FAIL**: Menos de 2 workers â†’ **BLOQUEANTE** - No puedes continuar

**Si solo tienes 1 worker:**
```bash
# OpciÃ³n 1: Agregar otro nodo worker (recomendado)
# Ver: Lab 02 de MÃ³dulo 22 (Worker Node Join)

# OpciÃ³n 2: Usar minikube multi-node
minikube start --nodes 3

# OpciÃ³n 3: Usar kind multi-node
kind create cluster --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
EOF
```

---

### Paso 2: Verificar Recursos de Nodos

```bash
# Ver capacidad de cada nodo
kubectl describe nodes | grep -A 5 "Capacity:"

# Verificar que hay recursos disponibles
kubectl top nodes

# Output esperado:
# NAME                CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
# k8s-worker-01       200m         10%    1500Mi          37%
# k8s-worker-02       180m         9%     1400Mi          35%
```

âœ… **PASS**: Cada worker tiene <70% CPU y <70% memoria usado  
âš ï¸ **WARNING**: >70% usado â†’ Lab puede tener problemas durante evacuaciÃ³n

---

### Paso 3: Verificar Conectividad kubectl

```bash
# Test de conexiÃ³n
kubectl cluster-info

# Output esperado:
# Kubernetes control plane is running at https://192.168.1.10:6443
# CoreDNS is running at ...
```

```bash
# Verificar permisos admin
kubectl auth can-i create pods
kubectl auth can-i delete nodes

# Ambos deben retornar: yes
```

âœ… **PASS**: ConexiÃ³n OK y permisos suficientes  
âŒ **FAIL**: Sin permisos â†’ Configurar RBAC o usar otro kubeconfig

---

### Paso 4: Verificar Pods del Sistema

```bash
# Verificar componentes core
kubectl get pods -n kube-system

# Componentes crÃ­ticos que deben estar Running:
# - coredns-* (2 pods)
# - kube-proxy-* (1 por nodo)
# - CNI pods (calico-node-*, flannel-*, etc.)
```

âœ… **PASS**: Todos los pods en `Running`  
âŒ **FAIL**: Pods crasheando â†’ Resolver antes de continuar

---

### Paso 5: Test de Scheduling

```bash
# Crear pod de prueba
kubectl run test-pod --image=nginx:alpine

# Verificar que se programa correctamente
kubectl get pod test-pod -o wide

# Eliminar
kubectl delete pod test-pod
```

âœ… **PASS**: Pod se creÃ³ y estÃ¡ Running  
âŒ **FAIL**: Pod en Pending â†’ Verificar recursos/CNI

---

## ğŸ”§ ConfiguraciÃ³n del Entorno

### Paso 1: Etiquetar Nodos (Opcional)

Para identificar fÃ¡cilmente los workers:

```bash
# Etiquetar workers
kubectl label node k8s-worker-01 node-role.kubernetes.io/worker=worker
kubectl label node k8s-worker-02 node-role.kubernetes.io/worker=worker

# Verificar
kubectl get nodes

# Output:
# NAME                STATUS   ROLES           AGE   VERSION
# k8s-control-plane   Ready    control-plane   30d   v1.28.0
# k8s-worker-01       Ready    worker          30d   v1.28.0
# k8s-worker-02       Ready    worker          30d   v1.28.0
```

---

### Paso 2: Crear Namespace de Prueba

```bash
# Crear namespace para el lab
kubectl create namespace drain-test

# Verificar
kubectl get namespace drain-test
```

---

### Paso 3: Configurar Aliases (Opcional)

```bash
# Agregar a ~/.bashrc o ~/.zshrc
cat >> ~/.bashrc << 'EOF'
# Kubernetes aliases para Lab 03
alias k='kubectl'
alias kgn='kubectl get nodes'
alias kgp='kubectl get pods -n drain-test -o wide'
alias kdrain='kubectl drain --ignore-daemonsets --delete-emptydir-data'
EOF

source ~/.bashrc
```

---

### Paso 4: Script Helper de DistribuciÃ³n de Pods

```bash
# Crear script para ver distribuciÃ³n de pods
cat > ~/pods-by-node.sh << 'EOF'
#!/bin/bash
echo "=== Pods por Nodo ==="
for node in $(kubectl get nodes -o name | cut -d'/' -f2); do
  echo ""
  echo "ğŸ“ $node:"
  kubectl get pods -A -o wide --field-selector spec.nodeName=$node | grep -v "NAMESPACE" | wc -l | xargs echo "  Total pods:"
  kubectl get pods -n drain-test -o wide --field-selector spec.nodeName=$node --no-headers | wc -l | xargs echo "  drain-test pods:"
done
EOF

chmod +x ~/pods-by-node.sh

# Uso:
# ~/pods-by-node.sh
```

---

## ğŸ§ª Pre-lab Validation

### Test 1: Multi-Node Cluster Validation

```bash
# Script de validaciÃ³n
cat > ~/validate-lab03-setup.sh << 'EOF'
#!/bin/bash
echo "=== Lab 03 Setup Validation ==="
echo ""

# Test 1: NÃºmero de workers
WORKERS=$(kubectl get nodes --no-headers | grep -v control-plane | wc -l)
echo -n "Workers disponibles: "
if [ $WORKERS -ge 2 ]; then
  echo "âœ… $WORKERS (OK)"
else
  echo "âŒ $WORKERS (Se requieren mÃ­nimo 2)"
  exit 1
fi

# Test 2: Nodos Ready
NOT_READY=$(kubectl get nodes --no-headers | grep -c NotReady)
echo -n "Nodos NotReady: "
if [ $NOT_READY -eq 0 ]; then
  echo "âœ… 0 (OK)"
else
  echo "âŒ $NOT_READY (Resolver antes de continuar)"
  kubectl get nodes
  exit 1
fi

# Test 3: Permisos
echo -n "Permisos drain: "
if kubectl auth can-i delete pods > /dev/null 2>&1; then
  echo "âœ… OK"
else
  echo "âŒ Insuficientes"
  exit 1
fi

# Test 4: Namespace
echo -n "Namespace drain-test: "
if kubectl get namespace drain-test > /dev/null 2>&1; then
  echo "âœ… Existe"
else
  echo "âš ï¸  No existe (se crearÃ¡ automÃ¡ticamente)"
fi

# Test 5: Capacidad de recursos
echo -n "Recursos disponibles: "
TOTAL_PODS=$(kubectl get pods -A --no-headers | wc -l)
if [ $TOTAL_PODS -lt 50 ]; then
  echo "âœ… Suficiente ($TOTAL_PODS/50 pods)"
else
  echo "âš ï¸  Cluster muy lleno ($TOTAL_PODS pods)"
fi

echo ""
echo "=== ValidaciÃ³n Completada ==="
EOF

chmod +x ~/validate-lab03-setup.sh
~/validate-lab03-setup.sh
```

**Output esperado:**
```
=== Lab 03 Setup Validation ===

Workers disponibles: âœ… 2 (OK)
Nodos NotReady: âœ… 0 (OK)
Permisos drain: âœ… OK
Namespace drain-test: âœ… Existe
Recursos disponibles: âœ… Suficiente (12/50 pods)

=== ValidaciÃ³n Completada ===
```

---

### Test 2: Scheduling Test

```bash
# Test de scheduling en mÃºltiples nodos
cat > ~/test-scheduling.sh << 'EOF'
#!/bin/bash
echo "=== Test de Scheduling Multi-Nodo ==="

# Crear deployment temporal
kubectl create deployment test-sched --image=nginx:alpine --replicas=4 -n default

sleep 10

# Ver distribuciÃ³n
echo ""
echo "DistribuciÃ³n de pods:"
kubectl get pods -l app=test-sched -o wide

# Verificar que estÃ¡n en diferentes nodos
NODES=$(kubectl get pods -l app=test-sched -o jsonpath='{.items[*].spec.nodeName}' | tr ' ' '\n' | sort -u | wc -l)

echo ""
echo -n "Nodos usados: "
if [ $NODES -ge 2 ]; then
  echo "âœ… $NODES (pods distribuidos)"
else
  echo "âš ï¸  $NODES (todos en mismo nodo - verificar scheduler)"
fi

# Cleanup
kubectl delete deployment test-sched -n default

echo ""
echo "=== Test Completado ==="
EOF

chmod +x ~/test-scheduling.sh
~/test-scheduling.sh
```

---

### Test 3: PodDisruptionBudget Support

```bash
# Verificar que PDBs estÃ¡n soportados
kubectl api-resources | grep PodDisruptionBudget

# Output esperado:
# poddisruptionbudgets   pdb   policy/v1   true   PodDisruptionBudget
```

âœ… **PASS**: PDB aparece en api-resources  
âŒ **FAIL**: No aparece â†’ VersiÃ³n de Kubernetes muy vieja (<1.21)

---

## ğŸ”§ Troubleshooting de Setup

### Problema 1: Solo 1 Worker Node

**SÃ­ntomas:**
```bash
kubectl get nodes
# Solo aparece 1 worker
```

**Soluciones:**

**OpciÃ³n A: Minikube multi-node** (recomendado para testing)
```bash
# Detener cluster actual
minikube stop

# Iniciar con mÃºltiples nodos
minikube start --nodes 3 --cpus 2 --memory 4096

# Verificar
kubectl get nodes
```

**OpciÃ³n B: kind multi-node**
```bash
kind create cluster --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
EOF
```

**OpciÃ³n C: Agregar worker real**
```bash
# En el nuevo nodo worker, ejecutar kubeadm join
# (Obtener comando del control plane)
sudo kubeadm token create --print-join-command
```

---

### Problema 2: Nodos en NotReady

**SÃ­ntomas:**
```bash
kubectl get nodes
NAME          STATUS     ROLES    AGE   VERSION
worker-01     NotReady   <none>   5d    v1.28.0
```

**DiagnÃ³stico:**
```bash
# Ver detalles del nodo
kubectl describe node worker-01

# Buscar en conditions:
# Type: Ready
# Status: False
# Reason: <motivo>
```

**Soluciones comunes:**

**CNI no instalado/funcionando:**
```bash
# Verificar pods de CNI
kubectl get pods -n kube-system | grep -E 'calico|flannel|weave'

# Reinstalar CNI si es necesario (ejemplo Calico)
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
```

**kubelet no corriendo:**
```bash
# SSH al nodo
ssh worker-01

# Verificar kubelet
sudo systemctl status kubelet
sudo systemctl restart kubelet
```

---

### Problema 3: Recursos Insuficientes

**SÃ­ntomas:**
```bash
kubectl top nodes
NAME          CPU(cores)   MEMORY(bytes)
worker-01     1800m/2000m  3500Mi/4000Mi  # >90% usado
```

**SoluciÃ³n:**
```bash
# Eliminar pods no esenciales
kubectl delete deployment <non-essential-deployments>

# O agregar mÃ¡s recursos al nodo
# (VM: aumentar CPU/RAM)
# (Cloud: cambiar instance type)
```

---

### Problema 4: Sin Permisos para Drain

**SÃ­ntomas:**
```bash
kubectl drain worker-01
Error from server (Forbidden): error when evicting pod...
```

**SoluciÃ³n:**
```bash
# Verificar permisos actuales
kubectl auth can-i delete pods
kubectl auth can-i create pods/eviction

# Si eres admin, crear ClusterRoleBinding
kubectl create clusterrolebinding drain-admin \
  --clusterrole=cluster-admin \
  --user=$(whoami)

# O usar kubeconfig con permisos admin
export KUBECONFIG=/etc/kubernetes/admin.conf
```

---

### Problema 5: PDBs No Soportados

**SÃ­ntomas:**
```bash
kubectl api-resources | grep PodDisruptionBudget
# No retorna nada
```

**Causa**: Kubernetes < v1.21

**SoluciÃ³n:**
```bash
# Verificar versiÃ³n
kubectl version --short

# Si < 1.21, upgradear cluster
# Ver Lab 02: Cluster Upgrade

# O saltarse la parte de PDBs en el lab
```

---

## âœ… Checklist Final Pre-Lab

Antes de comenzar el laboratorio, confirma:

- [ ] âœ… **Al menos 2 worker nodes** en estado Ready
- [ ] âœ… Todos los nodos con **recursos disponibles** (<70% CPU/RAM)
- [ ] âœ… kubectl con **permisos de admin** (can-i delete pods)
- [ ] âœ… **CNI funcional** (networking entre pods)
- [ ] âœ… **CoreDNS operacional** (DNS resolution)
- [ ] âœ… Namespace `drain-test` creado
- [ ] âœ… Scripts de validaciÃ³n ejecutados exitosamente
- [ ] âœ… **PodDisruptionBudgets soportados** (api-resources)
- [ ] âœ… Pods se pueden **programar en mÃºltiples nodos**

**ğŸ¯ Si todos los items estÃ¡n âœ…, estÃ¡s listo para comenzar el laboratorio.**

---

## ğŸ“š Referencias

- [Node Maintenance - Kubernetes Docs](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/)
- [PodDisruptionBudgets](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)
- [kubectl drain reference](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#drain)

---

*Setup Guide - Lab 03: Node Drain & Cordon | v1.0 | 2025-11-13*
