# MÃ³dulo 24: Advanced Scheduling

## ğŸ“‹ Tabla de Contenidos

1. [IntroducciÃ³n](#introducciÃ³n)
2. [Manual Scheduling](#manual-scheduling)
3. [Static Pods](#static-pods)
4. [Taints y Tolerations](#taints-y-tolerations)
5. [Node Affinity](#node-affinity)
6. [Pod Affinity y Anti-Affinity](#pod-affinity-y-anti-affinity)
7. [Resource Quotas y LimitRanges](#resource-quotas-y-limitranges)
8. [Priority Classes](#priority-classes)
9. [Scheduler Profiles](#scheduler-profiles)
10. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Objetivos de Aprendizaje

Al completar este mÃ³dulo, serÃ¡s capaz de:

- âœ… Controlar el scheduling de pods manualmente
- âœ… Crear y gestionar static pods
- âœ… Aplicar taints y tolerations para control de nodos
- âœ… Usar node affinity para scheduling avanzado
- âœ… Implementar pod affinity y anti-affinity
- âœ… Configurar resource quotas y limits
- âœ… Trabajar con priority classes
- âœ… Entender scheduler profiles personalizados

---

## ğŸ“š IntroducciÃ³n

### Â¿QuÃ© es el Scheduler?

El **kube-scheduler** es el componente que decide en quÃ© nodo debe ejecutarse cada pod.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Kubernetes Scheduler Workflow               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  1. FILTERING (Predicates)                              â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚     â”‚ Node tiene recursos suficientes?   â”‚             â”‚
â”‚     â”‚ Node satisface nodeSelector?       â”‚             â”‚
â”‚     â”‚ Pod tolera taints del node?        â”‚             â”‚
â”‚     â”‚ Puertos disponibles?                â”‚             â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                    â”‚                                     â”‚
â”‚                    â–¼                                     â”‚
â”‚  2. SCORING (Priorities)                                â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚     â”‚ Balance de recursos                â”‚             â”‚
â”‚     â”‚ Affinity rules                     â”‚             â”‚
â”‚     â”‚ Spreading                          â”‚             â”‚
â”‚     â”‚ Image locality                     â”‚             â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                    â”‚                                     â”‚
â”‚                    â–¼                                     â”‚
â”‚  3. BINDING                                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚     â”‚ Asignar pod al nodo con           â”‚             â”‚
â”‚     â”‚ mayor score                        â”‚             â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Scheduling Decision Factors

1. **Recursos:** CPU, Memoria, Storage
2. **Constraints:** nodeSelector, affinity, taints/tolerations
3. **Policies:** Spreading, packing, priorities
4. **Estado:** Capacidad disponible, health del nodo

---

## ğŸ® Manual Scheduling

### MÃ©todo 1: nodeName (Bypass Scheduler)

Asignar directamente un pod a un nodo especÃ­fico:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: manual-pod
spec:
  nodeName: worker-01  # Bypass scheduler, ir directo a worker-01
  containers:
  - name: nginx
    image: nginx
```

**CaracterÃ­sticas:**
- âœ… Scheduling inmediato (no espera scheduler)
- âœ… Ãštil para debugging
- âŒ No considera recursos disponibles
- âŒ No aplica taints/tolerations
- âŒ Si nodo no existe, pod queda Pending

```bash
# Crear pod con nodeName
kubectl apply -f manual-pod.yaml

# Verificar
kubectl get pod manual-pod -o wide
# NAME         READY   STATUS    RESTARTS   AGE   IP            NODE
# manual-pod   1/1     Running   0          5s    10.244.1.5    worker-01
```

### MÃ©todo 2: nodeSelector (Scheduler con Labels)

Usar labels para seleccionar nodos:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: selector-pod
spec:
  nodeSelector:
    disktype: ssd
    environment: production
  containers:
  - name: nginx
    image: nginx
```

**Workflow:**
1. Scheduler filtra nodos con labels especificados
2. Aplica scoring entre nodos vÃ¡lidos
3. Asigna al mejor nodo

```bash
# Etiquetar nodo
kubectl label nodes worker-01 disktype=ssd environment=production

# Ver labels
kubectl get nodes --show-labels

# Crear pod
kubectl apply -f selector-pod.yaml

# Pod irÃ¡ solo a nodos con esos labels
```

**âœ… CuÃ¡ndo usar:**
- Nodos con hardware especÃ­fico (GPU, SSD)
- SeparaciÃ³n por ambientes (prod, staging)
- Compliance requirements (regiÃ³n, zona)

---

## ğŸ—¿ Static Pods

### Â¿QuÃ© son Static Pods?

Pods gestionados directamente por kubelet en un nodo especÃ­fico, **sin pasar por API server**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Static Pods vs Regular Pods                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Regular Pod                    Static Pod              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ kubectl  â”‚                   â”‚   File   â”‚           â”‚
â”‚  â”‚ create   â”‚                   â”‚ /etc/.../ â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚
â”‚       â”‚                              â”‚                  â”‚
â”‚       â–¼                              â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   API    â”‚                   â”‚ kubelet  â”‚           â”‚
â”‚  â”‚  Server  â”‚                   â”‚  watch   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚
â”‚       â”‚                              â”‚                  â”‚
â”‚       â–¼                              â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚Scheduler â”‚                   â”‚   Pod    â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                   â”‚  created â”‚           â”‚
â”‚       â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚       â–¼                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚ kubelet  â”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚       â”‚                                                 â”‚
â”‚       â–¼                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚   Pod    â”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Crear Static Pod

**UbicaciÃ³n del manifesto:**
```bash
# Verificar staticPodPath en kubelet config
grep staticPodPath /var/lib/kubelet/config.yaml
# staticPodPath: /etc/kubernetes/manifests

# Crear static pod
sudo cat <<EOF > /etc/kubernetes/manifests/static-nginx.yaml
apiVersion: v1
kind: Pod
metadata:
  name: static-nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
EOF

# kubelet detecta el archivo automÃ¡ticamente (watch)
# Pod aparece en cluster con sufijo -<nodename>
kubectl get pods
# NAME                      READY   STATUS    RESTARTS   AGE
# static-nginx-worker-01    1/1     Running   0          10s
```

**CaracterÃ­sticas:**
- âœ… Sobreviven a reinicio de kubelet
- âœ… Kubelet los recrea automÃ¡ticamente
- âœ… Ãštiles para componentes del control plane
- âŒ No pueden ser eliminados vÃ­a kubectl
- âŒ Ligados a un nodo especÃ­fico

```bash
# Intentar eliminar static pod (no funciona)
kubectl delete pod static-nginx-worker-01
# pod "static-nginx-worker-01" deleted

# Pero reaparece inmediatamente
kubectl get pods
# NAME                      READY   STATUS    RESTARTS   AGE
# static-nginx-worker-01    1/1     Running   0          2s

# Para eliminar realmente, borrar el archivo
sudo rm /etc/kubernetes/manifests/static-nginx.yaml
```

**ğŸ¯ Casos de uso:**
- **Control plane components:** kube-apiserver, etcd, kube-scheduler (en clusters kubeadm)
- **Node-level daemons:** Monitoring agents especÃ­ficos de nodo
- **Bootstrap:** Componentes que deben estar antes de que cluster estÃ© funcional

---

## ğŸš« Taints y Tolerations

### Concepto

**Taints** = "Repelentes" en nodos  
**Tolerations** = "Tolerancia" en pods para ignorar taints

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Taints & Tolerations                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Node con Taint                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ Node: worker-01                â”‚                    â”‚
â”‚  â”‚ Taint: gpu=true:NoSchedule     â”‚                    â”‚
â”‚  â”‚         â˜¢ï¸ REPELENTE            â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                          â”‚
â”‚  Pod SIN Toleration        Pod CON Toleration          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚            â”‚            â”‚ Toleration:â”‚             â”‚
â”‚  â”‚  âŒ REJECTEDâ”‚            â”‚ gpu=true   â”‚             â”‚
â”‚  â”‚            â”‚            â”‚ âœ… ACCEPTED â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Taint Effects

1. **NoSchedule:** No schedule nuevos pods (los existentes siguen)
2. **PreferNoSchedule:** Intenta no schedulear, pero no es garantÃ­a
3. **NoExecute:** No schedule nuevos + evict pods existentes

### Aplicar Taints

```bash
# Sintaxis:
# kubectl taint nodes <node> <key>=<value>:<effect>

# Ejemplo 1: NoSchedule
kubectl taint nodes worker-01 gpu=true:NoSchedule

# Ejemplo 2: Dedicated node para equipo especÃ­fico
kubectl taint nodes worker-02 team=frontend:NoSchedule

# Ejemplo 3: NoExecute (evict existentes)
kubectl taint nodes worker-03 maintenance=true:NoExecute

# Ver taints de un nodo
kubectl describe node worker-01 | grep Taints
# Taints: gpu=true:NoSchedule

# Remover taint (aÃ±adir - al final)
kubectl taint nodes worker-01 gpu=true:NoSchedule-
```

### Tolerations en Pods

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  tolerations:
  - key: "gpu"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
  containers:
  - name: cuda-app
    image: nvidia/cuda:11.0-base
```

**Operators disponibles:**

```yaml
# Exact match
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"

# Cualquier valor para esa key
tolerations:
- key: "key1"
  operator: "Exists"
  effect: "NoSchedule"

# Tolerar todos los taints (wildcard)
tolerations:
- operator: "Exists"
```

**Toleration para NoExecute con tiempo de gracia:**

```yaml
tolerations:
- key: "node.kubernetes.io/unreachable"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 300  # Esperar 5 min antes de evict
```

### Taints AutomÃ¡ticos

Kubernetes aÃ±ade taints automÃ¡ticamente en ciertas condiciones:

```yaml
# Nodo NotReady
node.kubernetes.io/not-ready:NoExecute

# Nodo sin recursos
node.kubernetes.io/memory-pressure:NoSchedule
node.kubernetes.io/disk-pressure:NoSchedule
node.kubernetes.io/pid-pressure:NoSchedule

# Nodo sin conectividad
node.kubernetes.io/unreachable:NoExecute

# Nodo no inicializado
node.kubernetes.io/unschedulable:NoSchedule
```

**ğŸ¯ Casos de uso:**
- **Hardware especÃ­fico:** GPUs, FPGAs
- **Dedicated nodes:** Un equipo, una app
- **Maintenance:** Evict pods antes de mantenimiento
- **Multi-tenancy:** SeparaciÃ³n de cargas

---

## ğŸ§² Node Affinity

### Concepto

Node affinity = "AtracciÃ³n" hacia ciertos nodos (mÃ¡s poderoso que nodeSelector).

### Tipos de Node Affinity

1. **requiredDuringSchedulingIgnoredDuringExecution**
   - HARD requirement
   - Si no cumple, pod NO schedules (Pending)

2. **preferredDuringSchedulingIgnoredDuringExecution**
   - SOFT requirement
   - Intenta cumplir, pero si no puede, schedules igual

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: affinity-pod
spec:
  affinity:
    nodeAffinity:
      # REQUIRED (hard)
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
            - nvme
          - key: region
            operator: NotIn
            values:
            - us-west-1
      
      # PREFERRED (soft)
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100  # 0-100, mayor = mÃ¡s preferido
        preference:
          matchExpressions:
          - key: availability-zone
            operator: In
            values:
            - us-east-1a
  containers:
  - name: nginx
    image: nginx
```

### Operators Disponibles

```yaml
# In: Valor estÃ¡ en la lista
- key: environment
  operator: In
  values: [production, staging]

# NotIn: Valor NO estÃ¡ en la lista
- key: environment
  operator: NotIn
  values: [development]

# Exists: Key existe (valor no importa)
- key: ssd
  operator: Exists

# DoesNotExist: Key NO existe
- key: gpu
  operator: DoesNotExist

# Gt: Mayor que (valores numÃ©ricos)
- key: cpu-cores
  operator: Gt
  values: ["16"]

# Lt: Menor que
- key: memory-gb
  operator: Lt
  values: ["64"]
```

### Ejemplo Complejo

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: database-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        # OpciÃ³n 1: SSD + al menos 32GB RAM
        - matchExpressions:
          - key: disktype
            operator: In
            values: [ssd]
          - key: memory-gb
            operator: Gt
            values: ["32"]
        # O OpciÃ³n 2: NVMe (cualquier RAM)
        - matchExpressions:
          - key: disktype
            operator: In
            values: [nvme]
      
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: zone
            operator: In
            values: [us-east-1a]
      - weight: 50
        preference:
          matchExpressions:
          - key: rack
            operator: In
            values: [rack-1, rack-2]
  containers:
  - name: postgres
    image: postgres:14
```

**ğŸ¯ Casos de uso:**
- **Performance:** Scheduling en nodos high-performance
- **Compliance:** Datos en regiones especÃ­ficas
- **Cost optimization:** Preferir instancias spot/preemptible
- **Multi-cloud:** Dirigir workloads a proveedores especÃ­ficos

---

## ğŸ¤ Pod Affinity y Anti-Affinity

### Concepto

- **Pod Affinity:** "Quiero estar CERCA de estos pods"
- **Pod Anti-Affinity:** "Quiero estar LEJOS de estos pods"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Pod Affinity & Anti-Affinity                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Pod Affinity (Co-location)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Node 1                             â”‚                â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚                â”‚
â”‚  â”‚  â”‚ Cache  â”‚  â”‚  App   â”‚ â† Togetherâ”‚                â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                          â”‚
â”‚  Anti-Affinity (Spreading)                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Node 1   â”‚    â”‚ Node 2   â”‚    â”‚ Node 3   â”‚         â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚         â”‚
â”‚  â”‚ â”‚ App  â”‚ â”‚    â”‚ â”‚ App  â”‚ â”‚    â”‚ â”‚ App  â”‚ â”‚         â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                   â† Spread Apart                        â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pod Affinity

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-app
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - cache
        topologyKey: kubernetes.io/hostname
  containers:
  - name: web
    image: nginx
```

**Significado:**
- "SchedÃºlame en el MISMO nodo (hostname) que pods con label `app=cache`"
- Si no hay pod con `app=cache`, este pod queda Pending

**topologyKey opciones:**
```yaml
# Mismo nodo
topologyKey: kubernetes.io/hostname

# Misma zona
topologyKey: topology.kubernetes.io/zone

# Misma regiÃ³n
topologyKey: topology.kubernetes.io/region

# Custom topology
topologyKey: rack
```

### Pod Anti-Affinity

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web
            topologyKey: kubernetes.io/hostname
      containers:
      - name: nginx
        image: nginx
```

**Significado:**
- "NO me schedules en el mismo nodo que otros pods con `app=web`"
- Garantiza que cada rÃ©plica va a un nodo diferente
- Alta disponibilidad

### Preferred Anti-Affinity

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 5
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - api
              topologyKey: kubernetes.io/hostname
      containers:
      - name: api
        image: myapi:latest
```

**Significado:**
- "INTENTA no ponerme en el mismo nodo que otros pods `app=api`"
- Pero si solo hay 3 nodos y 5 rÃ©plicas, algunas compartirÃ¡n nodo
- MÃ¡s flexible que `required`

**ğŸ¯ Casos de uso:**
- **High Availability:** Spread replicas across nodes/zones
- **Performance:** Co-locate cache with app
- **Security:** Separate sensitive workloads
- **Compliance:** Data locality requirements

---

## ğŸ“Š Resource Quotas y LimitRanges

### Resource Quotas (Namespace-level)

Limitar recursos a nivel de namespace:

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: development
spec:
  hard:
    # Compute resources
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    
    # Storage
    requests.storage: 100Gi
    persistentvolumeclaims: "10"
    
    # Objects
    pods: "50"
    services: "10"
    configmaps: "20"
    secrets: "20"
    
    # Specific resource classes
    requests.nvidia.com/gpu: "2"
```

```bash
# Aplicar quota
kubectl apply -f resource-quota.yaml

# Ver quotas
kubectl get resourcequota -n development

# Describir (ver usage)
kubectl describe resourcequota compute-quota -n development
# Name:                   compute-quota
# Namespace:              development
# Resource                Used   Hard
# --------                ----   ----
# limits.cpu              8      20
# limits.memory           16Gi   40Gi
# pods                    15     50
# requests.cpu            4      10
# requests.memory         8Gi    20Gi
```

### LimitRange (Pod/Container-level)

Definir lÃ­mites default y rangos vÃ¡lidos:

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: resource-limits
  namespace: development
spec:
  limits:
  # LÃ­mites para Containers
  - type: Container
    max:
      cpu: "2"
      memory: 4Gi
    min:
      cpu: "100m"
      memory: 128Mi
    default:
      cpu: "500m"
      memory: 512Mi
    defaultRequest:
      cpu: "200m"
      memory: 256Mi
    maxLimitRequestRatio:
      cpu: "10"  # limit puede ser mÃ¡x 10x el request
      memory: "2"
  
  # LÃ­mites para Pods
  - type: Pod
    max:
      cpu: "4"
      memory: 8Gi
    min:
      cpu: "200m"
      memory: 256Mi
  
  # LÃ­mites para PVCs
  - type: PersistentVolumeClaim
    max:
      storage: 50Gi
    min:
      storage: 1Gi
```

```bash
# Aplicar limit range
kubectl apply -f limit-range.yaml

# Ver limit ranges
kubectl get limitrange -n development

# Describir
kubectl describe limitrange resource-limits -n development
```

**Efecto en pods SIN lÃ­mites definidos:**

```yaml
# Pod sin limits/requests
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  namespace: development
spec:
  containers:
  - name: app
    image: nginx

# Kubernetes aplica automÃ¡ticamente los defaults del LimitRange:
# requests:
#   cpu: 200m
#   memory: 256Mi
# limits:
#   cpu: 500m
#   memory: 512Mi
```

### Best Practices

```yaml
# Siempre definir requests y limits
resources:
  requests:
    cpu: "100m"
    memory: "128Mi"
  limits:
    cpu: "500m"
    memory: "512Mi"

# CPU: limits opcionales (permite bursting)
# Memory: limits obligatorios (OOMKill si excede)

# QoS Classes resultantes:
# 1. Guaranteed: requests == limits
# 2. Burstable: requests < limits
# 3. BestEffort: sin requests ni limits
```

---

## âš¡ Priority Classes

### Concepto

Priorizar ciertos pods sobre otros cuando hay escasez de recursos.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Pod Priority & Preemption                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Cluster con recursos limitados                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Node: 4 CPU disponibles            â”‚                â”‚
â”‚  â”‚                                     â”‚                â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚                â”‚
â”‚  â”‚ â”‚Priority=1â”‚  â”‚Priority=1â”‚  (6 CPU)â”‚                â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                          â”‚
â”‚  Llega pod con Priority=100                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚                â”‚
â”‚  â”‚ â”‚ Priority=100 â”‚ (2 CPU needed)    â”‚                â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                    â”‚                                     â”‚
â”‚                    â–¼ PREEMPTION                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                â”‚
â”‚  â”‚ â”‚ Priority=100 â”‚  â”‚Priority=1â”‚    â”‚                â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                â”‚
â”‚  â”‚                                     â”‚                â”‚
â”‚  â”‚ Pod Priority=1 eliminado (evicted) â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Crear PriorityClass

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000  # Mayor nÃºmero = mayor prioridad
globalDefault: false
preemptionPolicy: PreemptLowerPriority
description: "Para workloads crÃ­ticos de producciÃ³n"

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: medium-priority
value: 100000
globalDefault: false
description: "Para workloads normales de producciÃ³n"

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 1000
globalDefault: true  # Default para pods sin priorityClassName
description: "Para workloads de desarrollo y testing"
```

```bash
# Crear priority classes
kubectl apply -f priority-classes.yaml

# Ver priority classes
kubectl get priorityclasses
# NAME                      VALUE        GLOBAL-DEFAULT   AGE
# system-node-critical      2000001000   false            30d
# system-cluster-critical   2000000000   false            30d
# high-priority             1000000      false            1m
# medium-priority           100000       false            1m
# low-priority              1000         true             1m
```

### Usar PriorityClass en Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: critical-app
spec:
  priorityClassName: high-priority
  containers:
  - name: app
    image: critical-app:latest
    resources:
      requests:
        cpu: "1"
        memory: "2Gi"
```

### Preemption en AcciÃ³n

```bash
# Escenario: Cluster con 4 CPU totales

# 1. Crear pods low-priority que consumen todo
kubectl run low-1 --image=nginx --requests=cpu=2 --priority-class-name=low-priority
kubectl run low-2 --image=nginx --requests=cpu=2 --priority-class-name=low-priority

# 2. Crear pod high-priority que necesita 2 CPU
kubectl run critical --image=nginx --requests=cpu=2 --priority-class-name=high-priority

# Resultado:
# - low-1 o low-2 es evicted (preempted)
# - critical schedules exitosamente

# Ver events
kubectl get events --sort-by='.lastTimestamp' | grep -i preempt
```

**âš ï¸ Consideraciones:**
- Preemption puede causar disrupciones
- Usar PodDisruptionBudgets para proteger apps
- System priority classes (2000000000+) reservadas para sistema
- No abusar de high priority (todo no puede ser crÃ­tico)

---

## ğŸ¨ Scheduler Profiles

### Â¿QuÃ© son Scheduler Profiles?

Configuraciones personalizadas del scheduler para diferentes workloads.

```yaml
apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration
profiles:
# Profile 1: Default (balance general)
- schedulerName: default-scheduler
  plugins:
    score:
      enabled:
      - name: NodeResourcesBalancedAllocation
        weight: 1
      - name: NodeResourcesLeastAllocated
        weight: 1

# Profile 2: Bin packing (mÃ¡xima densidad)
- schedulerName: bin-packer
  plugins:
    score:
      enabled:
      - name: NodeResourcesMostAllocated
        weight: 5
      disabled:
      - name: NodeResourcesBalancedAllocation

# Profile 3: Spread (mÃ¡ximo spreading)
- schedulerName: spread-scheduler
  plugins:
    score:
      enabled:
      - name: PodTopologySpread
        weight: 10
      - name: NodeResourcesBalancedAllocation
        weight: 5
```

### Usar Scheduler Personalizado

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: packed-pod
spec:
  schedulerName: bin-packer  # Usar scheduler custom
  containers:
  - name: app
    image: myapp
```

**ğŸ¯ Casos de uso:**
- **Bin packing:** Minimizar nÃºmero de nodos (cost optimization)
- **Spreading:** Maximizar distribuciÃ³n (high availability)
- **GPU scheduling:** LÃ³gica especializada para GPUs
- **Multi-tenancy:** Diferentes polÃ­ticas por tenant

---

## ğŸ› Troubleshooting

### Pod en estado Pending

```bash
# Ver por quÃ© no schedules
kubectl describe pod <pod-name>

# Buscar en Events:
# - "0/3 nodes are available: 3 Insufficient cpu"
#   â†’ Cluster sin recursos

# - "0/3 nodes are available: 3 node(s) had taint {key=value:NoSchedule}"
#   â†’ Falta toleration

# - "0/3 nodes are available: 3 node(s) didn't match Pod's node affinity"
#   â†’ Node affinity no cumplida

# - "0/3 nodes are available: 3 node(s) didn't match pod anti-affinity rules"
#   â†’ Anti-affinity bloqueando
```

### Debugging Node Affinity

```bash
# Ver labels de nodos
kubectl get nodes --show-labels

# Ver si nodo cumple affinity
kubectl get nodes -l disktype=ssd

# AÃ±adir label faltante
kubectl label nodes worker-01 disktype=ssd
```

### Debugging Taints

```bash
# Ver taints de todos los nodos
kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints

# Describir nodo especÃ­fico
kubectl describe node worker-01 | grep -A 5 Taints

# Remover taint
kubectl taint nodes worker-01 gpu=true:NoSchedule-
```

### Verificar Resource Quotas

```bash
# Ver si namespace tiene quota excedida
kubectl describe resourcequota -n <namespace>

# Ver LimitRanges
kubectl describe limitrange -n <namespace>

# Si pod rechazado por quota:
# Error: "exceeded quota: compute-quota, requested: requests.cpu=2, used: requests.cpu=9, limited: requests.cpu=10"
```

---

## ğŸ“š Referencias

- [Scheduling Framework](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/)
- [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)
- [Taints and Tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)
- [Pod Priority and Preemption](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/)
- [Resource Quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/)

---

## ğŸ¯ PrÃ³ximos Pasos

1. **PrÃ¡ctica**: Completar laboratorios de este mÃ³dulo
2. **ExperimentaciÃ³n**: Combinar affinity + taints + priorities
3. **Monitoring**: Ver scheduling decisions con `kubectl describe`
4. **Avanzar**: Continuar con [MÃ³dulo 25: Networking Deep Dive](../modulo-25-networking/)

---

**Ver tambiÃ©n:**
- [Laboratorios](./laboratorios/README.md) - 4 labs prÃ¡cticos
- [Ejemplos](./ejemplos/README.md) - YAMLs y configs
- [RESUMEN](./RESUMEN-MODULO.md) - Cheatsheet de comandos

**ğŸ¯ CKA Coverage:** Este mÃ³dulo cubre ~15% del examen CKA (Workloads & Scheduling).
