# MÃ³dulo 22: Cluster Setup con kubeadm

## ğŸ“‹ InformaciÃ³n del MÃ³dulo

| Atributo | Detalle |
|----------|---------|
| **DuraciÃ³n estimada** | 2.5 horas |
| **Nivel** | ğŸ”´ Avanzado |
| **Prerequisitos** | MÃ³dulos 1-21, Linux bÃ¡sico, Networking bÃ¡sico |
| **Objetivos de aprendizaje** | Instalar y configurar clusters Kubernetes con kubeadm |
| **CertificaciÃ³n** | CKA (25% del examen) |
| **Laboratorios** | 4 labs prÃ¡cticos |

---

## ğŸ¯ Objetivos de Aprendizaje

Al completar este mÃ³dulo, serÃ¡s capaz de:

- âœ… **Instalar** un cluster Kubernetes desde cero con kubeadm
- âœ… **Configurar** control plane y worker nodes
- âœ… **Implementar** clusters High Availability (HA)
- âœ… **Gestionar** etcd y realizar backup/restore
- âœ… **Configurar** networking con CNI plugins
- âœ… **Troubleshootear** problemas comunes de instalaciÃ³n
- âœ… **Escalar** clusters agregando/removiendo nodos
- âœ… **Actualizar** versiones de Kubernetes

---

## ğŸ“š Contenido

1. [IntroducciÃ³n a kubeadm](#1-introducciÃ³n-a-kubeadm)
2. [Prerequisites del Sistema](#2-prerequisites-del-sistema)
3. [InstalaciÃ³n de Componentes](#3-instalaciÃ³n-de-componentes)
4. [Inicializar Control Plane](#4-inicializar-control-plane)
5. [Configurar Networking (CNI)](#5-configurar-networking-cni)
6. [Agregar Worker Nodes](#6-agregar-worker-nodes)
7. [High Availability (HA)](#7-high-availability-ha)
8. [GestiÃ³n de etcd](#8-gestiÃ³n-de-etcd)
9. [Troubleshooting](#9-troubleshooting)
10. [Mejores PrÃ¡cticas](#10-mejores-prÃ¡cticas)

---

## 1. IntroducciÃ³n a kubeadm

### Â¿QuÃ© es kubeadm?

`kubeadm` es la herramienta oficial de Kubernetes para:
- âœ… Inicializar clusters Kubernetes
- âœ… Realizar bootstrapping de control plane
- âœ… Gestionar certificados y configuraciones
- âœ… Facilitar upgrades de clusters
- âœ… Seguir mejores prÃ¡cticas de Kubernetes

### kubeadm vs Otras Herramientas

| Herramienta | Caso de Uso | Complejidad | ProducciÃ³n |
|-------------|-------------|-------------|------------|
| **kubeadm** | Clusters on-premises, VMs | Media | âœ… SÃ­ |
| **Minikube** | Desarrollo local | Baja | âŒ No |
| **Kind** | Testing, CI/CD | Baja | âŒ No |
| **kops** | AWS principalmente | Alta | âœ… SÃ­ |
| **Kubespray** | Ansible-based, multi-cloud | Alta | âœ… SÃ­ |
| **Managed** (AKS, EKS, GKE) | Cloud-native | Baja | âœ… SÃ­ |

### Arquitectura de Cluster con kubeadm

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTROL PLANE NODE(S)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ API Server  â”‚  â”‚  Scheduler  â”‚  â”‚ Controller  â”‚        â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚   Manager   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚               etcd (key-value store)            â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  kubelet  â”‚  kube-proxy  â”‚  Container Runtime (CRI) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ Network (CNI)
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WORKER NODES                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Node 1                                             â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚ kubelet â”‚  â”‚kube-proxyâ”‚  â”‚ Container Runtimeâ”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚         Application Pods                    â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Node 2, Node 3, ... (Similar structure)          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workflow de InstalaciÃ³n

```
1. Prerequisites
   â”œâ”€â”€ Sistema operativo compatible
   â”œâ”€â”€ Recursos mÃ­nimos (CPU, RAM, Disk)
   â”œâ”€â”€ Networking configurado
   â””â”€â”€ Puertos abiertos

2. InstalaciÃ³n de Componentes
   â”œâ”€â”€ Container Runtime (containerd/CRI-O)
   â”œâ”€â”€ kubeadm
   â”œâ”€â”€ kubelet
   â””â”€â”€ kubectl

3. Inicializar Control Plane
   â”œâ”€â”€ kubeadm init
   â”œâ”€â”€ Configurar kubeconfig
   â””â”€â”€ Verificar componentes

4. Instalar CNI Plugin
   â”œâ”€â”€ Calico / Flannel / Weave
   â””â”€â”€ Verificar networking

5. Agregar Worker Nodes
   â”œâ”€â”€ kubeadm join (con token)
   â””â”€â”€ Verificar nodos

6. ValidaciÃ³n Final
   â”œâ”€â”€ kubectl get nodes
   â”œâ”€â”€ kubectl get pods -A
   â””â”€â”€ Deploy test application
```

---

## 2. Prerequisites del Sistema

### Requisitos de Hardware

#### Control Plane Node

| Recurso | MÃ­nimo | Recomendado | ProducciÃ³n HA |
|---------|--------|-------------|---------------|
| **CPU** | 2 cores | 4 cores | 8+ cores |
| **RAM** | 2 GB | 4 GB | 16+ GB |
| **Disk** | 20 GB | 50 GB | 100+ GB SSD |
| **Network** | 1 Gbps | 10 Gbps | 10+ Gbps |

#### Worker Nodes

| Recurso | MÃ­nimo | Recomendado | ProducciÃ³n |
|---------|--------|-------------|------------|
| **CPU** | 1 core | 2 cores | 4+ cores |
| **RAM** | 1 GB | 2 GB | 8+ GB |
| **Disk** | 10 GB | 20 GB | 50+ GB |
| **Network** | 1 Gbps | 10 Gbps | 10+ Gbps |

### Requisitos de Sistema Operativo

**Soportados**:
- âœ… Ubuntu 20.04/22.04 LTS
- âœ… Debian 10/11
- âœ… CentOS/RHEL 8/9
- âœ… Rocky Linux 8/9
- âœ… Fedora 36+

**Kernel**:
- MÃ­nimo: 4.x
- Recomendado: 5.x+

### ConfiguraciÃ³n de Red

#### Puertos Requeridos

**Control Plane**:
```
6443        TCP  API Server
2379-2380   TCP  etcd server client API
10250       TCP  Kubelet API
10259       TCP  kube-scheduler
10257       TCP  kube-controller-manager
```

**Worker Nodes**:
```
10250       TCP  Kubelet API
30000-32767 TCP  NodePort Services
```

**etcd (solo para HA)**:
```
2379-2380   TCP  Client requests
2380        TCP  Peer communication
```

#### Verificar Puertos

```bash
# En cada nodo
nc -zv <control-plane-ip> 6443
nc -zv <control-plane-ip> 2379
nc -zv <control-plane-ip> 10250

# O usar nmap
nmap -p 6443,2379-2380,10250,10259,10257 <control-plane-ip>
```

### Deshabilitar Swap

**Â¿Por quÃ©?**
- Kubernetes requiere swap deshabilitado para garantizar rendimiento predecible
- kubelet no inicia si swap estÃ¡ habilitado

```bash
# Verificar swap
free -h
swapon --show

# Deshabilitar temporalmente
sudo swapoff -a

# Deshabilitar permanentemente
sudo sed -i '/ swap / s/^/#/' /etc/fstab

# Verificar
free -h  # Swap debe mostrar 0
```

### Configurar Firewall

```bash
# UFW (Ubuntu/Debian)
sudo ufw allow 6443/tcp
sudo ufw allow 2379:2380/tcp
sudo ufw allow 10250/tcp
sudo ufw allow 10259/tcp
sudo ufw allow 10257/tcp
sudo ufw allow 30000:32767/tcp

# firewalld (RHEL/CentOS)
sudo firewall-cmd --permanent --add-port=6443/tcp
sudo firewall-cmd --permanent --add-port=2379-2380/tcp
sudo firewall-cmd --permanent --add-port=10250/tcp
sudo firewall-cmd --permanent --add-port=10259/tcp
sudo firewall-cmd --permanent --add-port=10257/tcp
sudo firewall-cmd --permanent --add-port=30000-32767/tcp
sudo firewall-cmd --reload

# O deshabilitar firewall (NO recomendado para producciÃ³n)
sudo systemctl stop firewalld
sudo systemctl disable firewalld
```

### Configurar SELinux (RHEL/CentOS)

```bash
# OpciÃ³n 1: Permissive mode (recomendado para desarrollo)
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

# OpciÃ³n 2: Disabled (NO recomendado para producciÃ³n)
# sudo sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config
# sudo reboot

# Verificar
getenforce  # Debe mostrar Permissive o Disabled
```

### Configurar MÃ³dulos del Kernel

```bash
# Cargar mÃ³dulos necesarios
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# Verificar
lsmod | grep br_netfilter
lsmod | grep overlay
```

### Configurar ParÃ¡metros Sysctl

```bash
# ConfiguraciÃ³n de red para Kubernetes
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Aplicar cambios sin reiniciar
sudo sysctl --system

# Verificar
sudo sysctl net.bridge.bridge-nf-call-iptables net.ipv4.ip_forward
```

---

## 3. InstalaciÃ³n de Componentes

### Paso 1: Instalar Container Runtime

Kubernetes requiere un Container Runtime compatible con CRI (Container Runtime Interface).

#### OpciÃ³n A: containerd (Recomendado)

```bash
# Actualizar sistema
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl

# Instalar containerd
sudo apt-get install -y containerd

# Configurar containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml

# Habilitar SystemdCgroup (CRÃTICO)
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

# Reiniciar containerd
sudo systemctl restart containerd
sudo systemctl enable containerd

# Verificar
sudo systemctl status containerd
```

#### OpciÃ³n B: CRI-O

```bash
# Configurar repositorio CRI-O
export OS=xUbuntu_22.04
export VERSION=1.28

echo "deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /" | \
  sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list

echo "deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /" | \
  sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list

curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | \
  sudo apt-key add -
curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | \
  sudo apt-key add -

# Instalar CRI-O
sudo apt-get update
sudo apt-get install -y cri-o cri-o-runc

# Iniciar CRI-O
sudo systemctl daemon-reload
sudo systemctl enable crio --now
sudo systemctl start crio

# Verificar
sudo systemctl status crio
```

### Paso 2: Instalar kubeadm, kubelet y kubectl

```bash
# Agregar repositorio de Kubernetes
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gpg

sudo mkdir -p -m 755 /etc/apt/keyrings

# Agregar GPG key
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | \
  sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# Agregar repositorio
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | \
  sudo tee /etc/apt/sources.list.d/kubernetes.list

# Instalar componentes
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl

# Prevenir actualizaciones automÃ¡ticas
sudo apt-mark hold kubelet kubeadm kubectl

# Habilitar kubelet
sudo systemctl enable --now kubelet

# Verificar versiones
kubeadm version
kubelet --version
kubectl version --client
```

### Paso 3: Verificar InstalaciÃ³n

```bash
# Verificar que todos los componentes estÃ¡n instalados
which kubeadm kubelet kubectl
# Debe mostrar rutas: /usr/bin/kubeadm, /usr/bin/kubelet, /usr/bin/kubectl

# Verificar Container Runtime
crictl --version  # Para containerd/CRI-O

# Verificar prerequisitos de kubeadm
sudo kubeadm init phase preflight
```

---

## 4. Inicializar Control Plane

### Comando BÃ¡sico

```bash
# En el nodo control plane
sudo kubeadm init \
  --pod-network-cidr=192.168.0.0/16 \
  --apiserver-advertise-address=<CONTROL_PLANE_IP>

# Ejemplo:
sudo kubeadm init \
  --pod-network-cidr=192.168.0.0/16 \
  --apiserver-advertise-address=10.0.0.10
```

### ParÃ¡metros Importantes

| ParÃ¡metro | DescripciÃ³n | Ejemplo |
|-----------|-------------|---------|
| `--pod-network-cidr` | CIDR para pods | `192.168.0.0/16` (Calico), `10.244.0.0/16` (Flannel) |
| `--apiserver-advertise-address` | IP del API Server | `10.0.0.10` |
| `--control-plane-endpoint` | Endpoint para HA | `loadbalancer.example.com:6443` |
| `--kubernetes-version` | VersiÃ³n especÃ­fica | `v1.28.0` |
| `--upload-certs` | Subir certs para HA | (flag sin valor) |
| `--config` | Archivo de configuraciÃ³n | `kubeadm-config.yaml` |

### Output Esperado

```
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.0.0.10:6443 --token abc123.xyz789 \
    --discovery-token-ca-cert-hash sha256:1234567890abcdef...
```

### Configurar kubectl

```bash
# Como usuario regular (recomendado)
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Como root (NO recomendado para uso regular)
export KUBECONFIG=/etc/kubernetes/admin.conf

# Verificar
kubectl cluster-info
kubectl get nodes
```

### Guardar Join Command

```bash
# El comando join se muestra al final de kubeadm init
# IMPORTANTE: Guardar este comando en un lugar seguro

# Si perdiste el comando, puedes regenerarlo:
kubeadm token create --print-join-command
```

### Componentes del Control Plane

DespuÃ©s de `kubeadm init`, estos pods se crean automÃ¡ticamente:

```bash
kubectl get pods -n kube-system

# Pods esperados:
# - kube-apiserver-<hostname>
# - kube-controller-manager-<hostname>
# - kube-scheduler-<hostname>
# - etcd-<hostname>
# - kube-proxy-<random>
# - coredns-<random> (2 rÃ©plicas)
```

### Archivos de ConfiguraciÃ³n Generados

```bash
/etc/kubernetes/
â”œâ”€â”€ admin.conf              # Kubeconfig para admin
â”œâ”€â”€ kubelet.conf            # Config del kubelet
â”œâ”€â”€ controller-manager.conf # Config del controller manager
â”œâ”€â”€ scheduler.conf          # Config del scheduler
â”œâ”€â”€ manifests/              # Static pod manifests
â”‚   â”œâ”€â”€ kube-apiserver.yaml
â”‚   â”œâ”€â”€ kube-controller-manager.yaml
â”‚   â”œâ”€â”€ kube-scheduler.yaml
â”‚   â””â”€â”€ etcd.yaml
â””â”€â”€ pki/                    # Certificados y claves
    â”œâ”€â”€ ca.crt
    â”œâ”€â”€ ca.key
    â”œâ”€â”€ apiserver.crt
    â”œâ”€â”€ apiserver.key
    â””â”€â”€ ... (mÃ¡s certificados)
```

---

## 5. Configurar Networking (CNI)

### Â¿QuÃ© es un CNI Plugin?

Container Network Interface (CNI) plugins proveen:
- âœ… Conectividad entre pods
- âœ… AsignaciÃ³n de IPs
- âœ… Network policies
- âœ… Service discovery

### Opciones de CNI

| Plugin | Complejidad | Network Policies | IPAM | Mejor Para |
|--------|-------------|------------------|------|------------|
| **Calico** | Media | âœ… SÃ­ | âœ… SÃ­ | ProducciÃ³n, seguridad |
| **Flannel** | Baja | âŒ No | âš ï¸ BÃ¡sico | Desarrollo, simple |
| **Weave** | Baja | âœ… SÃ­ | âœ… SÃ­ | Multi-cloud |
| **Cilium** | Alta | âœ… SÃ­ (eBPF) | âœ… SÃ­ | Avanzado, eBPF |

### Instalar Calico (Recomendado)

```bash
# Aplicar manifest de Calico
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml

# Verificar pods de Calico
kubectl get pods -n kube-system | grep calico

# Output esperado:
# calico-kube-controllers-...  1/1   Running
# calico-node-...              1/1   Running (uno por nodo)

# Esperar a que todos los pods estÃ©n Running
kubectl wait --for=condition=Ready pods --all -n kube-system --timeout=300s
```

### Instalar Flannel

```bash
# Aplicar manifest de Flannel
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

# Verificar
kubectl get pods -n kube-flannel
```

### Verificar Networking

```bash
# 1. Verificar que el control plane node estÃ¡ Ready
kubectl get nodes
# Debe mostrar: STATUS = Ready

# 2. Verificar pods de CNI
kubectl get pods -n kube-system -l k8s-app=calico-node

# 3. Verificar CIDR de pods
kubectl cluster-info dump | grep -i cidr

# 4. Test de conectividad bÃ¡sico
kubectl run test-pod --image=nginx
kubectl wait --for=condition=Ready pod/test-pod --timeout=60s
kubectl get pod test-pod -o wide  # Ver IP asignada
kubectl delete pod test-pod
```

---

## 6. Agregar Worker Nodes

### Preparar Worker Nodes

En cada worker node, realizar los pasos 1-3 de [InstalaciÃ³n de Componentes](#3-instalaciÃ³n-de-componentes):
1. Instalar Container Runtime
2. Instalar kubeadm, kubelet, kubectl
3. Configurar prerequisites

### Obtener Join Command

**OpciÃ³n 1: Usar el comando generado en `kubeadm init`**

```bash
# El comando mostrado al final de kubeadm init:
kubeadm join 10.0.0.10:6443 --token abc123.xyz789 \
    --discovery-token-ca-cert-hash sha256:1234567890abcdef...
```

**OpciÃ³n 2: Regenerar token**

```bash
# En el control plane
kubeadm token create --print-join-command

# Output:
# kubeadm join 10.0.0.10:6443 --token newtoken.xyz123 \
#     --discovery-token-ca-cert-hash sha256:newhash...
```

### Ejecutar Join en Worker Node

```bash
# En el worker node, como root:
sudo kubeadm join 10.0.0.10:6443 \
  --token abc123.xyz789 \
  --discovery-token-ca-cert-hash sha256:1234567890abcdef...

# Output esperado:
# This node has joined the cluster:
# * Certificate signing request was sent to apiserver and a response was received.
# * The Kubelet was informed of the new secure connection details.
```

### Verificar Nodo Agregado

```bash
# En el control plane
kubectl get nodes

# Output:
# NAME               STATUS   ROLES           AGE   VERSION
# control-plane      Ready    control-plane   10m   v1.28.0
# worker-node-1      Ready    <none>          2m    v1.28.0

# Ver detalles del nodo
kubectl describe node worker-node-1

# Ver pods del sistema en el nodo
kubectl get pods -n kube-system -o wide | grep worker-node-1
```

### Etiquetar Worker Nodes

```bash
# Agregar label de rol (opcional, cosmÃ©tico)
kubectl label node worker-node-1 node-role.kubernetes.io/worker=worker

# Agregar labels personalizados
kubectl label node worker-node-1 environment=production
kubectl label node worker-node-1 disk=ssd

# Verificar labels
kubectl get nodes --show-labels
```

### Agregar MÃºltiples Workers

```bash
# Repetir el proceso en cada worker node:
# worker-node-2, worker-node-3, etc.

# Verificar todos los nodos
kubectl get nodes

# Output con 3 workers:
# NAME               STATUS   ROLES           AGE   VERSION
# control-plane      Ready    control-plane   20m   v1.28.0
# worker-node-1      Ready    worker          10m   v1.28.0
# worker-node-2      Ready    worker          5m    v1.28.0
# worker-node-3      Ready    worker          2m    v1.28.0
```

### Remover un Worker Node

```bash
# 1. Drenar el nodo (mover pods a otros nodos)
kubectl drain worker-node-1 --ignore-daemonsets --delete-emptydir-data

# 2. Eliminar el nodo del cluster
kubectl delete node worker-node-1

# 3. En el worker node, resetear kubeadm
sudo kubeadm reset

# 4. Limpiar reglas de iptables
sudo iptables -F && sudo iptables -t nat -F && sudo iptables -t mangle -F && sudo iptables -X
```

---

## 7. High Availability (HA)

### TopologÃ­as HA

#### OpciÃ³n 1: Stacked etcd (MÃ¡s Simple)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Load Balancer (HAProxy/NGINX)       â”‚
â”‚              VIP: 10.0.0.100:6443           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                        â”‚
      â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Control 1    â”‚         â”‚ Control 2    â”‚
â”‚              â”‚         â”‚              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚API Serverâ”‚ â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ â”‚API Serverâ”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  etcd    â”‚ â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ â”‚  etcd    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ventajas**:
- âœ… MÃ¡s simple de configurar
- âœ… Menos nodos requeridos
- âœ… Menos overhead

**Desventajas**:
- âš ï¸ etcd acoplado al control plane
- âš ï¸ Fallo de control plane afecta etcd

#### OpciÃ³n 2: External etcd (MÃ¡s Robusto)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Load Balancer (HAProxy/NGINX)       â”‚
â”‚              VIP: 10.0.0.100:6443           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                        â”‚
      â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Control 1    â”‚         â”‚ Control 2    â”‚
â”‚              â”‚         â”‚              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚API Serverâ”‚ â”‚         â”‚ â”‚API Serverâ”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                â”‚
            â–¼                â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ etcd-1  â”‚â—„â”€â”€â”€â”€â–ºâ”‚ etcd-2  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–²                â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ventajas**:
- âœ… etcd independiente
- âœ… Mayor resiliencia
- âœ… Escalabilidad separada

**Desventajas**:
- âš ï¸ MÃ¡s complejo
- âš ï¸ MÃ¡s nodos requeridos (6+ total)
- âš ï¸ Mayor overhead operacional

### Configurar Load Balancer

#### HAProxy Configuration

```bash
# /etc/haproxy/haproxy.cfg
frontend kubernetes-frontend
    bind *:6443
    mode tcp
    option tcplog
    default_backend kubernetes-backend

backend kubernetes-backend
    mode tcp
    option tcp-check
    balance roundrobin
    server control-plane-1 10.0.0.10:6443 check
    server control-plane-2 10.0.0.11:6443 check
    server control-plane-3 10.0.0.12:6443 check
```

#### NGINX Configuration

```bash
# /etc/nginx/nginx.conf
stream {
    upstream kubernetes {
        server 10.0.0.10:6443 max_fails=3 fail_timeout=30s;
        server 10.0.0.11:6443 max_fails=3 fail_timeout=30s;
        server 10.0.0.12:6443 max_fails=3 fail_timeout=30s;
    }

    server {
        listen 6443;
        proxy_pass kubernetes;
        proxy_timeout 10m;
        proxy_connect_timeout 1s;
    }
}
```

### Inicializar Primer Control Plane

```bash
# Crear archivo de configuraciÃ³n
cat <<EOF > kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
controlPlaneEndpoint: "loadbalancer.example.com:6443"  # VIP del LB
networking:
  podSubnet: "192.168.0.0/16"
apiServer:
  certSANs:
  - "loadbalancer.example.com"
  - "10.0.0.100"  # VIP
  - "10.0.0.10"   # Control 1
  - "10.0.0.11"   # Control 2
  - "10.0.0.12"   # Control 3
etcd:
  local:
    serverCertSANs:
    - "10.0.0.10"
    - "10.0.0.11"
    - "10.0.0.12"
EOF

# Inicializar con upload de certificados
sudo kubeadm init --config=kubeadm-config.yaml --upload-certs

# IMPORTANTE: Guardar el output que contiene:
# 1. Join command para control plane nodes
# 2. Join command para worker nodes
# 3. Certificate key (vÃ¡lido 2 horas)
```

### Agregar MÃ¡s Control Plane Nodes

```bash
# El comando se mostrÃ³ en la salida de kubeadm init:
sudo kubeadm join loadbalancer.example.com:6443 \
  --token abc123.xyz789 \
  --discovery-token-ca-cert-hash sha256:hash... \
  --control-plane \
  --certificate-key cert-key-123...

# Si expira el certificate-key (>2 horas), regenerar:
# En el primer control plane:
sudo kubeadm init phase upload-certs --upload-certs
```

### Verificar HA

```bash
# Ver todos los control plane nodes
kubectl get nodes -l node-role.kubernetes.io/control-plane

# Ver pods de control plane en todos los nodos
kubectl get pods -n kube-system -o wide | grep -E 'api|controller|scheduler|etcd'

# Verificar salud de etcd
kubectl exec -it -n kube-system etcd-control-plane-1 -- \
  etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list

# Test de failover: Apagar un control plane
# El cluster debe seguir funcionando
```

---

## 8. GestiÃ³n de etcd

### Â¿QuÃ© es etcd?

etcd es el **almacÃ©n de datos distribuido** de Kubernetes que guarda:
- âœ… ConfiguraciÃ³n del cluster
- âœ… Estado de todos los recursos
- âœ… Secrets
- âœ… ConfigMaps
- âœ… Todo el estado del cluster

### Arquitectura de etcd

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Kubernetes API Server          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ gRPC
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 etcd                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Member 1â”‚â—„â”€â”‚ Member 2â”‚â—„â”€â”‚ Member 3â”‚ â”‚
â”‚  â”‚(Leader) â”‚â”€â”€â”‚(Followerâ”‚â”€â”€â”‚(Followerâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        Raft Consensus Protocol          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Verificar Salud de etcd

```bash
# OpciÃ³n 1: Desde un pod de etcd
kubectl exec -it -n kube-system etcd-<control-plane-name> -- sh -c \
  "ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint health"

# OpciÃ³n 2: Desde el control plane node
sudo ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint health

# Output esperado:
# https://127.0.0.1:2379 is healthy: successfully committed proposal: took = 2.345ms
```

### Backup de etcd

**Â¿Por quÃ© hacer backup?**
- ğŸ”´ PÃ©rdida de etcd = pÃ©rdida TOTAL del cluster
- ğŸ”´ No se puede recrear el estado del cluster
- âœ… Backup permite disaster recovery

```bash
# Script de backup
#!/bin/bash
BACKUP_DIR="/backup/etcd"
DATE=$(date +%Y%m%d_%H%M%S)

sudo ETCDCTL_API=3 etcdctl snapshot save \
  ${BACKUP_DIR}/etcd-snapshot-${DATE}.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# Verificar backup
sudo ETCDCTL_API=3 etcdctl snapshot status \
  ${BACKUP_DIR}/etcd-snapshot-${DATE}.db \
  --write-out=table

# Comprimir
tar -czf ${BACKUP_DIR}/etcd-snapshot-${DATE}.tar.gz \
  ${BACKUP_DIR}/etcd-snapshot-${DATE}.db

# Limpiar snapshots antiguos (mantener Ãºltimos 7 dÃ­as)
find ${BACKUP_DIR} -name "etcd-snapshot-*.tar.gz" -mtime +7 -delete

echo "Backup completed: etcd-snapshot-${DATE}.tar.gz"
```

### Restore de etcd

```bash
# 1. Detener API Server (para evitar escrituras)
sudo mv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/

# 2. Restore del snapshot
sudo ETCDCTL_API=3 etcdctl snapshot restore \
  /backup/etcd/etcd-snapshot-20231113_120000.db \
  --data-dir=/var/lib/etcd-restore \
  --initial-cluster=control-plane-1=https://10.0.0.10:2380 \
  --initial-cluster-token=etcd-cluster-1 \
  --initial-advertise-peer-urls=https://10.0.0.10:2380

# 3. Actualizar manifiesto de etcd para usar nuevo data-dir
sudo vi /etc/kubernetes/manifests/etcd.yaml
# Cambiar:
# - --data-dir=/var/lib/etcd
# Por:
# - --data-dir=/var/lib/etcd-restore

# 4. Mover de vuelta el API Server
sudo mv /tmp/kube-apiserver.yaml /etc/kubernetes/manifests/

# 5. Esperar a que los pods se reinicien
watch kubectl get pods -n kube-system

# 6. Verificar cluster
kubectl get nodes
kubectl get pods --all-namespaces
```

### Monitoring de etcd

```bash
# Ver mÃ©tricas de etcd
kubectl exec -it -n kube-system etcd-<control-plane> -- sh -c \
  "ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint status --write-out=table"

# Ver miembros de etcd
kubectl exec -it -n kube-system etcd-<control-plane> -- sh -c \
  "ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list --write-out=table"

# Alarmas de etcd
kubectl exec -it -n kube-system etcd-<control-plane> -- sh -c \
  "ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  alarm list"
```

---

## 9. Troubleshooting

### Problemas Comunes

#### 1. kubeadm init falla

**SÃ­ntoma**: `kubeadm init` termina con error

**Causas comunes**:
```bash
# Swap habilitado
free -h  # Si Swap > 0
sudo swapoff -a

# Puertos en uso
sudo netstat -tulpn | grep -E '6443|2379|10250'

# Container runtime no corriendo
sudo systemctl status containerd

# Prerequisites no cumplidos
sudo kubeadm init phase preflight

# Restos de instalaciÃ³n previa
sudo kubeadm reset
sudo rm -rf /etc/kubernetes/
sudo rm -rf /var/lib/etcd/
```

#### 2. Nodo en estado NotReady

**SÃ­ntoma**: `kubectl get nodes` muestra STATUS=NotReady

**DiagnÃ³stico**:
```bash
# Ver eventos del nodo
kubectl describe node <node-name>

# Ver logs de kubelet
sudo journalctl -u kubelet -f

# Verificar CNI
kubectl get pods -n kube-system | grep -E 'calico|flannel|weave'

# Verificar container runtime
sudo systemctl status containerd
crictl ps
```

**Soluciones**:
```bash
# Reiniciar kubelet
sudo systemctl restart kubelet

# Reinstalar CNI
kubectl delete -f <cni-manifest.yaml>
kubectl apply -f <cni-manifest.yaml>

# Verificar firewall
sudo iptables -L -n | grep 6443
```

#### 3. Pods en CrashLoopBackOff

**SÃ­ntoma**: Pods del sistema no inician

```bash
# Ver logs del pod
kubectl logs -n kube-system <pod-name>
kubectl describe pod -n kube-system <pod-name>

# Ver eventos
kubectl get events -n kube-system --sort-by='.lastTimestamp'

# Para pods static (api-server, etc)
sudo cat /var/log/pods/<namespace>_<pod>_<uid>/<container>/*.log
```

#### 4. Certificados Expirados

**SÃ­ntoma**: API Server no responde, certificados expirados

```bash
# Verificar expiraciÃ³n de certificados
sudo kubeadm certs check-expiration

# Renovar certificados
sudo kubeadm certs renew all

# Reiniciar control plane
sudo systemctl restart kubelet
```

#### 5. etcd No Saludable

**SÃ­ntoma**: Cluster inestable, objetos no se crean

```bash
# Verificar salud
kubectl exec -it -n kube-system etcd-<node> -- sh -c \
  "ETCDCTL_API=3 etcdctl endpoint health ..."

# Ver alarmas
kubectl exec -it -n kube-system etcd-<node> -- sh -c \
  "ETCDCTL_API=3 etcdctl alarm list ..."

# Si hay alarma de espacio:
kubectl exec -it -n kube-system etcd-<node> -- sh -c \
  "ETCDCTL_API=3 etcdctl defrag ..."
kubectl exec -it -n kube-system etcd-<node> -- sh -c \
  "ETCDCTL_API=3 etcdctl alarm disarm ..."
```

### Comandos de DiagnÃ³stico

```bash
# Salud general del cluster
kubectl get componentstatuses  # Deprecated pero Ãºtil
kubectl get --raw='/readyz?verbose'
kubectl get --raw='/livez?verbose'

# Ver todos los recursos del sistema
kubectl get all -n kube-system

# Logs de kubelet
sudo journalctl -u kubelet -n 100 --no-pager

# Logs de containerd
sudo journalctl -u containerd -n 100 --no-pager

# ConfiguraciÃ³n de kubelet
sudo cat /var/lib/kubelet/config.yaml

# Verificar CNI config
ls -la /etc/cni/net.d/
cat /etc/cni/net.d/*.conf

# Ver rutas y reglas de red
ip route
sudo iptables -t nat -L -n -v
```

---

## 10. Mejores PrÃ¡cticas

### ProducciÃ³n

1. **Alta Disponibilidad**
   - âœ… MÃ­nimo 3 control plane nodes (nÃºmero impar)
   - âœ… Load balancer redundante
   - âœ… etcd en nodos separados (opcional)
   - âœ… MÃºltiples worker nodes

2. **Backup & Disaster Recovery**
   - âœ… Backup automatizado de etcd (diario)
   - âœ… Almacenar backups off-site
   - âœ… Probar restore periÃ³dicamente
   - âœ… Documentar procedimientos

3. **Seguridad**
   - âœ… Deshabilitar acceso a kubelet API
   - âœ… Usar RBAC estricto
   - âœ… Rotar certificados regularmente
   - âœ… Network policies habilitadas
   - âœ… Pod Security Policies/Standards

4. **Monitoring & Logging**
   - âœ… Prometheus para mÃ©tricas
   - âœ… Grafana para visualizaciÃ³n
   - âœ… ELK/Loki para logs
   - âœ… Alertas configuradas

5. **Actualizaciones**
   - âœ… Mantener 2-3 versiones detrÃ¡s de latest
   - âœ… Probar upgrades en staging primero
   - âœ… Seguir proceso de upgrade de kubeadm
   - âœ… Planificar ventanas de mantenimiento

### Sizing de Cluster

**PequeÃ±o** (Dev/Test):
- 1 control plane
- 2-3 workers
- 4 vCPU, 8 GB RAM por nodo

**Mediano** (Staging/QA):
- 3 control planes
- 5-10 workers
- 8 vCPU, 16 GB RAM por nodo

**Grande** (ProducciÃ³n):
- 3+ control planes
- 20+ workers
- 16+ vCPU, 32+ GB RAM por nodo
- SSD storage
- 10 Gbps network

### PlanificaciÃ³n de Capacidad

```
Pods por Nodo:
- Default: 110 pods/node
- Recomendado: 30-50 pods/node para mejor performance

CPU Overcommit:
- Desarrollo: 3:1
- ProducciÃ³n: 1.5:1 o 2:1

Memoria Overcommit:
- Desarrollo: 2:1
- ProducciÃ³n: 1:1 (sin overcommit)

Storage:
- etcd: IOPS altos, SSD recomendado
- Logs: 10-50 GB por nodo
- Images: 50-100 GB por nodo
```

---

## ğŸ“ Resumen

En este mÃ³dulo aprendiste:

âœ… **Instalar Kubernetes** con kubeadm desde cero  
âœ… **Configurar control plane** y worker nodes  
âœ… **Implementar HA** con mÃºltiples control planes  
âœ… **Gestionar etcd** incluyendo backup/restore  
âœ… **Configurar networking** con CNI plugins  
âœ… **Troubleshootear** problemas comunes  
âœ… **Aplicar mejores prÃ¡cticas** de producciÃ³n

### Comandos Esenciales

```bash
# Inicializar cluster
sudo kubeadm init --pod-network-cidr=192.168.0.0/16

# Agregar worker
sudo kubeadm join <endpoint>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>

# Backup etcd
sudo ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-snapshot.db ...

# Verificar salud
kubectl get nodes
kubectl get pods -A
kubectl get componentstatuses
```

---

## ğŸ”— Referencias

- [kubeadm Documentation](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/)
- [Kubernetes The Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way)
- [CNI Plugins](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)
- [etcd Documentation](https://etcd.io/docs/)
- [Cluster Administration](https://kubernetes.io/docs/tasks/administer-cluster/)

---

## â­ï¸ PrÃ³ximos Pasos

- **Laboratorios**: Practicar instalaciÃ³n hands-on
- **MÃ³dulo 23**: Maintenance & Upgrades
- **MÃ³dulo 26**: Troubleshooting avanzado

---

**Â¡Felicitaciones!** ğŸ‰ Ahora sabes cÃ³mo instalar y gestionar clusters Kubernetes production-ready con kubeadm.
