# =============================================================================
# OOMKilled - Memory Leak Intencional
# =============================================================================
#
# Simula memory leak para demostrar comportamiento de OOMKilled.
#
# ‚ö†Ô∏è SOLO PARA TESTING/EDUCACI√ìN - NO usar en producci√≥n
#
# Qu√© demuestra:
# 1. Container intenta usar m√°s memoria que su l√≠mite
# 2. Kernel OOM Killer termina el proceso
# 3. Exit Code 137 (128 + SIGKILL)
# 4. Pod se reinicia seg√∫n restartPolicy
# 5. CrashLoopBackOff si falla repetidamente
# =============================================================================

apiVersion: v1
kind: Pod
metadata:
  name: oomkilled-demo
  labels:
    app: memory-test
    example: troubleshooting
    type: oomkilled
  annotations:
    warning: "Este Pod ser√° OOMKilled intencionalmente"
spec:
  restartPolicy: Always  # ‚Üê Se reiniciar√° tras OOMKill
  
  containers:
  - name: memory-hog
    image: polinux/stress
    
    command: ["stress"]
    args:
    - "--vm"
    - "1"           # 1 memory worker
    - "--vm-bytes"
    - "512M"        # ‚Üê Intenta usar 512MB
    - "--timeout"
    - "120s"        # Corre por 120 segundos
    
    resources:
      limits:
        cpu: "500m"
        memory: "256Mi"  # ‚Üê L√≠mite MENOR que los 512MB que intenta usar
      requests:
        cpu: "250m"
        memory: "128Mi"

# =============================================================================
# TIMELINE ESPERADO
# =============================================================================
#
# 0-5s:     Pod inicia, stress command ejecuta
#           ‚úÖ Estado: Running
#
# 5-10s:    Intenta alocar 512MB de memoria
#           ‚ö†Ô∏è Excede l√≠mite de 256Mi
#           ‚ö†Ô∏è Kernel detecta exceso de memoria
#
# ~10s:     Kernel OOM Killer termina el proceso
#           ‚ùå Exit Code: 137 (128 + 9 SIGKILL)
#           ‚ùå Last State: Terminated (Reason: OOMKilled)
#
# 10-20s:   Kubernetes reinicia el contenedor (restartPolicy: Always)
#           üîÑ Restart Count: 1
#           ‚úÖ Estado: Running
#
# 20-30s:   Segunda ejecuci√≥n, mismo problema
#           ‚ùå OOMKilled otra vez
#           üîÑ Restart Count: 2
#
# 30-60s:   M√∫ltiples reinicios fallan
#           ‚ö†Ô∏è Estado: CrashLoopBackOff
#           üîÑ Restart Count: 3, 4, 5...
#           ‚è∏Ô∏è Delays crecientes entre reintentos
#
# =============================================================================
# EXIT CODE 137 EXPLICADO
# =============================================================================
#
# Exit Code 137 = 128 + 9
#
# Donde:
# - 128: Base para signals
# - 9: SIGKILL (signal number)
#
# Otros exit codes relacionados:
# - 137: OOMKilled (SIGKILL por OOM)
# - 143: Terminado por SIGTERM (shutdown graceful)
# - 1-127: Exit codes de aplicaci√≥n
# - 130: Terminado por SIGINT (Ctrl+C)
#
# =============================================================================
# OBSERVAR OOMKILLED
# =============================================================================
#
# Aplicar:
# kubectl apply -f pod.yaml
#
# Ver estado inicial:
# kubectl get pod oomkilled-demo
# NAME             READY   STATUS    RESTARTS   AGE
# oomkilled-demo   1/1     Running   0          5s
#
# Esperar ~15 segundos, ver estado despu√©s del primer OOMKill:
# kubectl get pod oomkilled-demo
# NAME             READY   STATUS    RESTARTS   AGE
# oomkilled-demo   1/1     Running   1          20s
#                                    ‚Üë Restart count aument√≥
#
# Esperar ~60 segundos, ver CrashLoopBackOff:
# kubectl get pod oomkilled-demo
# NAME             READY   STATUS             RESTARTS   AGE
# oomkilled-demo   0/1     CrashLoopBackOff   5          2m
#
# Ver √∫ltimo estado del contenedor:
# kubectl describe pod oomkilled-demo | grep -A 15 "Last State"
#
# Output:
# Last State:     Terminated
#   Reason:       OOMKilled
#   Exit Code:    137
#   Started:      Fri, 10 Jan 2025 10:05:30 +0000
#   Finished:     Fri, 10 Jan 2025 10:05:40 +0000
#
# Ver restart count detallado:
# kubectl get pod oomkilled-demo -o jsonpath='{.status.containerStatuses[0].restartCount}'
# Output: 5
#
# Ver eventos:
# kubectl get events --field-selector involvedObject.name=oomkilled-demo --sort-by='.lastTimestamp'
#
# Buscar:
# LAST SEEN   TYPE      REASON      OBJECT               MESSAGE
# 2m          Normal    Scheduled   pod/oomkilled-demo   Successfully assigned...
# 2m          Normal    Pulled      pod/oomkilled-demo   Container image pulled
# 2m          Normal    Created     pod/oomkilled-demo   Created container
# 2m          Normal    Started     pod/oomkilled-demo   Started container
# 1m          Warning   BackOff     pod/oomkilled-demo   Back-off restarting failed container
#
# Ver logs del √∫ltimo intento (antes del crash):
# kubectl logs oomkilled-demo --previous
#
# Output (de stress command):
# stress: info: [1] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hdd
# stress: FAIL: [1] (415) <-- worker 7 got signal 9
#                                                        ‚Üë SIGKILL
#
# =============================================================================
# DIFERENCIA: OOMKilled vs Evicted
# =============================================================================
#
# OOMKilled (Memory):
# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# ‚îÇ Trigger:     Excede memory limit       ‚îÇ
# ‚îÇ Acci√≥n:      Kernel mata el proceso    ‚îÇ
# ‚îÇ Exit Code:   137 (SIGKILL)             ‚îÇ
# ‚îÇ Restart:     Seg√∫n restartPolicy       ‚îÇ
# ‚îÇ Estado Pod:  Running ‚Üí OOMKilled ‚Üí     ‚îÇ
# ‚îÇ              CrashLoopBackOff          ‚îÇ
# ‚îÇ Velocidad:   Inmediato (milisegundos)  ‚îÇ
# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
#
# Evicted (Ephemeral Storage):
# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# ‚îÇ Trigger:     Excede storage limit      ‚îÇ
# ‚îÇ Acci√≥n:      Kubelet evicted el Pod    ‚îÇ
# ‚îÇ Exit Code:   N/A                       ‚îÇ
# ‚îÇ Restart:     NO (Pod eliminado)        ‚îÇ
# ‚îÇ Estado Pod:  Running ‚Üí Evicted (Failed)‚îÇ
# ‚îÇ Velocidad:   Delay (kubelet polling)   ‚îÇ
# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
#
# =============================================================================
# CRASHLOOPBACKOFF EXPLICADO
# =============================================================================
#
# CrashLoopBackOff = Estrategia de exponential backoff
#
# Delays entre reintentos:
# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# ‚îÇ Restart  ‚îÇ Delay      ‚îÇ
# ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
# ‚îÇ 1        ‚îÇ 0s         ‚îÇ
# ‚îÇ 2        ‚îÇ 10s        ‚îÇ
# ‚îÇ 3        ‚îÇ 20s        ‚îÇ
# ‚îÇ 4        ‚îÇ 40s        ‚îÇ
# ‚îÇ 5        ‚îÇ 80s        ‚îÇ
# ‚îÇ 6        ‚îÇ 160s (2m)  ‚îÇ
# ‚îÇ 7+       ‚îÇ 300s (5m)  ‚îÇ ‚Üê Max delay
# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
#
# Ver tiempo hasta pr√≥ximo reinicio:
# kubectl describe pod oomkilled-demo | grep -i "back-off"
# Output: Back-off 2m40s restarting failed container
#
# =============================================================================
# TROUBLESHOOTING OOMKILLED EN PRODUCCI√ìN
# =============================================================================
#
# 1. Confirmar que es OOMKilled
# ------------------------------
# kubectl describe pod <pod-name> | grep -i oom
#
# Buscar:
# Last State:     Terminated
#   Reason:       OOMKilled
#
#
# 2. Ver uso de memoria antes del crash
# --------------------------------------
# # Si tienes metrics-server:
# kubectl top pod <pod-name>
#
# # O revisar Prometheus/Grafana:
# container_memory_usage_bytes{pod="<pod-name>"}
#
#
# 3. Comparar con l√≠mite
# ----------------------
# kubectl get pod <pod-name> -o jsonpath='{.spec.containers[*].resources.limits.memory}'
#
# Si uso >= l√≠mite ‚Üí OOMKilled esperado
#
#
# 4. Ver logs para identificar leak
# ----------------------------------
# kubectl logs <pod-name> --previous | grep -i "memory\|allocation\|heap"
#
# Buscar patrones:
# - Crecimiento ilimitado de caches
# - Leaks en conexiones/threads
# - Buffers sin l√≠mite
#
#
# 5. Soluciones
# -------------
# Opci√≥n A: Aumentar memory limit
# resources:
#   limits:
#     memory: "512Mi"  # Era 256Mi
#
# Opci√≥n B: Optimizar aplicaci√≥n
# - Fix memory leaks
# - Liberar recursos no usados
# - Usar memory pooling
#
# Opci√≥n C: Ajustar JVM/runtime
# # Para Java:
# env:
# - name: JAVA_OPTS
#   value: "-Xmx200m -Xms100m"
#
# =============================================================================
# MONITOREO Y ALERTAS
# =============================================================================
#
# Prometheus query para OOMKilled:
# rate(kube_pod_container_status_terminated_reason{reason="OOMKilled"}[5m]) > 0
#
# Alerta ejemplo:
# - alert: PodOOMKilled
#   expr: |
#     kube_pod_container_status_terminated_reason{reason="OOMKilled"} == 1
#   for: 1m
#   labels:
#     severity: warning
#   annotations:
#     summary: "Pod {{ $labels.pod }} was OOMKilled"
#     description: "Container {{ $labels.container }} in pod {{ $labels.pod }} was terminated due to OOMKilled"
#
# =============================================================================
# LIMPIAR
# =============================================================================
#
# kubectl delete -f pod.yaml
#
# Verificar eliminaci√≥n:
# kubectl get pod oomkilled-demo
# Error from server (NotFound): pods "oomkilled-demo" not found
