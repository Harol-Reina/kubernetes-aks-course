# =============================================================================
# OOMKilled - Memory Leak Gradual
# =============================================================================
#
# Simula memory leak gradual (mÃ¡s realista que stress test).
#
# Comportamiento:
# - Comienza con bajo uso de memoria
# - Gradualmente consume mÃ¡s memoria
# - Eventualmente excede lÃ­mite â†’ OOMKilled
#
# MÃ¡s cercano a memory leaks reales en aplicaciones de producciÃ³n.
# =============================================================================

apiVersion: v1
kind: Pod
metadata:
  name: gradual-memory-leak
  labels:
    app: memory-leak-test
    example: troubleshooting
    type: memory-leak
spec:
  restartPolicy: Always
  
  containers:
  - name: leaker
    image: busybox:1.36
    
    command: ["/bin/sh", "-c"]
    args:
      - |
        echo "=== Gradual Memory Leak Simulation ==="
        echo "Memory limit: 256Mi"
        echo "Will allocate 10MB every 2 seconds"
        echo "Expected OOMKill after ~50 iterations (500MB)"
        echo ""
        
        i=0
        while true; do
          # Escribe a /dev/shm (tmpfs) para consumir memoria
          dd if=/dev/zero of=/dev/shm/file-$i bs=1M count=10 2>/dev/null
          
          i=$((i+1))
          allocated=$((i * 10))
          
          echo "[$i] Allocated ${allocated} MB total"
          
          # Mostrar uso actual
          echo "   Memory usage:"
          df -h /dev/shm | tail -1
          
          sleep 2
        done
    
    resources:
      limits:
        cpu: "200m"
        memory: "256Mi"  # â† LÃ­mite que serÃ¡ excedido
      requests:
        cpu: "100m"
        memory: "128Mi"

# =============================================================================
# TIMELINE ESPERADO
# =============================================================================
#
# Tiempo  â”‚ Allocated â”‚ Estado
# â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0s      â”‚ 0 MB      â”‚ Pod inicia
# 2s      â”‚ 10 MB     â”‚ âœ… Escribiendo a /dev/shm
# 4s      â”‚ 20 MB     â”‚ âœ… OK
# 6s      â”‚ 30 MB     â”‚ âœ… OK
# ...     â”‚ ...       â”‚ âœ… Crecimiento gradual
# 50s     â”‚ 250 MB    â”‚ âš ï¸ Cerca del lÃ­mite (256Mi)
# 52s     â”‚ 260 MB    â”‚ âŒ Excede lÃ­mite
# 52s     â”‚ -         â”‚ âŒ OOMKilled (Exit Code 137)
# 55s     â”‚ 0 MB      â”‚ ğŸ”„ Contenedor reinicia
# 57s     â”‚ 10 MB     â”‚ âœ… Comienza de nuevo
# ...     â”‚ ...       â”‚ ğŸ”„ Ciclo se repite
# 120s    â”‚ -         â”‚ âš ï¸ CrashLoopBackOff
#
# =============================================================================
# OBSERVAR EL LEAK EN TIEMPO REAL
# =============================================================================
#
# Aplicar:
# kubectl apply -f pod.yaml
#
# Monitorear logs en tiempo real (terminal 1):
# kubectl logs -f gradual-memory-leak
#
# Output esperado:
# === Gradual Memory Leak Simulation ===
# Memory limit: 256Mi
# Will allocate 10MB every 2 seconds
#
# [1] Allocated 10 MB total
#    Memory usage:
#    tmpfs            256M   10M    246M   4% /dev/shm
#
# [2] Allocated 20 MB total
#    Memory usage:
#    tmpfs            256M   20M    236M   8% /dev/shm
#
# [3] Allocated 30 MB total
#    Memory usage:
#    tmpfs            256M   30M    226M  12% /dev/shm
# ...
# [25] Allocated 250 MB total
#    Memory usage:
#    tmpfs            256M  250M      6M  98% /dev/shm
#
# (despuÃ©s de esto â†’ OOMKilled, logs se detienen)
#
#
# Monitorear uso de memoria (terminal 2):
# watch -n 2 kubectl top pod gradual-memory-leak
#
# Output:
# NAME                    CPU(cores)   MEMORY(bytes)
# gradual-memory-leak     1m           50Mi
# â†“ (despuÃ©s de ~20s)
# gradual-memory-leak     1m           150Mi
# â†“ (despuÃ©s de ~40s)
# gradual-memory-leak     1m           250Mi
# â†“ (despuÃ©s de ~50s)
# gradual-memory-leak     1m           10Mi    â† ReiniciÃ³!
#
#
# Ver eventos en tiempo real (terminal 3):
# kubectl get events --watch --field-selector involvedObject.name=gradual-memory-leak
#
# VerÃ¡s:
# LAST SEEN   TYPE      REASON      MESSAGE
# 0s          Normal    Scheduled   Successfully assigned...
# 0s          Normal    Pulling     Pulling image "busybox:1.36"
# 0s          Normal    Pulled      Successfully pulled image
# 0s          Normal    Created     Created container leaker
# 0s          Normal    Started     Started container leaker
# 52s         Warning   BackOff     Back-off restarting failed container
#
# =============================================================================
# ANÃLISIS POST-MORTEM
# =============================================================================
#
# Ver estado actual:
# kubectl get pod gradual-memory-leak
#
# Describir para ver detalles del crash:
# kubectl describe pod gradual-memory-leak
#
# Buscar secciÃ³n "Last State":
# Last State:     Terminated
#   Reason:       OOMKilled
#   Exit Code:    137
#   Started:      Fri, 10 Jan 2025 10:10:00 +0000
#   Finished:     Fri, 10 Jan 2025 10:10:52 +0000
#
# Calcular tiempo hasta OOMKill:
# Finished - Started = 52 segundos âœ… (esperado ~50s)
#
# Ver restart count:
# kubectl get pod gradual-memory-leak -o jsonpath='{.status.containerStatuses[0].restartCount}'
#
# Ver logs del intento anterior (antes del crash):
# kubectl logs gradual-memory-leak --previous
#
# Ãšltima lÃ­nea mostrarÃ¡ el Ãºltimo allocation exitoso:
# [25] Allocated 250 MB total
#
# =============================================================================
# COMPARACIÃ“N CON MEMORY LEAK REAL
# =============================================================================
#
# Este ejemplo (simulado):
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ â€¢ Crecimiento lineal y predecible      â”‚
# â”‚ â€¢ 10MB cada 2 segundos                 â”‚
# â”‚ â€¢ FÃ¡cil de calcular cuÃ¡ndo crashearÃ¡   â”‚
# â”‚ â€¢ Siempre llega al OOMKill             â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# Memory leak real (producciÃ³n):
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ â€¢ Crecimiento irregular                â”‚
# â”‚ â€¢ Depende de trÃ¡fico/carga             â”‚
# â”‚ â€¢ Puede tomar horas/dÃ­as               â”‚
# â”‚ â€¢ A veces se estabiliza antes de OOM   â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# Ejemplos reales de memory leaks:
# - Cache sin eviction policy
# - Event listeners no removidos
# - Conexiones de DB no cerradas
# - Large objects en memoria global
# - Circular references (en GC languages)
#
# =============================================================================
# IDENTIFICAR MEMORY LEAKS EN PRODUCCIÃ“N
# =============================================================================
#
# 1. Monitorear tendencia de memoria
# -----------------------------------
# Prometheus query:
# container_memory_usage_bytes{pod="my-app"}
#
# Si ves crecimiento constante sin plateau â†’ Posible leak
#
#
# 2. Heap dump (para Java)
# -------------------------
# kubectl exec -it <pod> -- jmap -dump:live,format=b,file=/tmp/heap.bin <pid>
# kubectl cp <pod>:/tmp/heap.bin ./heap.bin
# # Analizar con Eclipse MAT o VisualVM
#
#
# 3. Memory profiling (para Go)
# ------------------------------
# # En la app:
# import _ "net/http/pprof"
#
# # Port-forward:
# kubectl port-forward <pod> 6060:6060
#
# # Heap profile:
# go tool pprof http://localhost:6060/debug/pprof/heap
#
#
# 4. Node.js heap snapshot
# -------------------------
# kubectl exec -it <pod> -- node --expose-gc --inspect=0.0.0.0:9229 app.js
# # Conecta Chrome DevTools para heap snapshot
#
# =============================================================================
# PREVENCIÃ“N DE MEMORY LEAKS
# =============================================================================
#
# âœ… Best practices:
#
# 1. Set memory limits apropiados
#    resources:
#      limits:
#        memory: "512Mi"  # Basado en profiling
#
# 2. Implementa health checks
#    livenessProbe:
#      httpGet:
#        path: /healthz
#        port: 8080
#      initialDelaySeconds: 30
#      periodSeconds: 10
#
# 3. Restart periÃ³dico preventivo
#    # Para apps con leaks conocidos difÃ­ciles de fix:
#    # Usa un CronJob que reinicia Pods cada X horas
#
# 4. Monitoring y alertas
#    - alert: MemoryLeakDetected
#      expr: |
#        (container_memory_usage_bytes - container_memory_usage_bytes offset 1h) > 100000000
#      for: 2h
#      annotations:
#        summary: "Memory growing consistently in {{ $labels.pod }}"
#
# 5. Code reviews enfocados en:
#    - Cache management
#    - Resource cleanup
#    - Event listener lifecycle
#    - Connection pooling
#
# =============================================================================
# LIMPIAR
# =============================================================================
#
# kubectl delete -f pod.yaml
