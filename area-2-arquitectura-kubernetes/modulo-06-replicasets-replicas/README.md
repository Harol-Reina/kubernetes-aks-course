# ğŸ”„ MÃ³dulo 06: ReplicaSets y GestiÃ³n de RÃ©plicas

> **De Pods Individuales a Fleets de RÃ©plicas: Auto-recuperaciÃ³n y Escalado**

---

## ğŸ“‹ InformaciÃ³n del MÃ³dulo

| Aspecto | Detalle |
|---------|---------|
| **DuraciÃ³n estimada** | 3-4 horas (teorÃ­a + labs) |
| **Nivel** | Intermedio |
| **Prerequisito** | [MÃ³dulo 05: GestiÃ³n de Pods](../modulo-05-gestion-pods/) |
| **Modalidad** | PrÃ¡ctico-Intensivo |
| **VersiÃ³n K8s** | 1.28+ (Noviembre 2025) |
| **Entorno** | Minikube + Docker driver |

---

## ğŸ¯ Objetivos de Aprendizaje

Al finalizar este mÃ³dulo, serÃ¡s capaz de:

### ğŸ“ Conceptos Fundamentales
- âœ… Comprender quÃ© es un ReplicaSet y su arquitectura
- âœ… Diferenciar entre ReplicaSets, Pods y Deployments
- âœ… Entender el ciclo de reconciliaciÃ³n de estado

### ğŸ”§ GestiÃ³n Operacional
- âœ… Crear y configurar ReplicaSets con manifiestos YAML
- âœ… Gestionar el ciclo de vida de rÃ©plicas
- âœ… Escalar aplicaciones horizontalmente (manual y automÃ¡tico)
- âœ… Implementar auto-recuperaciÃ³n (self-healing)

### ğŸ¨ Casos de Uso Avanzados
- âœ… Usar selectores de labels efectivamente
- âœ… Gestionar ownership y referencias
- âœ… Identificar limitaciones y migrar a Deployments
- âœ… Aplicar mejores prÃ¡cticas de producciÃ³n

---

## ğŸ“š Prerequisitos

### Conocimientos Previos
- âœ… Completado [MÃ³dulo 05: GestiÃ³n de Pods](../modulo-05-gestion-pods/)
- âœ… Dominio de manifiestos YAML de Pods
- âœ… ComprensiÃ³n de labels y selectors
- âœ… Familiaridad con comandos kubectl bÃ¡sicos
- âœ… Experiencia creando y debuggando Pods

### Entorno TÃ©cnico
```bash
# Verificar Minikube
minikube version  # â‰¥ v1.32.0

# Verificar Docker
docker --version  # â‰¥ 24.0.0

# Verificar kubectl
kubectl version --client  # â‰¥ v1.28.0

# Cluster debe estar corriendo
minikube status
# Expected: Running

# Limpiar recursos del mÃ³dulo anterior
kubectl delete pods --all
kubectl delete replicasets --all 2>/dev/null || true
```

### âš ï¸ Importante: SeparaciÃ³n con MÃ³dulo 05

| Aspecto | MÃ³dulo 05 (Prerequisito) | **MÃ³dulo 06 (Este)** |
|---------|--------------------------|----------------------|
| **Enfoque** | GestiÃ³n de Pods individuales | GestiÃ³n de fleets de rÃ©plicas |
| **Nivel** | OperaciÃ³n bÃ¡sica | Escalado y alta disponibilidad |
| **Contenido** | Pod lifecycle, probes, resources | ReplicaSets, auto-healing, escalado |
| **Objetivo** | Dominar configuraciÃ³n de Pods | Entender controladores de rÃ©plicas |

Si no has completado el MÃ³dulo 05, **hazlo primero** para comprender cÃ³mo gestionar Pods individuales.

---

## ï¿½ï¸ Estructura del MÃ³dulo

Este mÃ³dulo sigue la progresiÃ³n **TeorÃ­a â†’ Ejemplo â†’ Laboratorio**:

| SecciÃ³n | Tema | Contenido |
|---------|------|-----------|
| **1** | [Â¿QuÃ© es un ReplicaSet?](#-1-quÃ©-es-un-replicaset) | Arquitectura, reconciliaciÃ³n, diferencias con Pods |
| **2** | [Manifiestos YAML](#-2-manifiestos-yaml-de-replicasets) | Estructura, campos obligatorios, template |
| **3** | [Selectors y Labels](#-3-selectors-y-gestiÃ³n-de-pods) | matchLabels, matchExpressions, ownership |
| **4** | [Escalado](#-4-escalado-horizontal) | Manual, declarativo, imperativo |
| **5** | [Auto-recuperaciÃ³n](#-5-auto-recuperaciÃ³n-self-healing) | Self-healing, resiliencia, monitoreo |
| **6** | [Limitaciones](#-6-limitaciones-de-replicasets) | Problemas de updates, cuÃ¡ndo usar Deployments |
| **7** | [Best Practices](#-7-best-practices-de-producciÃ³n) | Patrones, antipatrones, seguridad |

---

## ğŸ“ Recursos de Aprendizaje

### Ejemplos PrÃ¡cticos
ğŸ“ **Carpeta**: [`ejemplos/`](./ejemplos/)
- 15+ manifiestos YAML production-ready
- Organizado por tema y complejidad
- Cada ejemplo incluye comentarios explicativos

### Laboratorios Guiados
ğŸ“ **Carpeta**: [`laboratorios/`](./laboratorios/)
- Laboratorios hands-on con verificaciones
- DuraciÃ³n total: ~2-3 horas de prÃ¡ctica
- Incluyen troubleshooting y cleanup

### DocumentaciÃ³n de Referencia
- ğŸ“– [`ejemplos/README.md`](./ejemplos/README.md) - Ãndice completo de ejemplos
- ğŸ“– [`laboratorios/README.md`](./laboratorios/README.md) - GuÃ­a de laboratorios
- ğŸ“˜ **[`RESUMEN-MODULO.md`](./RESUMEN-MODULO.md)** - **GuÃ­a de estudio estructurada** (RECOMENDADO)

---

## ğŸ“ GuÃ­a de Estudio Recomendada

Para maximizar tu aprendizaje, sigue esta ruta estructurada:

```
Fase 1: Conceptos de ReplicaSets (45-60 min)
â”œâ”€ Â¿QuÃ© es un ReplicaSet?
â”œâ”€ Arquitectura y reconciliaciÃ³n
â”œâ”€ Diferencias con Pods y Deployments
â””â”€ Lab 01: Crear primer ReplicaSet

Fase 2: Manifiestos y Selectors (60-90 min)
â”œâ”€ Estructura de manifiestos YAML
â”œâ”€ Selectors: matchLabels y matchExpressions
â”œâ”€ Template de Pods
â””â”€ Lab 02: Manifiestos avanzados

Fase 3: Escalado y Auto-recuperaciÃ³n (60-90 min)
â”œâ”€ Escalado manual vs declarativo
â”œâ”€ Auto-recuperaciÃ³n (self-healing)
â”œâ”€ Ownership y referencias
â””â”€ Lab 03: Escalado bajo carga

Fase 4: Limitaciones y ProducciÃ³n (45-60 min)
â”œâ”€ Limitaciones de ReplicaSets
â”œâ”€ CuÃ¡ndo usar Deployments
â”œâ”€ Best practices de producciÃ³n
â””â”€ Lab 04: MigraciÃ³n a Deployments
```

ğŸ‘‰ **[ABRIR GUÃA DE ESTUDIO](./RESUMEN-MODULO.md)**

---

---

## ğŸ” 1. Â¿QuÃ© es un ReplicaSet?

### **1.1 DefiniciÃ³n y PropÃ³sito**

Un **ReplicaSet** es un **controlador de Kubernetes** que:

> **Garantiza** que un **nÃºmero especÃ­fico** de rÃ©plicas de Pod estÃ©n **corriendo en todo momento**

**CaracterÃ­sticas principales**:
- ğŸ”„ **Auto-recuperaciÃ³n**: Recrea Pods que fallan automÃ¡ticamente
- ğŸ“ˆ **Escalado horizontal**: Gestiona mÃºltiples rÃ©plicas de la misma aplicaciÃ³n
- ğŸ¯ **GestiÃ³n declarativa**: Defines el estado deseado, Kubernetes lo mantiene
- ğŸ·ï¸ **Selector-based**: Usa labels para identificar quÃ© Pods gestionar

---

### **1.2 Â¿Por quÃ© necesitamos ReplicaSets?**

Imagina este escenario **sin** ReplicaSet:

```bash
# Crear un Pod manualmente
kubectl run my-app --image=nginx:alpine

# Pod se ejecuta normalmente
kubectl get pods
# NAME      READY   STATUS    RESTARTS   AGE
# my-app    1/1     Running   0          10s

# âŒ PROBLEMA: Pod es eliminado (fallo de nodo, eliminaciÃ³n accidental, etc.)
kubectl delete pod my-app

# Pod desaparece permanentemente
kubectl get pods
# No resources found
# âŒ AplicaciÃ³n CAÃDA - requiere intervenciÃ³n manual
```

**Con** ReplicaSet:

```bash
# Crear ReplicaSet con 1 rÃ©plica
kubectl apply -f replicaset.yaml

# Pod se ejecuta
kubectl get pods
# NAME           READY   STATUS    RESTARTS   AGE
# my-app-abc12   1/1     Running   0          10s

# Pod es eliminado
kubectl delete pod my-app-abc12

# âœ… ReplicaSet lo recrea AUTOMÃTICAMENTE
kubectl get pods
# NAME           READY   STATUS    RESTARTS   AGE
# my-app-xyz34   1/1     Running   0          2s  â† Nuevo Pod creado
# âœ… AplicaciÃ³n sigue disponible - CERO intervenciÃ³n manual
```

**ConclusiÃ³n**: ReplicaSets proporcionan **resiliencia automÃ¡tica**

---

### **1.3 ReplicaSet vs Pod: ComparaciÃ³n Detallada**

| Aspecto | Pod (Nivel Bajo) | ReplicaSet (Nivel Alto) |
|---------|------------------|-------------------------|
| **PropÃ³sito** | Ejecutar contenedores | Gestionar mÃºltiples Pods |
| **Auto-recuperaciÃ³n** | âŒ No | âœ… SÃ­ |
| **Escalado** | âŒ Manual (crear/eliminar Pods) | âœ… AutomÃ¡tico (cambiar replicas) |
| **Alta disponibilidad** | âŒ Single point of failure | âœ… MÃºltiples rÃ©plicas |
| **GestiÃ³n** | Imperativa | Declarativa |
| **Uso tÃ­pico** | Testing, Jobs Ãºnicos | Aplicaciones stateless en producciÃ³n |
| **Complejidad** | Baja | Media |

**VisualizaciÃ³n**:

```
ğŸ”´ POD (Nivel Bajo)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pod: my-app   â”‚
â”‚   image: nginx  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
  Falla (node crash, OOM, etc.)
     â†“
  âŒ CAÃDA PERMANENTE
  Requiere creaciÃ³n manual


ğŸŸ¢ REPLICASET (Nivel Alto)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ReplicaSet        â”‚
â”‚    replicas: 3       â”‚
â”‚    selector:         â”‚
â”‚      app: my-app     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼             â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pod 1   â”‚  â”‚ Pod 2   â”‚  â”‚ Pod 3   â”‚
â”‚ Running â”‚  â”‚ Running â”‚  â”‚ Running â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“ Falla
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DELETED â”‚  â”‚ Running â”‚  â”‚ Running â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“ ReplicaSet detecta (replicas: 2 < desired: 3)
     â†“ Crea nuevo Pod automÃ¡ticamente
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pod 4   â”‚  â”‚ Pod 2   â”‚  â”‚ Pod 3   â”‚
â”‚ Running â”‚  â”‚ Running â”‚  â”‚ Running â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
âœ… RECUPERACIÃ“N AUTOMÃTICA - CERO downtime
```

---

### **1.4 Arquitectura de un ReplicaSet**

#### **Componentes Clave**

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp-rs            # 1ï¸âƒ£ Nombre del ReplicaSet
spec:
  replicas: 3                # 2ï¸âƒ£ Estado deseado (3 Pods)
  selector:                  # 3ï¸âƒ£ CÃ³mo identificar Pods
    matchLabels:
      app: webapp
  template:                  # 4ï¸âƒ£ Plantilla para crear Pods
    metadata:
      labels:
        app: webapp          # DEBE coincidir con selector
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
```

**ExplicaciÃ³n componente por componente**:

| # | Componente | FunciÃ³n | Ejemplo |
|---|------------|---------|---------|
| 1ï¸âƒ£ | **metadata.name** | Identifica el ReplicaSet | `webapp-rs` |
| 2ï¸âƒ£ | **spec.replicas** | NÃºmero de Pods deseados | `3` |
| 3ï¸âƒ£ | **spec.selector** | Filtro de labels para encontrar Pods | `app: webapp` |
| 4ï¸âƒ£ | **spec.template** | Blueprint para crear nuevos Pods | Pod completo con containers, volumes, etc. |

#### **Flujo de Control (Reconciliation Loop)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          RECONCILIATION LOOP DE REPLICASET                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Paso 1: Leer manifiesto
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Estado Deseado:      â”‚
   â”‚ replicas: 3          â”‚
   â”‚ selector: app=webapp â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
Paso 2: Query al API Server
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ kubectl get pods     â”‚
   â”‚ -l app=webapp        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
Paso 3: Contar Pods actuales
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Estado Actual:       â”‚
   â”‚ Pods encontrados: 2  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
Paso 4: Comparar (desired vs actual)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Desired: 3           â”‚
   â”‚ Actual:  2           â”‚
   â”‚ Diff:   +1 (falta 1) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
Paso 5: Reconciliar (crear/eliminar Pods)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ AcciÃ³n:              â”‚
   â”‚ Crear 1 Pod nuevo    â”‚
   â”‚ usando template      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
Paso 6: Verificar
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Estado Actual:       â”‚
   â”‚ Pods: 3              â”‚
   â”‚ âœ… Reconciliado      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
   Esperar 5s â†’ Repetir desde Paso 2 (loop infinito)
```

**Este loop corre continuamente** cada ~5 segundos

---

### **1.5 Ejemplo PrÃ¡ctico: Observar ReconciliaciÃ³n**

Vamos a **ver en vivo** cÃ³mo funciona el loop de reconciliaciÃ³n:

```bash
# Terminal 1: Observar Pods en tiempo real
kubectl get pods -l app=nginx-demo --watch

# Terminal 2: Crear ReplicaSet
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-demo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-demo
  template:
    metadata:
      labels:
        app: nginx-demo
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
EOF

# Observa en Terminal 1: Pods creÃ¡ndose uno por uno
# nginx-demo-abc12   0/1     ContainerCreating   0          0s
# nginx-demo-abc12   1/1     Running             0          2s
# nginx-demo-def34   0/1     ContainerCreating   0          0s
# nginx-demo-def34   1/1     Running             0          2s
# nginx-demo-ghi56   0/1     ContainerCreating   0          0s
# nginx-demo-ghi56   1/1     Running             0          2s

# Terminal 2: Eliminar un Pod manualmente
POD_NAME=$(kubectl get pods -l app=nginx-demo -o jsonpath='{.items[0].metadata.name}')
kubectl delete pod $POD_NAME

# Observa en Terminal 1: ReconciliaciÃ³n automÃ¡tica
# nginx-demo-abc12   1/1     Terminating   0          30s
# nginx-demo-xyz99   0/1     Pending       0          0s   â† Nuevo Pod creado
# nginx-demo-xyz99   0/1     ContainerCreating   0    0s
# nginx-demo-xyz99   1/1     Running             0    2s
# âœ… ReplicaSet detectÃ³ (2 < 3) y creÃ³ nuevo Pod automÃ¡ticamente
```

ğŸ“„ **Ver ejemplo completo**: [`ejemplos/01-conceptos/demo-reconciliacion.yaml`](./ejemplos/01-conceptos/demo-reconciliacion.yaml)

---

### **1.6 Ownership: Â¿QuiÃ©n posee quÃ©?**

Los Pods creados por un ReplicaSet tienen una relaciÃ³n de **ownership** (propiedad):

```bash
# Ver owner del Pod
kubectl get pod nginx-demo-abc12 -o yaml | grep -A 5 ownerReferences
```

**Salida**:
```yaml
ownerReferences:
- apiVersion: apps/v1
  kind: ReplicaSet           # â† Tipo del dueÃ±o
  name: nginx-demo           # â† Nombre del ReplicaSet dueÃ±o
  uid: 12345-67890-abcde     # â† ID Ãºnico del ReplicaSet
  controller: true           # â† Este ReplicaSet CONTROLA el Pod
  blockOwnerDeletion: true   # â† No puedes eliminar el RS si el Pod existe
```

**Implicaciones**:

| AcciÃ³n | Resultado | ExplicaciÃ³n |
|--------|-----------|-------------|
| Eliminar Pod | âœ… Pod se recrea | RS detecta falta de rÃ©plica |
| Eliminar ReplicaSet | âŒ Pods tambiÃ©n se eliminan | Owner deletion cascade |
| Eliminar RS con `--cascade=orphan` | âœ… Pods sobreviven | Pods quedan huÃ©rfanos |
| Cambiar label de Pod | Pod ya NO es gestionado | RS crea nuevo Pod |

---

### **âœ… Checkpoint 01: VerificaciÃ³n de Conceptos**

Antes de continuar, asegÃºrate de poder responder:

- [ ] Â¿QuÃ© problema resuelven los ReplicaSets?
- [ ] Â¿CuÃ¡l es la diferencia clave entre un Pod y un ReplicaSet?
- [ ] Â¿QuÃ© es el "reconciliation loop"?
- [ ] Â¿QuÃ© son los "owner references"?
- [ ] Â¿QuÃ© pasa si eliminas un Pod gestionado por un ReplicaSet?

ğŸ“ **Laboratorio**: [`laboratorios/lab-01-conceptos-replicasets.md`](./laboratorios/lab-01-conceptos-replicasets.md)
- DuraciÃ³n: 30 minutos
- Experimenta con reconciliaciÃ³n y ownership

---

## ğŸš€ 2. Manifiestos YAML de ReplicaSets

### **2.1 AnatomÃ­a de un Manifiesto ReplicaSet**

Un manifiesto de ReplicaSet tiene **4 secciones principales**:

```yaml
# 1ï¸âƒ£ API VERSION Y KIND
apiVersion: apps/v1      # API Group especÃ­fico para controladores
kind: ReplicaSet         # Tipo de recurso

# 2ï¸âƒ£ METADATA
metadata:
  name: webapp-rs        # Nombre Ãºnico en el namespace
  namespace: default     # Namespace (opcional, default: "default")
  labels:                # Labels del REPLICASET (opcional)
    app: webapp
    tier: frontend
    managed-by: ops-team

# 3ï¸âƒ£ SPEC (EspecificaciÃ³n)
spec:
  replicas: 3            # Estado deseado: 3 Pods

  # 3ï¸âƒ£.1 SELECTOR (Â¿QuÃ© Pods gestionar?)
  selector:
    matchLabels:
      app: webapp        # DEBE coincidir con template.metadata.labels
      version: v1

  # 3ï¸âƒ£.2 TEMPLATE (Blueprint para crear Pods)
  template:
    metadata:
      labels:
        app: webapp      # âš ï¸ DEBE incluir todos los labels del selector
        version: v1
        pod-label: custom  # Puede tener labels adicionales
    spec:
      # AquÃ­ va la especificaciÃ³n COMPLETA del Pod
      containers:
      - name: webapp
        image: nginx:alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
```

---

### **2.2 Campos Obligatorios vs Opcionales**

| Campo | Obligatorio | DescripciÃ³n | Ejemplo |
|-------|-------------|-------------|---------|
| **apiVersion** | âœ… SÃ­ | Siempre `apps/v1` para ReplicaSets | `apps/v1` |
| **kind** | âœ… SÃ­ | Siempre `ReplicaSet` | `ReplicaSet` |
| **metadata.name** | âœ… SÃ­ | Nombre Ãºnico (DNS-1123) | `webapp-rs` |
| **metadata.namespace** | âŒ No | Namespace donde crear el RS | `default` (si omitido) |
| **metadata.labels** | âŒ No | Labels del ReplicaSet mismo | `{app: webapp}` |
| **spec.replicas** | âœ… SÃ­ | NÃºmero de Pods deseados | `3` |
| **spec.selector** | âœ… SÃ­ | CÃ³mo identificar Pods | `matchLabels: {app: webapp}` |
| **spec.template** | âœ… SÃ­ | Blueprint completo del Pod | Ver estructura de Pod |
| **spec.template.metadata.labels** | âœ… SÃ­ | DEBE incluir selector labels | Mismo que selector |

**âš ï¸ Regla CRÃTICA**:

```
spec.selector.matchLabels  âŠ†  spec.template.metadata.labels
       (subconjunto)              (superconjunto)

Los labels del selector DEBEN estar incluidos en los labels del template
```

**Ejemplo vÃ¡lido**:
```yaml
selector:
  matchLabels:
    app: webapp        # âœ… Incluido en template
template:
  metadata:
    labels:
      app: webapp      # âœ… Coincide
      version: v1      # âœ… Label adicional permitido
```

**Ejemplo INVÃLIDO**:
```yaml
selector:
  matchLabels:
    app: webapp        # âŒ NO estÃ¡ en template
template:
  metadata:
    labels:
      application: webapp  # âŒ Label diferente
# Error: selector doesn't match template labels
```

---

### **2.3 Crear tu Primer ReplicaSet**

#### **Ejemplo 1: ReplicaSet Simple**

ğŸ“„ **Archivo**: [`ejemplos/01-basico/01-replicaset-simple.yaml`](./ejemplos/01-basico/01-replicaset-simple.yaml)

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-simple
  labels:
    app: nginx
    tier: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
```

**Crear y verificar**:

```bash
# Aplicar manifiesto
kubectl apply -f ejemplos/01-basico/01-replicaset-simple.yaml

# Ver ReplicaSet
kubectl get rs nginx-simple
# NAME            DESIRED   CURRENT   READY   AGE
# nginx-simple    3         3         3       10s

# Ver Pods creados (con labels)
kubectl get pods -l app=nginx --show-labels
# NAME                  READY   STATUS    LABELS
# nginx-simple-abc12    1/1     Running   app=nginx
# nginx-simple-def34    1/1     Running   app=nginx
# nginx-simple-ghi56    1/1     Running   app=nginx

# Ver detalles del ReplicaSet
kubectl describe rs nginx-simple
```

**Salida de `describe`**:
```
Name:         nginx-simple
Namespace:    default
Selector:     app=nginx
Labels:       app=nginx
              tier=frontend
Replicas:     3 current / 3 desired  â† Estado actual vs deseado
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:alpine
    Port:         80/TCP
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  30s   replicaset-controller  Created pod: nginx-simple-abc12
  Normal  SuccessfulCreate  30s   replicaset-controller  Created pod: nginx-simple-def34
  Normal  SuccessfulCreate  30s   replicaset-controller  Created pod: nginx-simple-ghi56
```

---

#### **Ejemplo 2: ReplicaSet con ConfiguraciÃ³n de ProducciÃ³n**

ğŸ“„ **Archivo**: [`ejemplos/01-basico/02-replicaset-production.yaml`](./ejemplos/01-basico/02-replicaset-production.yaml)

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp-prod
  labels:
    app: webapp
    environment: production
    managed-by: ops
spec:
  replicas: 5
  selector:
    matchLabels:
      app: webapp
      environment: production
  template:
    metadata:
      labels:
        app: webapp
        environment: production
        version: v1.2.0
    spec:
      containers:
      - name: webapp
        image: nginx:alpine
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        
        # Resource management
        resources:
          requests:
            memory: "128Mi"
            cpu: "250m"
          limits:
            memory: "256Mi"
            cpu: "500m"
        
        # Health checks
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 5
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 3
        
        # Environment variables
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: LOG_LEVEL
          value: "info"
      
      # Security context
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
```

**Aplicar**:
```bash
kubectl apply -f ejemplos/01-basico/02-replicaset-production.yaml

# Ver recursos de los Pods
kubectl top pods -l app=webapp

# Ver eventos en tiempo real
kubectl get events --watch --field-selector involvedObject.kind=ReplicaSet
```

---

### **2.4 Template: El Blueprint del Pod**

El campo `spec.template` es **exactamente** lo que pondrÃ­as en un manifiesto de Pod:

```yaml
template:
  # AquÃ­ va UN POD COMPLETO (sin apiVersion/kind)
  metadata:
    labels: {...}
    annotations: {...}
  spec:
    containers: [...]
    volumes: [...]
    initContainers: [...]
    securityContext: {...}
    # ... TODO lo que pondrias en un Pod
```

**Equivalencia**:

```yaml
# Pod standalone
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  labels:
    app: webapp
spec:
  containers:
  - name: nginx
    image: nginx:alpine

# Template en ReplicaSet (MISMO contenido)
template:
  metadata:
    labels:
      app: webapp
  spec:
    containers:
    - name: nginx
      image: nginx:alpine
```

---

### **2.5 Nombrado de Pods Generados**

Los Pods creados por un ReplicaSet tienen nombres generados automÃ¡ticamente:

```
<replicaset-name>-<random-suffix>
```

**Ejemplo**:
```bash
# ReplicaSet llamado: webapp-rs
kubectl get pods
# NAME                READY
# webapp-rs-abc12     1/1    â† webapp-rs + sufijo aleatorio
# webapp-rs-def34     1/1    â† webapp-rs + sufijo aleatorio
# webapp-rs-ghi56     1/1    â† webapp-rs + sufijo aleatorio
```

**CaracterÃ­sticas del sufijo**:
- 5 caracteres alfanumÃ©ricos lowercase
- Generado aleatoriamente por Kubernetes
- Garantiza unicidad
- **NO** puedes controlarlo

---

### **2.6 Ejemplo PrÃ¡ctico: Multi-Contenedor en ReplicaSet**

ğŸ“„ **Archivo**: [`ejemplos/01-basico/03-replicaset-multi-container.yaml`](./ejemplos/01-basico/03-replicaset-multi-container.yaml)

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: app-with-sidecar
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
      pattern: sidecar
  template:
    metadata:
      labels:
        app: webapp
        pattern: sidecar
    spec:
      # Shared volume para comunicaciÃ³n
      volumes:
      - name: shared-logs
        emptyDir: {}
      
      containers:
      # Contenedor principal
      - name: app
        image: nginx:alpine
        volumeMounts:
        - name: shared-logs
          mountPath: /var/log/nginx
        ports:
        - containerPort: 80
      
      # Sidecar para procesar logs
      - name: log-processor
        image: busybox
        command: ["/bin/sh"]
        args:
        - -c
        - |
          while true; do
            echo "Processing logs..."
            tail -f /logs/access.log 2>/dev/null || sleep 5
          done
        volumeMounts:
        - name: shared-logs
          mountPath: /logs
```

**Aplicar y verificar**:
```bash
kubectl apply -f ejemplos/01-basico/03-replicaset-multi-container.yaml

# Ver Pods con mÃºltiples contenedores
kubectl get pods -l pattern=sidecar
# NAME                      READY   STATUS
# app-with-sidecar-abc12    2/2     Running  â† 2 contenedores

# Ver logs del sidecar
kubectl logs app-with-sidecar-abc12 -c log-processor

# Exec en contenedor especÃ­fico
kubectl exec app-with-sidecar-abc12 -c app -- ls /var/log/nginx
```

---

### **2.7 Comandos de GestiÃ³n**

```bash
# CREAR
kubectl apply -f replicaset.yaml
kubectl create -f replicaset.yaml

# LISTAR
kubectl get rs
kubectl get rs -o wide
kubectl get rs --show-labels
kubectl get rs -n <namespace>

# INSPECCIONAR
kubectl describe rs <nombre>
kubectl get rs <nombre> -o yaml
kubectl get rs <nombre> -o json | jq

# EDITAR
kubectl edit rs <nombre>
kubectl apply -f replicaset-updated.yaml

# ELIMINAR
kubectl delete rs <nombre>
kubectl delete rs <nombre> --cascade=orphan  # Mantener Pods
kubectl delete -f replicaset.yaml

# VER PODS DEL REPLICASET
kubectl get pods -l <selector>
kubectl get pods --selector=app=webapp

# VER EVENTOS
kubectl get events --field-selector involvedObject.kind=ReplicaSet
kubectl get events --field-selector involvedObject.name=<rs-name>
```

---

### **âœ… Checkpoint 02: VerificaciÃ³n de Manifiestos**

Antes de continuar, asegÃºrate de poder:

- [ ] Escribir un manifiesto bÃ¡sico de ReplicaSet
- [ ] Identificar los 4 campos obligatorios
- [ ] Explicar la regla selector âŠ† template.labels
- [ ] Crear un ReplicaSet con template de Pod completo
- [ ] Usar kubectl para crear y gestionar ReplicaSets

ğŸ“ **Laboratorio**: [`laboratorios/lab-02-manifiestos-replicasets.md`](./laboratorios/lab-02-manifiestos-replicasets.md)
- DuraciÃ³n: 40 minutos
- Crea ReplicaSets con configuraciones progresivamente complejas

## ğŸ·ï¸ 3. Selectors y GestiÃ³n de Pods

### **3.1 El Rol del Selector**

El **selector** es el mecanismo que ReplicaSet usa para **identificar quÃ© Pods gestionar**:

```yaml
spec:
  selector:
    matchLabels:      # â† BÃºsqueda de Pods
      app: webapp
```

**Flujo de operaciÃ³n**:

```
1. ReplicaSet lee su selector
   â†“
2. Query al API Server: "Dame todos los Pods con labels: app=webapp"
   â†“
3. Kubernetes devuelve lista de Pods que coinciden
   â†“
4. ReplicaSet cuenta Pods (ejemplo: 2 encontrados)
   â†“
5. Compara con desired state (replicas: 3)
   â†“
6. AcciÃ³n: Crear 1 Pod adicional (2 < 3)
```

---

### **3.2 matchLabels: Selector Simple**

**Uso**: Cuando necesitas coincidencia **exacta** de labels

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp-basic
spec:
  replicas: 3
  selector:
    matchLabels:      # Coincidencia AND (todas deben coincidir)
      app: webapp
      tier: frontend
      environment: prod
  template:
    metadata:
      labels:
        app: webapp
        tier: frontend
        environment: prod
        version: v1.0    # â† Label adicional permitido
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
```

**Comportamiento**:
- Busca Pods con **TODOS** los labels especificados
- Operador lÃ³gico: **AND**
- Coincidencia **exacta** de valores

**Ejemplo**:
```bash
# Estos Pods SÃ son gestionados
Labels: {app=webapp, tier=frontend, environment=prod}           âœ…
Labels: {app=webapp, tier=frontend, environment=prod, ver=v1.0} âœ…

# Estos Pods NO son gestionados
Labels: {app=webapp, tier=frontend}                             âŒ Falta environment
Labels: {app=webapp, tier=backend, environment=prod}            âŒ tier diferente
Labels: {app=other, tier=frontend, environment=prod}            âŒ app diferente
```

---

### **3.3 matchExpressions: Selector Avanzado**

**Uso**: Cuando necesitas **condiciones flexibles**

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp-advanced
spec:
  replicas: 3
  selector:
    matchExpressions:
    - key: app
      operator: In
      values: [webapp, webapi]    # app=webapp OR app=webapi
    - key: tier
      operator: NotIn
      values: [database]           # tier != database
    - key: environment
      operator: Exists             # Debe tener label "environment" (cualquier valor)
  template:
    metadata:
      labels:
        app: webapp
        tier: frontend
        environment: production
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
```

**Operadores disponibles**:

| Operador | DescripciÃ³n | Ejemplo | Coincide con |
|----------|-------------|---------|--------------|
| **In** | Valor estÃ¡ en la lista | `key: app, values: [web, api]` | `app=web` o `app=api` |
| **NotIn** | Valor NO estÃ¡ en la lista | `key: tier, values: [db]` | `tier=frontend`, `tier=cache` (NO `tier=db`) |
| **Exists** | Label existe (cualquier valor) | `key: environment` | `environment=prod`, `environment=dev` |
| **DoesNotExist** | Label NO existe | `key: deprecated` | Pods sin label "deprecated" |

ğŸ“„ **Ver ejemplo**: [`ejemplos/02-selectors/01-match-expressions.yaml`](./ejemplos/02-selectors/01-match-expressions.yaml)

---

### **3.4 Combinando matchLabels y matchExpressions**

Puedes combinar ambos mÃ©todos (operador lÃ³gico: **AND**):

```yaml
spec:
  selector:
    matchLabels:              # Todas deben coincidir (AND)
      app: webapp
      tier: frontend
    matchExpressions:         # Todas deben cumplirse (AND)
    - key: environment
      operator: In
      values: [prod, staging]
    - key: deprecated
      operator: DoesNotExist
```

**LÃ³gica resultante**:
```
(app=webapp)  
AND  
(tier=frontend)  
AND  
(environment IN [prod, staging])  
AND  
(deprecated DOES NOT EXIST)
```

**Pods que coinciden**:
```yaml
# âœ… Coincide
labels:
  app: webapp
  tier: frontend
  environment: prod

# âœ… Coincide
labels:
  app: webapp
  tier: frontend
  environment: staging
  version: v2.0

# âŒ NO coincide (environment=dev no estÃ¡ en [prod, staging])
labels:
  app: webapp
  tier: frontend
  environment: dev

# âŒ NO coincide (tiene label "deprecated")
labels:
  app: webapp
  tier: frontend
  environment: prod
  deprecated: "true"
```

ğŸ“„ **Ver ejemplo**: [`ejemplos/02-selectors/02-combined-selectors.yaml`](./ejemplos/02-selectors/02-combined-selectors.yaml)

---

### **3.5 Caso de Uso: AdopciÃ³n de Pods Existentes**

âš ï¸ **PELIGRO**: ReplicaSet puede **adoptar** Pods existentes si coinciden con su selector

**Escenario problemÃ¡tico**:

```bash
# Paso 1: Crear Pods manualmente
kubectl run manual-pod-1 --image=nginx:alpine --labels="app=webapp"
kubectl run manual-pod-2 --image=nginx:alpine --labels="app=webapp"
kubectl run manual-pod-3 --image=nginx:alpine --labels="app=webapp"

# Ver Pods creados
kubectl get pods -l app=webapp
# NAME           READY   STATUS
# manual-pod-1   1/1     Running
# manual-pod-2   1/1     Running
# manual-pod-3   1/1     Running

# Paso 2: Crear ReplicaSet con replicas: 3 y selector app=webapp
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
EOF

# Ver resultado
kubectl get pods -l app=webapp --show-labels
# NAME           READY   LABELS         OWNER
# manual-pod-1   1/1     app=webapp     webapp-rs  â† âŒ ADOPTADO
# manual-pod-2   1/1     app=webapp     webapp-rs  â† âŒ ADOPTADO
# manual-pod-3   1/1     app=webapp     webapp-rs  â† âŒ ADOPTADO

# ReplicaSet ve 3 Pods, no crea nuevos
kubectl get rs webapp-rs
# NAME        DESIRED   CURRENT   READY
# webapp-rs   3         3         3      â† âœ… Ya tiene 3 (adoptÃ³ los manuales)
```

**Â¿Por quÃ© es problemÃ¡tico?**
- Los Pods manuales pueden tener **configuraciÃ³n diferente**
- No fueron creados con el template del ReplicaSet
- Crea **inconsistencias** en el cluster

**SoluciÃ³n**: Eliminar Pods manuales antes de crear ReplicaSet

```bash
# Eliminar Pods manuales
kubectl delete pod manual-pod-1 manual-pod-2 manual-pod-3

# ReplicaSet crearÃ¡ Pods con su template
kubectl get pods -l app=webapp
# NAME               READY   STATUS
# webapp-rs-abc12    1/1     Running  â† Creado por RS
# webapp-rs-def34    1/1     Running  â† Creado por RS
# webapp-rs-ghi56    1/1     Running  â† Creado por RS
```

ğŸ“„ **Ver ejemplo**: [`ejemplos/02-selectors/03-pod-adoption-danger.yaml`](./ejemplos/02-selectors/03-pod-adoption-danger.yaml)

---

### **3.6 Ejemplo PrÃ¡ctico: SegregaciÃ³n de Ambientes**

Usa selectores para segregar ambientes (dev, staging, prod):

ğŸ“„ **Archivo**: [`ejemplos/02-selectors/04-environment-segregation.yaml`](./ejemplos/02-selectors/04-environment-segregation.yaml)

```yaml
# ReplicaSet para PRODUCCIÃ“N
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp-prod
  namespace: production
spec:
  replicas: 5
  selector:
    matchLabels:
      app: webapp
      environment: production
  template:
    metadata:
      labels:
        app: webapp
        environment: production
        version: v2.1.0
    spec:
      containers:
      - name: webapp
        image: nginx:alpine
        resources:
          requests:
            memory: "256Mi"
            cpu: "500m"

---
# ReplicaSet para STAGING
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp-staging
  namespace: staging
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
      environment: staging
  template:
    metadata:
      labels:
        app: webapp
        environment: staging
        version: v2.2.0-rc1
    spec:
      containers:
      - name: webapp
        image: nginx:alpine
        resources:
          requests:
            memory: "128Mi"
            cpu: "250m"
```

**Aplicar y verificar**:
```bash
# Crear namespaces
kubectl create namespace production
kubectl create namespace staging

# Aplicar ambos ReplicaSets
kubectl apply -f ejemplos/02-selectors/04-environment-segregation.yaml

# Ver Pods de producciÃ³n
kubectl get pods -n production -l environment=production
# NAME                READY   LABELS
# webapp-prod-abc12   1/1     app=webapp,environment=production,version=v2.1.0
# webapp-prod-def34   1/1     ...
# ... (5 Pods)

# Ver Pods de staging
kubectl get pods -n staging -l environment=staging
# NAME                  READY   LABELS
# webapp-staging-xyz12  1/1     app=webapp,environment=staging,version=v2.2.0-rc1
# webapp-staging-uvw34  1/1     ...
# ... (2 Pods)
```

---

### **3.7 Inspeccionar Selector de un ReplicaSet**

```bash
# Ver selector del ReplicaSet
kubectl get rs webapp-rs -o jsonpath='{.spec.selector}'
# Output: {"matchLabels":{"app":"webapp","tier":"frontend"}}

# Ver selector formateado
kubectl get rs webapp-rs -o jsonpath='{.spec.selector}' | jq
# {
#   "matchLabels": {
#     "app": "webapp",
#     "tier": "frontend"
#   }
# }

# Listar Pods que coinciden con el selector
kubectl get pods -l app=webapp,tier=frontend

# Verificar owner de un Pod
kubectl get pod webapp-rs-abc12 -o jsonpath='{.metadata.ownerReferences[0].name}'
# Output: webapp-rs
```

---

### **âœ… Checkpoint 03: VerificaciÃ³n de Selectors**

Antes de continuar, asegÃºrate de poder:

- [ ] Explicar quÃ© es un selector y su funciÃ³n
- [ ] Usar `matchLabels` para selecciÃ³n simple
- [ ] Usar `matchExpressions` con los 4 operadores
- [ ] Combinar `matchLabels` y `matchExpressions`
- [ ] Identificar el peligro de adopciÃ³n de Pods
- [ ] Segregar ambientes usando selectores

ğŸ“ **Laboratorio**: [`laboratorios/lab-03-selectors-avanzados.md`](./laboratorios/lab-03-selectors-avanzados.md)
## ğŸ“Š 4. Escalado Horizontal

### **4.1 Â¿QuÃ© es el Escalado Horizontal?**

**Escalado horizontal** = Aumentar/disminuir el **nÃºmero de rÃ©plicas** (Pods)

```
Escalado VERTICAL (NO lo hace ReplicaSet)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pod: 2 GB â”‚   â†’    â”‚  Pod: 4 GB â”‚   MÃ¡s recursos por Pod
â”‚  CPU: 1    â”‚        â”‚  CPU: 2    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Escalado HORIZONTAL (SÃ lo hace ReplicaSet)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pod: 2 GB â”‚   â†’    â”‚  Pod: 2 GB â”‚  â”‚  Pod: 2 GB â”‚  â”‚  Pod: 2 GB â”‚
â”‚  CPU: 1    â”‚        â”‚  CPU: 1    â”‚  â”‚  CPU: 1    â”‚  â”‚  CPU: 1    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  1 rÃ©plica               3 rÃ©plicas (mÃ¡s Pods)
```

**Ventajas del escalado horizontal**:
- âœ… Alta disponibilidad (si 1 Pod falla, hay otros)
- âœ… DistribuciÃ³n de carga
- âœ… Sin downtime durante escalado
- âœ… Costo-efectivo (escala bajo demanda)

---

### **4.2 Escalado Declarativo (Recomendado)**

**MÃ©todo**: Modificar el manifiesto YAML y aplicar

```yaml
# Archivo: replicaset.yaml (original)
spec:
  replicas: 3  # â† Estado actual

# Modificar a:
spec:
  replicas: 5  # â† Nuevo estado deseado
```

```bash
# Aplicar cambios
kubectl apply -f replicaset.yaml
# replicaset.apps/webapp-rs configured

# Observar escalado en tiempo real
kubectl get pods -l app=webapp --watch
# NAME             READY   STATUS              AGE
# webapp-rs-abc    1/1     Running             2m
# webapp-rs-def    1/1     Running             2m
# webapp-rs-ghi    1/1     Running             2m
# webapp-rs-jkl    0/1     ContainerCreating   0s  â† Nuevo
# webapp-rs-mno    0/1     ContainerCreating   0s  â† Nuevo
# webapp-rs-jkl    1/1     Running             3s
# webapp-rs-mno    1/1     Running             3s
```

**âœ… Ventajas**:
- Auditable (cambios en Git)
- Reproducible
- Declarativo (estado deseado)
- Mejor para producciÃ³n

ğŸ“„ **Ver ejemplo**: [`ejemplos/03-escalado/01-escalado-declarativo.yaml`](./ejemplos/03-escalado/01-escalado-declarativo.yaml)

---

### **4.3 Escalado Imperativo**

**MÃ©todo**: Comando `kubectl scale` directo

```bash
# Escalar a 5 rÃ©plicas
kubectl scale rs webapp-rs --replicas=5
# replicaset.apps/webapp-rs scaled

# Verificar
kubectl get rs webapp-rs
# NAME        DESIRED   CURRENT   READY   AGE
# webapp-rs   5         5         5       5m

# Ver nuevos Pods
kubectl get pods -l app=webapp
# NAME             READY   STATUS    AGE
# webapp-rs-abc    1/1     Running   5m
# webapp-rs-def    1/1     Running   5m
# webapp-rs-ghi    1/1     Running   5m
# webapp-rs-jkl    1/1     Running   10s  â† Nuevo
# webapp-rs-mno    1/1     Running   10s  â† Nuevo
```

**âš ï¸ Desventajas**:
- Cambios NO se reflejan en manifiesto
- No auditable
- Se pierde en prÃ³ximo `kubectl apply`

**Uso recomendado**: Solo para testing rÃ¡pido

---

### **4.4 Reducir RÃ©plicas (Scale Down)**

```bash
# Reducir de 5 a 2 rÃ©plicas
kubectl scale rs webapp-rs --replicas=2

# Observar eliminaciÃ³n
kubectl get pods -l app=webapp --watch
# NAME             READY   STATUS        AGE
# webapp-rs-abc    1/1     Running       10m
# webapp-rs-def    1/1     Running       10m
# webapp-rs-ghi    1/1     Terminating   5m   â† Eliminando
# webapp-rs-jkl    1/1     Terminating   5m   â† Eliminando
# webapp-rs-mno    1/1     Terminating   5m   â† Eliminando
```

**Â¿QuÃ© Pods se eliminan?**
- Kubernetes elige **automÃ¡ticamente**
- Generalmente: **mÃ¡s recientes primero**
- Garantiza **terminaciÃ³n graceful** (grace period: 30s)
- No puedes controlar cuÃ¡les se eliminan

---

### **4.5 Ejemplo PrÃ¡ctico: Escalar bajo Carga**

ğŸ“„ **Archivo**: [`ejemplos/03-escalado/02-escalado-bajo-carga.yaml`](./ejemplos/03-escalado/02-escalado-bajo-carga.yaml)

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: web-load-test
spec:
  replicas: 2  # Empezar con 2
  selector:
    matchLabels:
      app: web
      test: load
  template:
    metadata:
      labels:
        app: web
        test: load
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: web-load-svc
spec:
  selector:
    app: web
    test: load
  ports:
  - port: 80
    targetPort: 80
```

**Escenario de prueba**:

```bash
# Paso 1: Crear ReplicaSet y Service
kubectl apply -f ejemplos/03-escalado/02-escalado-bajo-carga.yaml

# Paso 2: Generar carga (en terminal separado)
kubectl run load-generator \
  --image=busybox \
  --restart=Never \
  -- /bin/sh -c "while true; do wget -q -O- http://web-load-svc; done"

# Paso 3: Monitorear recursos
kubectl top pods -l app=web

# Paso 4: Escalar para manejar carga
kubectl scale rs web-load-test --replicas=10

# Paso 5: Ver distribuciÃ³n de carga
kubectl top pods -l app=web
# NAME                  CPU(cores)   MEMORY(bytes)
# web-load-test-abc     45m          32Mi
# web-load-test-def     48m          34Mi
# web-load-test-ghi     42m          31Mi
# ... (distribuciÃ³n entre 10 Pods)

# Cleanup
kubectl delete pod load-generator
kubectl delete -f ejemplos/03-escalado/02-escalado-bajo-carga.yaml
```

---

### **4.6 Escalar a Cero (Scale to Zero)**

```bash
# Escalar a 0 rÃ©plicas
kubectl scale rs webapp-rs --replicas=0

# Ver resultado
kubectl get rs webapp-rs
# NAME        DESIRED   CURRENT   READY   AGE
# webapp-rs   0         0         0       10m

kubectl get pods -l app=webapp
# No resources found
```

**Uso**:
- Detener temporalmente la aplicaciÃ³n
- Ahorrar recursos en ambientes no productivos
- Mantenimiento programado

**âš ï¸ Importante**: El ReplicaSet sigue existiendo (solo sin Pods)

---

### **âœ… Checkpoint 04: VerificaciÃ³n de Escalado**

Antes de continuar, asegÃºrate de poder:

- [ ] Explicar la diferencia entre escalado horizontal y vertical
- [ ] Escalar un ReplicaSet de forma declarativa
- [ ] Escalar un ReplicaSet de forma imperativa
- [ ] Reducir rÃ©plicas y observar terminaciÃ³n de Pods
- [ ] Escalar a cero y volver a escalar
- [ ] Simular escalado bajo carga

ğŸ“ **Laboratorio**: [`laboratorios/lab-04-escalado-horizontal.md`](./laboratorios/lab-04-escalado-horizontal.md)
- DuraciÃ³n: 35 minutos
- Practica escalado en escenarios reales

---

## ğŸ”„ 5. Auto-recuperaciÃ³n (Self-Healing)

### **5.1 Â¿QuÃ© es Self-Healing?**

**Self-healing** = Capacidad de Kubernetes de **detectar y corregir fallos automÃ¡ticamente**

```
Sin ReplicaSet:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pod 1  â”‚  â† Falla
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
  âŒ CAÃDO
  Requiere intervenciÃ³n manual

Con ReplicaSet:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ReplicaSet         â”‚
â”‚     replicas: 3        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
   â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”
   â–¼    â–¼    â–¼
â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”
â”‚ 1 â”‚ â”‚ 2 â”‚ â”‚ 3 â”‚
â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜
   â†“ Pod 2 falla
â”Œâ”€â”€â”€â”       â”Œâ”€â”€â”€â”
â”‚ 1 â”‚   X   â”‚ 3 â”‚
â””â”€â”€â”€â”˜       â””â”€â”€â”€â”˜
   â†“ ReplicaSet detecta (2 < 3)
   â†“ Crea nuevo Pod automÃ¡ticamente
â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”
â”‚ 1 â”‚ â”‚ 4 â”‚ â”‚ 3 â”‚  â† âœ… Recuperado
â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜
```

---

### **5.2 DemostraciÃ³n de Auto-recuperaciÃ³n**

ğŸ“„ **Archivo**: [`ejemplos/04-self-healing/01-auto-recuperacion.yaml`](./ejemplos/04-self-healing/01-auto-recuperacion.yaml)

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: self-healing-demo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: demo
      test: self-healing
  template:
    metadata:
      labels:
        app: demo
        test: self-healing
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
```

**Prueba prÃ¡ctica**:

```bash
# Terminal 1: Observar Pods en tiempo real
kubectl get pods -l test=self-healing --watch

# Terminal 2: Crear ReplicaSet
kubectl apply -f ejemplos/04-self-healing/01-auto-recuperacion.yaml

# Observa en Terminal 1: 3 Pods creÃ¡ndose
# self-healing-demo-abc12   0/1     ContainerCreating   0s
# self-healing-demo-abc12   1/1     Running             2s
# self-healing-demo-def34   0/1     ContainerCreating   0s
# self-healing-demo-def34   1/1     Running             2s
# self-healing-demo-ghi56   0/1     ContainerCreating   0s
# self-healing-demo-ghi56   1/1     Running             2s

# Terminal 2: Eliminar un Pod
POD_NAME=$(kubectl get pods -l test=self-healing -o jsonpath='{.items[0].metadata.name}')
echo "Eliminando Pod: $POD_NAME"
kubectl delete pod $POD_NAME

# Observa en Terminal 1: RecuperaciÃ³n INMEDIATA
# self-healing-demo-abc12   1/1     Terminating         30s
# self-healing-demo-xyz99   0/1     Pending             0s   â† NUEVO POD CREADO
# self-healing-demo-xyz99   0/1     ContainerCreating   0s
# self-healing-demo-xyz99   1/1     Running             2s
# âœ… TIEMPO DE RECUPERACIÃ“N: ~2 segundos
```

**MÃ©tricas observadas**:
- **Detection time**: ~1-2 segundos (reconciliation loop)
- **Recovery time**: ~2-5 segundos (dependiendo de imagen)
- **Total downtime**: ~3-7 segundos

---

### **5.3 Escenarios de Auto-recuperaciÃ³n**

#### **Escenario 1: Pod Crasheado**

```bash
# Crear Pod que crashea despuÃ©s de 10 segundos
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: crash-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: crash-test
  template:
    metadata:
      labels:
        app: crash-test
    spec:
      containers:
      - name: crasher
        image: busybox
        command: ["sh", "-c"]
        args:
        - |
          echo "Starting..."
          sleep 10
          echo "Crashing now!"
          exit 1
EOF

# Observar comportamiento
kubectl get pods -l app=crash-test --watch
# crash-demo-abc   1/1     Running             0s
# crash-demo-abc   0/1     Error               11s  â† CrasheÃ³
# crash-demo-xyz   0/1     Pending             0s   â† Nuevo Pod
# crash-demo-xyz   0/1     ContainerCreating   0s
# crash-demo-xyz   1/1     Running             2s   â† Recuperado

# Ver historial de restarts
kubectl get pods -l app=crash-test
# NAME             READY   STATUS    RESTARTS   AGE
# crash-demo-abc   1/1     Running   0          15s  â† Nuevo Pod
# crash-demo-def   1/1     Running   0          30s
```

#### **Escenario 2: Node Failure**

```bash
# Simular fallo de nodo (en minikube)
minikube ssh "sudo systemctl stop kubelet"

# Ver Pods migrando a nodos disponibles
kubectl get pods -l app=demo -o wide --watch
# NAME           READY   STATUS        NODE
# demo-abc       1/1     Running       node1
# demo-def       1/1     Running       node1   â† Node1 cayÃ³
# demo-ghi       1/1     Running       node2
# demo-def       1/1     Terminating   node1   â† Detectado como caÃ­do
# demo-xyz       0/1     Pending       <none>  â† Recreando
# demo-xyz       0/1     Running       node2   â† Movido a node2
```

#### **Escenario 3: OOMKilled (Out of Memory)**

```bash
# Crear ReplicaSet con lÃ­mite de memoria bajo
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: oom-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: oom-test
  template:
    metadata:
      labels:
        app: oom-test
    spec:
      containers:
      - name: memory-hog
        image: progrium/stress
        args: ["--vm", "1", "--vm-bytes", "200M"]
        resources:
          limits:
            memory: "150Mi"  # â† LÃ­mite < consumo
EOF

# Ver Pods siendo OOMKilled y recreados
kubectl get pods -l app=oom-test --watch
# oom-demo-abc   0/1     OOMKilled           10s
# oom-demo-xyz   0/1     Pending             0s   â† Nuevo Pod
# oom-demo-xyz   0/1     OOMKilled           12s  â† TambiÃ©n OOMKilled
# ... (loop infinito hasta ajustar recursos)
```

---

### **5.4 Monitorear Auto-recuperaciÃ³n**

```bash
# Ver eventos de recreaciÃ³n
kubectl get events --field-selector involvedObject.kind=ReplicaSet,reason=SuccessfulCreate
# LAST SEEN   TYPE     REASON              MESSAGE
# 30s         Normal   SuccessfulCreate    Created pod: demo-abc12
# 15s         Normal   SuccessfulCreate    Created pod: demo-xyz34

# Ver historial de Pods eliminados
kubectl get events --field-selector involvedObject.kind=Pod,reason=Killing
# LAST SEEN   TYPE     REASON   MESSAGE
# 20s         Normal   Killing  Stopping container nginx

# Ver estado del ReplicaSet
kubectl describe rs demo
# Events:
#   Type    Reason            Age   Message
#   ----    ------            ----  -------
#   Normal  SuccessfulCreate  5m    Created pod: demo-abc12
#   Normal  SuccessfulCreate  3m    Created pod: demo-def34
#   Normal  SuccessfulCreate  1m    Created pod: demo-ghi56
#   Normal  SuccessfulCreate  30s   Created pod: demo-xyz99 â† RecuperaciÃ³n
```

---

### **5.5 Limitaciones de Self-Healing**

âš ï¸ **ReplicaSet NO puede resolver**:

| Problema | ReplicaSet | SoluciÃ³n |
|----------|------------|----------|
| **App crashea por bug de cÃ³digo** | âŒ RecrearÃ¡ Pod infinitamente | Arreglar cÃ³digo |
| **ConfiguraciÃ³n incorrecta** | âŒ Pod reinicia constantemente (CrashLoopBackOff) | Corregir ConfigMap/Secret |
| **Recursos insuficientes en cluster** | âŒ Pod queda en Pending | Agregar nodos o liberar recursos |
| **Image pull error** | âŒ Pod queda en ImagePullBackOff | Corregir imagen o registry |
| **Bug en init container** | âŒ Pod queda en Init:Error | Arreglar init container |

**CrashLoopBackOff**: ReplicaSet recrea Pod â†’ Pod crashea â†’ ReplicaSet recrea â†’ ... (loop)

---

### **âœ… Checkpoint 05: VerificaciÃ³n de Self-Healing**

Antes de continuar, asegÃºrate de poder:

- [ ] Explicar quÃ© es self-healing
- [ ] Demostrar auto-recuperaciÃ³n eliminando un Pod
- [ ] Identificar tiempo de detecciÃ³n y recuperaciÃ³n
- [ ] Reconocer escenarios donde self-healing NO funciona
- [ ] Interpretar eventos de recreaciÃ³n de Pods
- [ ] Diagnosticar CrashLoopBackOff

ğŸ“ **Laboratorio**: [`laboratorios/lab-05-self-healing.md`](./laboratorios/lab-05-self-healing.md)
- DuraciÃ³n: 40 minutos
- Simula fallos y observa recuperaciÃ³n automÃ¡tica

---

## âš ï¸ 6. Limitaciones de ReplicaSets

### **6.1 Problema #1: No Actualiza Pods Existentes**

**El problema mÃ¡s crÃ­tico de ReplicaSets**:

```yaml
# Manifiesto INICIAL
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: nginx:1.20-alpine  # â† VersiÃ³n 1.20
```

```bash
# Aplicar manifiesto inicial
kubectl apply -f replicaset.yaml

# Ver Pods con imagen 1.20
kubectl get pods -o jsonpath='{.items[*].spec.containers[0].image}'
# nginx:1.20-alpine nginx:1.20-alpine nginx:1.20-alpine âœ…
```

**Ahora actualizar la imagen**:

```yaml
# Manifiesto ACTUALIZADO
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: nginx:1.21-alpine  # â† Cambiar a 1.21
```

```bash
# Aplicar cambios
kubectl apply -f replicaset.yaml
# replicaset.apps/webapp configured âœ…

# Ver Pods... âŒ SIGUEN CON VERSIÃ“N VIEJA
kubectl get pods -o jsonpath='{.items[*].spec.containers[0].image}'
# nginx:1.20-alpine nginx:1.20-alpine nginx:1.20-alpine
# âŒ NO SE ACTUALIZARON
```

**Â¿Por quÃ©?**
- ReplicaSet solo garantiza **NÃšMERO** de rÃ©plicas
- NO verifica ni actualiza **CONFIGURACIÃ“N** de Pods existentes
- Pods existentes NO se tocan
- Solo Pods **nuevos** usan el template actualizado

**Workaround manual** (tedioso):

```bash
# Eliminar Pods UNO POR UNO manualmente
kubectl delete pod webapp-abc12
# ReplicaSet crea nuevo Pod con imagen 1.21 âœ…

kubectl delete pod webapp-def34
# ReplicaSet crea nuevo Pod con imagen 1.21 âœ…

kubectl delete pod webapp-ghi56
# ReplicaSet crea nuevo Pod con imagen 1.21 âœ…

# Ahora TODOS tienen imagen 1.21
kubectl get pods -o jsonpath='{.items[*].spec.containers[0].image}'
# nginx:1.21-alpine nginx:1.21-alpine nginx:1.21-alpine âœ…
```

**âŒ Problemas de este approach**:
- Manual y tedioso
- Propenso a errores
- **Downtime** (mientras eliminas Pods)
- No escalable (imagina 100 Pods)

---

### **6.2 Problema #2: Sin Rolling Updates**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        COMPARACIÃ“N: REPLICASET vs DEPLOYMENT             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  âŒ REPLICASET (Update Manual):                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Step 1: Cambiar manifiesto          â”‚                â”‚
â”‚  â”‚ Step 2: kubectl apply               â”‚                â”‚
â”‚  â”‚ Step 3: Template actualizado âœ…     â”‚                â”‚
â”‚  â”‚ Step 4: Pods VIEJOS siguen corriendoâ”‚                â”‚
â”‚  â”‚ Step 5: Eliminar Pod 1 manualmente  â”‚                â”‚
â”‚  â”‚ Step 6: Esperar que se cree         â”‚                â”‚
â”‚  â”‚ Step 7: Repetir para Pod 2, 3, 4... â”‚                â”‚
â”‚  â”‚ Step 8: âš ï¸ DOWNTIME durante procesoâ”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                          â”‚
â”‚  âœ… DEPLOYMENT (Update AutomÃ¡tico):                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Step 1: Cambiar manifiesto          â”‚                â”‚
â”‚  â”‚ Step 2: kubectl apply               â”‚                â”‚
â”‚  â”‚ Step 3: Rolling update AUTOMÃTICO   â”‚                â”‚
â”‚  â”‚         â”œâ”€ Crea Pod nuevo (v2)      â”‚                â”‚
â”‚  â”‚         â”œâ”€ Espera que estÃ© Ready    â”‚                â”‚
â”‚  â”‚         â”œâ”€ Elimina Pod viejo (v1)   â”‚                â”‚
â”‚  â”‚         â””â”€ Repite para todos        â”‚                â”‚
â”‚  â”‚ Step 4: âœ… ZERO downtime            â”‚                â”‚
â”‚  â”‚ Step 5: âœ… Rollback automÃ¡tico      â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **6.3 Problema #3: Sin Historial de Versiones**

```bash
# Intentar ver historial
kubectl rollout history rs webapp-rs
# error: replicasets.apps "webapp-rs" is not a valid rollout target

# Intentar rollback
kubectl rollout undo rs webapp-rs
# error: replicasets.apps "webapp-rs" is not a valid rollout target

# âŒ NO HAY ROLLBACK con ReplicaSets
```

---

### **6.4 Problema #4: Sin Estrategias de Despliegue**

**Deployments ofrecen**:
- âœ… **RollingUpdate**: Actualizar gradualmente (default)
- âœ… **Recreate**: Eliminar todos, crear todos (downtime aceptado)
- âœ… **Blue-Green**: Dos ambientes paralelos
- âœ… **Canary**: Desplegar gradualmente a porcentaje de usuarios

**ReplicaSets**:
- âŒ Solo escalado bÃ¡sico
- âŒ Sin control de updates
- âŒ Sin estrategias avanzadas

---

### **6.5 CuÃ¡ndo Usar ReplicaSet vs Deployment**

| CaracterÃ­stica | ReplicaSet | Deployment |
|----------------|------------|------------|
| **Auto-recuperaciÃ³n** | âœ… SÃ­ | âœ… SÃ­ |
| **Escalado horizontal** | âœ… SÃ­ | âœ… SÃ­ |
| **Rolling updates** | âŒ No | âœ… SÃ­ |
| **Rollback** | âŒ No | âœ… SÃ­ |
| **Historial de versiones** | âŒ No | âœ… SÃ­ |
| **Estrategias de deploy** | âŒ No | âœ… SÃ­ (RollingUpdate, Recreate) |
| **Pause/Resume** | âŒ No | âœ… SÃ­ |
| **Status de rollout** | âŒ No | âœ… SÃ­ |
| **Uso recomendado** | ğŸŸ¡ Aprendizaje, testing | ï¿½ï¿½ **ProducciÃ³n** |

**ConclusiÃ³n**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸŸ¡ REPLICASET: Ãštil para...                        â”‚
â”‚  â”œâ”€ Aprender arquitectura de Kubernetes             â”‚
â”‚  â”œâ”€ Entender reconciliation loop                    â”‚
â”‚  â”œâ”€ Testing rÃ¡pido de escalado                      â”‚
â”‚  â””â”€ Base teÃ³rica (Deployments usan ReplicaSets)     â”‚
â”‚                                                     â”‚
â”‚  ğŸŸ¢ DEPLOYMENT: SIEMPRE Ãºsalo para...               â”‚
â”‚  â”œâ”€ Aplicaciones en PRODUCCIÃ“N                      â”‚
â”‚  â”œâ”€ Cualquier aplicaciÃ³n stateless                  â”‚
â”‚  â”œâ”€ Aplicaciones que requieren updates frecuentes   â”‚
â”‚  â””â”€ Aplicaciones que necesitan rollback             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **âœ… Checkpoint 06: VerificaciÃ³n de Limitaciones**

Antes de continuar, asegÃºrate de poder:

- [ ] Explicar por quÃ© ReplicaSets NO actualizan Pods existentes
- [ ] Demostrar el problema de actualizaciÃ³n manual
- [ ] Comparar ReplicaSet vs Deployment
- [ ] Identificar 4 limitaciones clave de ReplicaSets
- [ ] Decidir cuÃ¡ndo usar ReplicaSet vs Deployment
- [ ] Justificar por quÃ© Deployments son mejores para producciÃ³n

ğŸ“ **Laboratorio**: [`laboratorios/lab-06-limitaciones-replicasets.md`](./laboratorios/lab-06-limitaciones-replicasets.md)
- DuraciÃ³n: 35 minutos
- Experimenta con problemas de actualizaciÃ³n

---

## âœ… 7. Best Practices de ProducciÃ³n

### **7.1 Naming Conventions**

**PatrÃ³n recomendado**:

```yaml
metadata:
  name: <app>-<component>-<environment>-rs

# Ejemplos:
# myapp-frontend-prod-rs
# myapp-backend-staging-rs
# myapp-cache-dev-rs
```

**Labels consistentes**:

```yaml
metadata:
  labels:
    app: myapp              # Nombre de la aplicaciÃ³n
    component: frontend     # Componente (frontend, backend, cache, db)
    environment: production # Ambiente (prod, staging, dev)
    tier: web              # Capa arquitectÃ³nica (web, api, data)
    version: v2.1.0        # VersiÃ³n de la aplicaciÃ³n
    managed-by: kubectl    # Herramienta de gestiÃ³n
```

---

### **7.2 Selector Best Practices**

**âœ… Hacer**:

```yaml
# Selector especÃ­fico y Ãºnico
spec:
  selector:
    matchLabels:
      app: myapp
      component: frontend
      environment: production  # â† Segregar por ambiente
```

**âŒ NO Hacer**:

```yaml
# Selector demasiado genÃ©rico
spec:
  selector:
    matchLabels:
      app: myapp  # â† âŒ Puede adoptar Pods de otros componentes
```

**Regla de oro**:
> Selector debe ser **suficientemente especÃ­fico** para evitar adopciones accidentales

---

### **7.3 Resource Management**

**SIEMPRE define requests y limits**:

```yaml
spec:
  template:
    spec:
      containers:
      - name: app
        image: nginx:alpine
        resources:
          requests:
            memory: "128Mi"  # â† MÃ­nimo garantizado
            cpu: "250m"
          limits:
            memory: "256Mi"  # â† MÃ¡ximo permitido
            cpu: "500m"
```

**GuÃ­a de sizing**:

| Tipo de App | CPU Request | CPU Limit | Memory Request | Memory Limit |
|-------------|-------------|-----------|----------------|--------------|
| Web estÃ¡tico | 100m | 200m | 64Mi | 128Mi |
| API REST | 250m | 500m | 128Mi | 256Mi |
| App pesada | 500m | 1000m | 512Mi | 1Gi |

---

### **7.4 Health Checks (Probes)**

**SIEMPRE implementa probes**:

```yaml
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:latest
        
        # Liveness: Â¿EstÃ¡ vivo?
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          failureThreshold: 3
        
        # Readiness: Â¿EstÃ¡ listo para trÃ¡fico?
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 3
        
        # Startup: Â¿Ya arrancÃ³? (apps lentas)
        startupProbe:
          httpGet:
            path: /startup
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 5
          failureThreshold: 30  # 30*5s = 150s mÃ¡ximo
```

---

### **7.5 Security Context**

```yaml
spec:
  template:
    spec:
      # Security context a nivel Pod
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      
      containers:
      - name: app
        image: myapp:latest
        
        # Security context a nivel Container
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE  # Solo si necesita port < 1024
```

---

### **7.6 Antipatrones Comunes**

#### **âŒ AntipatrÃ³n 1: ReplicaSet en ProducciÃ³n**

```yaml
# âŒ NO USES ESTO EN PRODUCCIÃ“N
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-prod
```

**SoluciÃ³n**:

```yaml
# âœ… USA DEPLOYMENT
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-prod
```

---

#### **âŒ AntipatrÃ³n 2: Selector GenÃ©rico**

```yaml
# âŒ PELIGROSO: Puede adoptar Pods no deseados
spec:
  selector:
    matchLabels:
      app: myapp  # Solo 1 label
```

**SoluciÃ³n**:

```yaml
# âœ… ESPECÃFICO: Reduce riesgo
spec:
  selector:
    matchLabels:
      app: myapp
      component: frontend
      environment: production
```

---

#### **âŒ AntipatrÃ³n 3: Sin Resource Limits**

```yaml
# âŒ SIN LÃMITES: Pod puede consumir todo el nodo
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:latest
        # âŒ Sin resources definidos
```

**Consecuencias**:
- Pod puede causar OOM en el nodo
- Afecta a otros Pods
- Cluster inestable

---

#### **âŒ AntipatrÃ³n 4: Sin Health Checks**

```yaml
# âŒ SIN PROBES: ReplicaSet no sabe si app estÃ¡ sana
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:latest
        # âŒ Sin livenessProbe ni readinessProbe
```

**Consecuencias**:
- Pods "Running" pero app crasheada
- TrÃ¡fico enviado a Pods no listos
- Debugging complicado

---

### **7.7 Template de ProducciÃ³n Completo**

ğŸ“„ **Archivo**: [`ejemplos/05-best-practices/production-ready-replicaset.yaml`](./ejemplos/05-best-practices/production-ready-replicaset.yaml)

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp-frontend-prod-rs
  labels:
    app: webapp
    component: frontend
    environment: production
    version: v2.1.0
    managed-by: kubectl
spec:
  replicas: 5
  
  selector:
    matchLabels:
      app: webapp
      component: frontend
      environment: production
  
  template:
    metadata:
      labels:
        app: webapp
        component: frontend
        environment: production
        version: v2.1.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
    
    spec:
      # Security context a nivel Pod
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      
      containers:
      - name: webapp
        image: nginx:alpine
        
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - name: metrics
          containerPort: 9090
          protocol: TCP
        
        # Resources
        resources:
          requests:
            memory: "256Mi"
            cpu: "500m"
          limits:
            memory: "512Mi"
            cpu: "1000m"
        
        # Health checks
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        
        startupProbe:
          httpGet:
            path: /startup
            port: http
          initialDelaySeconds: 0
          periodSeconds: 5
          failureThreshold: 30
        
        # Environment variables
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: LOG_LEVEL
          value: "info"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        
        # Security context
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        
        # Volume mounts (para readOnlyRootFilesystem)
        volumeMounts:
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run
      
      # Volumes
      volumes:
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}
      
      # Node affinity (opcional)
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - webapp
              topologyKey: kubernetes.io/hostname
```

---

### **âœ… Checkpoint 07: VerificaciÃ³n de Best Practices**

Antes de continuar, asegÃºrate de poder:

- [ ] Aplicar naming conventions consistentes
- [ ] Crear selectores especÃ­ficos y seguros
- [ ] Definir resources (requests/limits) apropiados
- [ ] Implementar health checks (liveness/readiness/startup)
- [ ] Aplicar security contexts
- [ ] Identificar 4 antipatrones comunes
- [ ] Crear un template production-ready completo

ğŸ“ **Laboratorio**: [`laboratorios/lab-07-production-ready.md`](./laboratorios/lab-07-production-ready.md)
- DuraciÃ³n: 50 minutos
- Crea ReplicaSet production-ready desde cero

---

## ğŸ“ Resumen del MÃ³dulo

### **Lo que aprendiste**

âœ… **Conceptos fundamentales**:
- QuÃ© es un ReplicaSet y su arquitectura
- Reconciliation loop y owner references
- Diferencias entre Pod y ReplicaSet

âœ… **GestiÃ³n operacional**:
- Crear manifiestos YAML completos
- Usar selectores (matchLabels y matchExpressions)
- Escalar horizontal (declarativo e imperativo)
- Auto-recuperaciÃ³n (self-healing)

âœ… **Limitaciones crÃ­ticas**:
- ReplicaSets NO actualizan Pods existentes
- Sin rolling updates ni rollback
- Sin historial de versiones
- **Usa Deployments en producciÃ³n**

âœ… **Best practices**:
- Naming conventions y labels consistentes
- Resources y health checks obligatorios
- Security contexts y hardening
- Template production-ready

---

### **Puntos Clave para Recordar**

| # | Concepto | Punto Clave |
|---|----------|-------------|
| 1ï¸âƒ£ | **ReplicaSet** | Garantiza **nÃºmero** de rÃ©plicas, NO configuraciÃ³n |
| 2ï¸âƒ£ | **Reconciliation** | Loop continuo cada ~5s: desired vs actual |
| 3ï¸âƒ£ | **Selector** | Debe ser especÃ­fico para evitar adopciones |
| 4ï¸âƒ£ | **Self-healing** | AutomÃ¡tico para Pod failures, NO para bugs |
| 5ï¸âƒ£ | **Escalado** | Horizontal = mÃ¡s Pods, Vertical = mÃ¡s recursos |
| 6ï¸âƒ£ | **LimitaciÃ³n #1** | NO actualiza Pods existentes (problema crÃ­tico) |
| 7ï¸âƒ£ | **ProducciÃ³n** | **SIEMPRE usa Deployments**, NO ReplicaSets |

---

### **Comandos de Referencia RÃ¡pida**

```bash
# CREAR
kubectl apply -f replicaset.yaml
kubectl create -f replicaset.yaml

# LISTAR
kubectl get rs
kubectl get rs -o wide
kubectl get rs --show-labels

# INSPECCIONAR
kubectl describe rs <nombre>
kubectl get rs <nombre> -o yaml

# ESCALAR
kubectl scale rs <nombre> --replicas=<N>
kubectl edit rs <nombre>

# VER PODS
kubectl get pods -l <selector>
kubectl get pods --selector=app=webapp

# ELIMINAR
kubectl delete rs <nombre>
kubectl delete rs <nombre> --cascade=orphan  # Mantener Pods

# EVENTOS
kubectl get events --field-selector involvedObject.kind=ReplicaSet
kubectl get events --field-selector involvedObject.name=<rs-name>
```

---

## ğŸ“š Recursos Adicionales

### **DocumentaciÃ³n Oficial**
- ğŸ“– [ReplicaSets - Kubernetes Docs](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)
- ğŸ“– [Owner References](https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/)
- ğŸ“– [Labels and Selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)

### **PrÃ³ximo MÃ³dulo**

En el **MÃ³dulo 07: Deployments y Rolling Updates**, aprenderÃ¡s:
- âœ… **Rolling updates** automÃ¡ticos sin downtime
- âœ… **Rollback** a versiones anteriores
- âœ… **Estrategias de despliegue** (RollingUpdate, Recreate)
- âœ… **Historial de versiones** y revisiones
- âœ… **Pause/Resume** de deployments
- âœ… **Blue-Green** y **Canary** deployments

**Diferencia clave**:
- **MÃ³dulo 06** (este): GestiÃ³n de **rÃ©plicas** y escalado
- **MÃ³dulo 07**: GestiÃ³n de **versiones** y actualizaciones

---

## ğŸ† VerificaciÃ³n Final de Conocimientos

Antes de continuar al MÃ³dulo 07, deberÃ­as poder responder:

### Conceptos
- [ ] Â¿QuÃ© problema resuelven los ReplicaSets?
- [ ] Â¿CÃ³mo funciona el reconciliation loop?
- [ ] Â¿QuÃ© son los owner references?
- [ ] Â¿CuÃ¡l es la diferencia entre escalado horizontal y vertical?

### Operaciones
- [ ] Â¿CÃ³mo crear un ReplicaSet production-ready?
- [ ] Â¿CÃ³mo escalar un ReplicaSet de 3 a 10 rÃ©plicas?
- [ ] Â¿CÃ³mo verificar que self-healing funciona?
- [ ] Â¿CÃ³mo usar matchExpressions para selectors complejos?

### Limitaciones
- [ ] Â¿Por quÃ© ReplicaSets NO actualizan Pods existentes?
- [ ] Â¿CuÃ¡les son las 4 limitaciones principales?
- [ ] Â¿CuÃ¡ndo usar ReplicaSet vs Deployment?
- [ ] Â¿Por quÃ© Deployments son mejores para producciÃ³n?

### Best Practices
- [ ] Â¿QuÃ© labels debe tener un Pod production-ready?
- [ ] Â¿QuÃ© probes son obligatorias?
- [ ] Â¿QuÃ© security contexts debe aplicar?
- [ ] Menciona 3 antipatrones comunes

---

**ğŸ“… Fecha de actualizaciÃ³n**: Noviembre 2025  
**ğŸ”– VersiÃ³n**: 2.0  
**ğŸ‘¨â€ğŸ’» Autor**: Curso Kubernetes AKS

---

**â¬…ï¸ Anterior**: [MÃ³dulo 05 - GestiÃ³n de Pods](../modulo-05-gestion-pods/README.md)  
**â¡ï¸ Siguiente**: [MÃ³dulo 07 - Deployments y Rolling Updates](../modulo-07-deployments-rollouts/README.md)
