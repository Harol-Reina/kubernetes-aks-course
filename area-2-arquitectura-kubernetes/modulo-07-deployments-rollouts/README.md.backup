# ğŸš€ MÃ³dulo 07: Deployments y Rolling Updates

> **GestiÃ³n de Versiones y Despliegues Sin Downtime en Kubernetes**

---

## ğŸ“Š InformaciÃ³n del MÃ³dulo

| Aspecto | Detalle |
|---------|---------|
| **DuraciÃ³n estimada** | 4-5 horas (teorÃ­a + prÃ¡ctica) |
| **Nivel** | Intermedio-Avanzado |
| **Prerequisitos** | MÃ³dulos 05 (Pods) y 06 (ReplicaSets) |
| **Objetivos** | Dominar Deployments, rolling updates, rollbacks y estrategias de despliegue |
| **Laboratorios** | 8 prÃ¡cticas hands-on |
| **Proyecto final** | Deployment production-ready con CI/CD simulation |

---

## ğŸ¯ Objetivos de Aprendizaje

Al completar este mÃ³dulo serÃ¡s capaz de:

### **Conceptos Fundamentales**
- âœ… Comprender la arquitectura de Deployments y su relaciÃ³n con ReplicaSets
- âœ… Explicar el reconciliation loop de Deployments
- âœ… Identificar cuÃ¡ndo usar Deployment vs ReplicaSet vs StatefulSet
- âœ… Entender owner references en la jerarquÃ­a Deployment â†’ ReplicaSet â†’ Pod

### **Operaciones BÃ¡sicas**
- âœ… Crear Deployments usando manifiestos YAML
- âœ… Actualizar aplicaciones con rolling updates automÃ¡ticos
- âœ… Realizar rollbacks a versiones anteriores
- âœ… Escalar Deployments horizontal
- âœ… Inspeccionar estado y historial de revisiones

### **Operaciones Avanzadas**
- âœ… Configurar estrategias de despliegue (RollingUpdate, Recreate)
- âœ… Ajustar parÃ¡metros: maxSurge, maxUnavailable
- âœ… Implementar Blue-Green deployments
- âœ… Implementar Canary deployments
- âœ… Pausar y reanudar deployments para cambios batch
- âœ… Gestionar historial de revisiones y change causes

### **Best Practices de ProducciÃ³n**
- âœ… Aplicar naming conventions y labels consistentes
- âœ… Definir resources, limits y health checks
- âœ… Configurar readiness/liveness probes correctamente
- âœ… Implementar security contexts
- âœ… Crear templates production-ready
- âœ… Integrar con pipelines CI/CD

---

## ğŸ“‹ Prerequisitos

### **Conocimientos TÃ©cnicos**

Debes dominar estos conceptos del **MÃ³dulo 06**:

| Concepto | Por quÃ© es importante |
|----------|----------------------|
| **ReplicaSets** | Deployments gestionan ReplicaSets internamente |
| **Auto-recuperaciÃ³n** | Deployment hereda self-healing de ReplicaSets |
| **Selectors y Labels** | CrÃ­ticos para identificar Pods gestionados |
| **Limitaciones de ReplicaSets** | Justifican por quÃ© necesitamos Deployments |
| **Escalado horizontal** | Deployment tambiÃ©n escala (pero mejor) |

**âš ï¸ Importante**: Si no has completado el MÃ³dulo 06, **hazlo primero**. Deployments son una capa de abstracciÃ³n sobre ReplicaSets.

### **Entorno TÃ©cnico**

```bash
# Verificar cluster activo
minikube status

# Verificar conexiÃ³n
kubectl cluster-info

# Verificar versiÃ³n (1.25+)
kubectl version --short

# Limpiar recursos del mÃ³dulo anterior
kubectl delete rs --all
kubectl delete pods --all
kubectl delete deploy --all
```

**Requerimientos**:
- âœ… Minikube instalado y corriendo
- âœ… Driver: Docker
- âœ… Kubectl configurado
- âœ… Recursos: 2 CPU, 4GB RAM mÃ­nimo

---

## ğŸ“š Estructura del MÃ³dulo

Este mÃ³dulo estÃ¡ organizado en **8 secciones temÃ¡ticas**:

| # | SecciÃ³n | DuraciÃ³n | Contenido |
|---|---------|----------|-----------|
| **1** | Â¿QuÃ© es un Deployment? | 30 min | DefiniciÃ³n, arquitectura, comparaciÃ³n vs ReplicaSet |
| **2** | CreaciÃ³n de Deployments | 35 min | Manifiestos YAML, anatomÃ­a, comandos kubectl |
| **3** | Rolling Updates | 45 min | Actualizaciones sin downtime, maxSurge, maxUnavailable |
| **4** | Rollback y Versiones | 40 min | Historial de revisiones, undo, rollback automÃ¡tico |
| **5** | Estrategias de Despliegue | 50 min | RollingUpdate vs Recreate, parÃ¡metros avanzados |
| **6** | TÃ©cnicas Avanzadas | 45 min | Blue-Green, Canary, pause/resume |
| **7** | Monitoreo y Troubleshooting | 35 min | Status, events, debugging common issues |
| **8** | Best Practices | 50 min | ProducciÃ³n-ready, security, anti-patterns |

**Total**: ~4.5 horas (teorÃ­a + prÃ¡ctica)

---

## ğŸ—‚ï¸ Recursos de Aprendizaje

### **Archivos del MÃ³dulo**

```
modulo-07-deployments-rollouts/
â”œâ”€â”€ README.md                          # â† TeorÃ­a completa (este archivo)
â”œâ”€â”€ RESUMEN-MODULO.md                  # GuÃ­a de estudio y referencia rÃ¡pida
â”œâ”€â”€ ejemplos/                          # Manifiestos YAML de ejemplo
â”‚   â”œâ”€â”€ 01-basico/
â”‚   â”‚   â”œâ”€â”€ 01-deployment-simple.yaml
â”‚   â”‚   â”œâ”€â”€ 02-deployment-production.yaml
â”‚   â”‚   â””â”€â”€ 03-deployment-multi-container.yaml
â”‚   â”œâ”€â”€ 02-rolling-updates/
â”‚   â”‚   â”œâ”€â”€ 01-rolling-update-demo.yaml
â”‚   â”‚   â”œâ”€â”€ 02-max-surge-unavailable.yaml
â”‚   â”‚   â””â”€â”€ 03-progressive-rollout.yaml
â”‚   â”œâ”€â”€ 03-strategies/
â”‚   â”‚   â”œâ”€â”€ 01-recreate-strategy.yaml
â”‚   â”‚   â”œâ”€â”€ 02-rollingupdate-strategy.yaml
â”‚   â”‚   â””â”€â”€ 03-blue-green-deployment.yaml
â”‚   â”œâ”€â”€ 04-canary/
â”‚   â”‚   â”œâ”€â”€ 01-canary-v1.yaml
â”‚   â”‚   â”œâ”€â”€ 02-canary-v2.yaml
â”‚   â”‚   â””â”€â”€ 03-canary-service.yaml
â”‚   â””â”€â”€ 05-best-practices/
â”‚       â””â”€â”€ production-ready-deployment.yaml
â””â”€â”€ laboratorios/                      # PrÃ¡cticas guiadas
    â”œâ”€â”€ lab-01-crear-primer-deployment.md
    â”œâ”€â”€ lab-02-rolling-updates.md
    â”œâ”€â”€ lab-03-rollback-versiones.md
    â”œâ”€â”€ lab-04-estrategias-despliegue.md
    â”œâ”€â”€ lab-05-blue-green-deployment.md
    â”œâ”€â”€ lab-06-canary-deployment.md
    â”œâ”€â”€ lab-07-troubleshooting.md
    â””â”€â”€ lab-08-production-ready.md
```

### **MetodologÃ­a de Estudio**

Este mÃ³dulo sigue la metodologÃ­a **TeorÃ­a â†’ Ejemplo â†’ PrÃ¡ctica**:

1. **TeorÃ­a**: Lee la explicaciÃ³n conceptual en este README
2. **Ejemplo inline**: Observa ejemplos de cÃ³digo comentados
3. **Archivo de referencia**: Consulta manifiestos en `ejemplos/`
4. **Checkpoint**: Verifica tu comprensiÃ³n
5. **Laboratorio**: Practica hands-on en `laboratorios/`

---

## ğŸš€ GuÃ­a de Estudio Recomendada

### **Fase 1: Fundamentos (DÃ­a 1 - 2 horas)**
- Leer Secciones 1-2
- Completar Labs 1-2
- **Objetivo**: Crear y gestionar Deployments bÃ¡sicos

### **Fase 2: Actualizaciones (DÃ­a 2 - 2 horas)**
- Leer Secciones 3-4
- Completar Labs 3-4
- **Objetivo**: Dominar rolling updates y rollbacks

### **Fase 3: Estrategias Avanzadas (DÃ­a 3 - 2.5 horas)**
- Leer Secciones 5-6
- Completar Labs 5-6
- **Objetivo**: Implementar Blue-Green y Canary

### **Fase 4: ProducciÃ³n (DÃ­a 4 - 2 horas)**
- Leer Secciones 7-8
- Completar Labs 7-8
- **Objetivo**: Production-ready deployments

### **Fase 5: ConsolidaciÃ³n (DÃ­a 5 - 1 hora)**
- Repasar RESUMEN-MODULO.md
- Proyecto final: Deploy full-stack app
- **Objetivo**: Aplicar todo lo aprendido

---

## ï¿½ 1. Â¿QuÃ© es un Deployment?

### **1.1 El Problema que Resuelven los Deployments**

Recordemos el **problema crÃ­tico de ReplicaSets** del MÃ³dulo 06:

```yaml
# ESCENARIO: Tienes un ReplicaSet con nginx:1.20
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: nginx
        image: nginx:1.20-alpine  # â† VersiÃ³n 1.20
        ports:
        - containerPort: 80
```

**Paso 1**: Aplicar manifiesto
```bash
kubectl apply -f replicaset.yaml
# replicaset.apps/webapp-rs created

kubectl get pods
# NAME             READY   STATUS    IMAGE
# webapp-rs-abc    1/1     Running   nginx:1.20-alpine âœ…
# webapp-rs-def    1/1     Running   nginx:1.20-alpine âœ…
# webapp-rs-ghi    1/1     Running   nginx:1.20-alpine âœ…
```

**Paso 2**: Actualizar imagen a nginx:1.21
```yaml
spec:
  template:
    spec:
      containers:
      - image: nginx:1.21-alpine  # â† CAMBIO DE VERSIÃ“N
```

**Paso 3**: Aplicar cambios
```bash
kubectl apply -f replicaset.yaml
# replicaset.apps/webapp-rs configured âœ…

# âŒ PERO... Los Pods SIGUEN con versiÃ³n vieja
kubectl get pods -o jsonpath='{.items[*].spec.containers[0].image}'
# nginx:1.20-alpine nginx:1.20-alpine nginx:1.20-alpine
# âŒ NO SE ACTUALIZARON
```

**Â¿Por quÃ©?**
- ReplicaSet solo garantiza **nÃºmero** de rÃ©plicas
- NO verifica ni actualiza **configuraciÃ³n** de Pods existentes
- Solo Pods **nuevos** usarÃ­an el template actualizado

**Workaround manual** (tedioso y peligroso):
```bash
# Eliminar Pods uno por uno manualmente
kubectl delete pod webapp-rs-abc  # âš ï¸ DOWNTIME
# Esperar que se cree con nueva imagen...
kubectl delete pod webapp-rs-def  # âš ï¸ MÃS DOWNTIME
kubectl delete pod webapp-rs-ghi  # âš ï¸ AÃšN MÃS DOWNTIME
```

**Problemas**:
- âŒ **Downtime** durante eliminaciÃ³n
- âŒ Manual y propenso a errores
- âŒ No escalable (100 Pods = 100 eliminaciones)
- âŒ Sin rollback si algo falla
- âŒ Sin historial de versiones

---

### **1.2 La SoluciÃ³n: Deployments**

```yaml
# MISMO ESCENARIO: Pero con Deployment
apiVersion: apps/v1
kind: Deployment  # â† Cambio de ReplicaSet a Deployment
metadata:
  name: webapp-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: nginx
        image: nginx:1.20-alpine  # â† VersiÃ³n 1.20
        ports:
        - containerPort: 80
```

**Paso 1**: Aplicar Deployment
```bash
kubectl apply -f deployment.yaml
# deployment.apps/webapp-deploy created

kubectl get pods
# NAME                            READY   STATUS    IMAGE
# webapp-deploy-5d7f8c9b-abc      1/1     Running   nginx:1.20-alpine âœ…
# webapp-deploy-5d7f8c9b-def      1/1     Running   nginx:1.20-alpine âœ…
# webapp-deploy-5d7f8c9b-ghi      1/1     Running   nginx:1.20-alpine âœ…
```

**Paso 2**: Actualizar imagen a nginx:1.21
```yaml
spec:
  template:
    spec:
      containers:
      - image: nginx:1.21-alpine  # â† CAMBIO DE VERSIÃ“N
```

**Paso 3**: Aplicar cambios
```bash
kubectl apply -f deployment.yaml
# deployment.apps/webapp-deploy configured âœ…

# âœ… MAGIA: Rolling update automÃ¡tico
kubectl get pods --watch
# NAME                            READY   STATUS              AGE
# webapp-deploy-5d7f8c9b-abc      1/1     Running             2m
# webapp-deploy-5d7f8c9b-def      1/1     Running             2m
# webapp-deploy-5d7f8c9b-ghi      1/1     Running             2m
# webapp-deploy-7c8d9e0f-xyz      0/1     ContainerCreating   0s   â† NUEVO v1.21
# webapp-deploy-7c8d9e0f-xyz      1/1     Running             2s
# webapp-deploy-5d7f8c9b-abc      1/1     Terminating         2m   â† VIEJO eliminado
# webapp-deploy-7c8d9e0f-mno      0/1     ContainerCreating   0s   â† NUEVO v1.21
# webapp-deploy-7c8d9e0f-mno      1/1     Running             2s
# webapp-deploy-5d7f8c9b-def      1/1     Terminating         2m   â† VIEJO eliminado
# webapp-deploy-7c8d9e0f-pqr      0/1     ContainerCreating   0s   â† NUEVO v1.21
# webapp-deploy-7c8d9e0f-pqr      1/1     Running             2s
# webapp-deploy-5d7f8c9b-ghi      1/1     Terminating         2m   â† VIEJO eliminado
# âœ… ACTUALIZACIÃ“N COMPLETA SIN DOWNTIME

# Verificar versiones
kubectl get pods -o jsonpath='{.items[*].spec.containers[0].image}'
# nginx:1.21-alpine nginx:1.21-alpine nginx:1.21-alpine âœ…
```

**Ventajas**:
- âœ… **Zero downtime**: Siempre hay Pods disponibles
- âœ… **AutomÃ¡tico**: No intervenciÃ³n manual
- âœ… **Gradual**: Un Pod a la vez (configurable)
- âœ… **Rollback**: Si falla, vuelve atrÃ¡s automÃ¡ticamente
- âœ… **Historial**: Guarda versiones anteriores

---

### **1.3 DefiniciÃ³n Formal**

Un **Deployment** es un **controlador de alto nivel** en Kubernetes que:

| Capacidad | DescripciÃ³n | Beneficio |
|-----------|-------------|-----------|
| **GestiÃ³n de ReplicaSets** | Crea y gestiona ReplicaSets automÃ¡ticamente | AbstracciÃ³n sobre complejidad |
| **Rolling Updates** | Actualiza Pods gradualmente sin downtime | Alta disponibilidad |
| **Rollback** | Vuelve a versiones anteriores si algo falla | RecuperaciÃ³n rÃ¡pida |
| **Historial de revisiones** | Mantiene hasta 10 versiones por defecto | AuditorÃ­a y troubleshooting |
| **Escalado declarativo** | Define rÃ©plicas deseadas, Kubernetes lo cumple | Simplicidad operacional |
| **Pause/Resume** | Pausa updates para hacer cambios batch | Control fino |
| **Estrategias configurables** | RollingUpdate, Recreate | Flexibilidad segÃºn caso de uso |

---

### **1.4 Deployment vs ReplicaSet: ComparaciÃ³n Completa**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 REPLICASET vs DEPLOYMENT                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  ğŸŸ¡ REPLICASET (GestiÃ³n de RÃ©plicas)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚                                                    â”‚          â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”                        â”‚          â”‚
â”‚  â”‚  â”‚Pod 1â”‚  â”‚Pod 2â”‚  â”‚Pod 3â”‚  â† Mantiene N rÃ©plicas â”‚          â”‚
â”‚  â”‚  â”‚v1.20â”‚  â”‚v1.20â”‚  â”‚v1.20â”‚                        â”‚          â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜                        â”‚          â”‚
â”‚  â”‚                                                    â”‚          â”‚
â”‚  â”‚  Actualizar imagen a v1.21:                        â”‚          â”‚
â”‚  â”‚  âŒ Pods NO se actualizan automÃ¡ticamente          â”‚          â”‚
â”‚  â”‚  âŒ Requiere eliminaciÃ³n manual                    â”‚          â”‚
â”‚  â”‚  âŒ Sin rollback                                   â”‚          â”‚
â”‚  â”‚  âŒ Sin historial                                  â”‚          â”‚
â”‚  â”‚                                                    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                  â”‚
â”‚  ğŸŸ¢ DEPLOYMENT (GestiÃ³n de Versiones)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚                                                    â”‚          â”‚
â”‚  â”‚  ReplicaSet v1 (histÃ³rico)   ReplicaSet v2 (activo)â”‚         â”‚
â”‚  â”‚  replicas: 0                 replicas: 3          â”‚          â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”â”‚     â”‚
â”‚  â”‚  â”‚     â”‚  (ningÃºn Pod)        â”‚Pod 4â”‚  â”‚Pod 5â”‚  â”‚Pod 6â”‚â”‚     â”‚
â”‚  â”‚  â”‚v1.20â”‚                      â”‚v1.21â”‚  â”‚v1.21â”‚  â”‚v1.21â”‚â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜â”‚     â”‚
â”‚  â”‚                                                    â”‚          â”‚
â”‚  â”‚  Actualizar imagen a v1.21:                        â”‚          â”‚
â”‚  â”‚  âœ… Crea nuevo ReplicaSet (v2)                     â”‚          â”‚
â”‚  â”‚  âœ… Escala v2 UP, v1 DOWN gradualmente             â”‚          â”‚
â”‚  â”‚  âœ… Zero downtime                                  â”‚          â”‚
â”‚  â”‚  âœ… Rollback disponible (undo)                     â”‚          â”‚
â”‚  â”‚  âœ… Historial de versiones                         â”‚          â”‚
â”‚  â”‚                                                    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tabla Comparativa**:

| CaracterÃ­stica | ReplicaSet | Deployment |
|----------------|------------|------------|
| **Auto-recuperaciÃ³n** | âœ… SÃ­ | âœ… SÃ­ (vÃ­a ReplicaSets) |
| **Escalado horizontal** | âœ… SÃ­ | âœ… SÃ­ (mejor integrado) |
| **Actualizar configuraciÃ³n** | âŒ Manual | âœ… AutomÃ¡tico (rolling update) |
| **Rolling updates** | âŒ No | âœ… SÃ­ (configurable) |
| **Rollback** | âŒ No | âœ… SÃ­ (a cualquier revisiÃ³n) |
| **Historial de versiones** | âŒ No | âœ… SÃ­ (hasta 10 por defecto) |
| **Estrategias de deploy** | âŒ No | âœ… RollingUpdate, Recreate |
| **Pause/Resume** | âŒ No | âœ… SÃ­ (para cambios batch) |
| **Change causes** | âŒ No | âœ… SÃ­ (auditorÃ­a) |
| **Uso recomendado** | ğŸŸ¡ Aprendizaje | ğŸŸ¢ **PRODUCCIÃ“N** |

---

### **1.5 CuÃ¡ndo Usar Deployment vs Otros Controladores**

| Tipo de AplicaciÃ³n | Controlador Recomendado | Por quÃ© |
|--------------------|------------------------|---------|
| **Web app stateless** (frontend, API REST) | âœ… **Deployment** | No guarda estado, necesita rolling updates |
| **Background workers** (procesamiento async) | âœ… **Deployment** | Stateless, necesita escalado |
| **Bases de datos** (MySQL, PostgreSQL) | âŒ StatefulSet | Necesita identidad persistente y orden |
| **Cache distribuido** (Redis cluster) | âŒ StatefulSet | Requiere networking estable |
| **Jobs puntuales** (migrations, backups) | âŒ Job/CronJob | Tarea finita, no long-running |
| **Daemonset** (log collector, monitoring) | âŒ DaemonSet | Un Pod por nodo |

**Regla de oro**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Â¿Tu aplicaciÃ³n es STATELESS (sin estado)?            â”‚
â”‚  Â¿Necesitas actualizaciones frecuentes?               â”‚
â”‚  Â¿Requieres alta disponibilidad?                      â”‚
â”‚                                                        â”‚
â”‚  SI a las 3 preguntas â†’ âœ… USA DEPLOYMENT              â”‚
â”‚                                                        â”‚
â”‚  Â¿Tu aplicaciÃ³n necesita persistencia de identidad?   â”‚
â”‚  Â¿Requiere orden en inicio/apagado?                   â”‚
â”‚                                                        â”‚
â”‚  SI a alguna â†’ âŒ USA STATEFULSET                      â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **1.6 Arquitectura Interna de un Deployment**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    JERARQUÃA DE OBJETOS                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚      DEPLOYMENT         â”‚
                     â”‚   (Controlador Alto)    â”‚
                     â”‚                         â”‚
                     â”‚  spec:                  â”‚
                     â”‚    replicas: 3          â”‚
                     â”‚    strategy:            â”‚
                     â”‚      type: RollingUpdateâ”‚
                     â”‚      rollingUpdate:     â”‚
                     â”‚        maxSurge: 1      â”‚
                     â”‚        maxUnavailable: 0â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚   Gestiona            â”‚
                      â–¼                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  ReplicaSet v1   â”‚    â”‚  ReplicaSet v2   â”‚
            â”‚  (histÃ³rico)     â”‚    â”‚  (activo)        â”‚
            â”‚                  â”‚    â”‚                  â”‚
            â”‚  replicas: 0     â”‚    â”‚  replicas: 3     â”‚
            â”‚  image: v1.20    â”‚    â”‚  image: v1.21    â”‚
            â”‚  revision: 1     â”‚    â”‚  revision: 2     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚          â”‚          â”‚
                                   â–¼          â–¼          â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ Pod 1  â”‚  â”‚ Pod 2  â”‚  â”‚ Pod 3  â”‚
                            â”‚ v1.21  â”‚  â”‚ v1.21  â”‚  â”‚ v1.21  â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Flujo de ActualizaciÃ³n:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Usuario actualiza spec.template (cambia imagen)
2. Deployment detecta cambio â†’ Crea nuevo ReplicaSet (v2)
3. Deployment Controller:
   - Escala ReplicaSet v2 de 0 â†’ 1 (crea 1 Pod nuevo)
   - Espera que Pod estÃ© Ready
   - Escala ReplicaSet v1 de 3 â†’ 2 (elimina 1 Pod viejo)
   - Repite hasta que v2=3 y v1=0
4. ReplicaSet v1 queda con replicas: 0 (historial)
5. ReplicaSet v2 tiene replicas: 3 (activo)
```

**Owner References (Propiedad)**:

```yaml
# Deployment
metadata:
  name: webapp-deploy
  uid: abc-123-def

---
# ReplicaSet (hijo del Deployment)
metadata:
  name: webapp-deploy-7c8d9e0f
  ownerReferences:
  - apiVersion: apps/v1
    kind: Deployment
    name: webapp-deploy
    uid: abc-123-def  # â† Mismo UID del padre

---
# Pod (hijo del ReplicaSet)
metadata:
  name: webapp-deploy-7c8d9e0f-xyz12
  ownerReferences:
  - apiVersion: apps/v1
    kind: ReplicaSet
    name: webapp-deploy-7c8d9e0f
```

**Cadena de propiedad**:
```
Deployment â†’ ReplicaSet â†’ Pod
 (abuelo)     (padre)    (hijo)
```

**Implicaciones**:
- Si eliminas Deployment â†’ se eliminan ReplicaSets y Pods (cascade delete)
- Si eliminas ReplicaSet â†’ se eliminan Pods
- Si eliminas Pod â†’ ReplicaSet lo recrea (self-healing)

---

### **âœ… Checkpoint 01: Fundamentos de Deployments**

Antes de continuar, asegÃºrate de poder:

- [ ] Explicar el problema que tienen los ReplicaSets con updates
- [ ] Describir cÃ³mo Deployments resuelven ese problema
- [ ] Mencionar 5 ventajas de Deployments sobre ReplicaSets
- [ ] Identificar cuÃ¡ndo usar Deployment vs StatefulSet
- [ ] Dibujar la jerarquÃ­a: Deployment â†’ ReplicaSet â†’ Pod
- [ ] Explicar quÃ© es un rolling update

ğŸ“ **Laboratorio**: [`laboratorios/lab-01-crear-primer-deployment.md`](./laboratorios/lab-01-crear-primer-deployment.md)
- DuraciÃ³n: 30 minutos
- Crea tu primer Deployment
- Observa rolling update en acciÃ³n
- Compara comportamiento vs ReplicaSet

---

## ğŸ—ï¸ 2. CreaciÃ³n y GestiÃ³n de Deployments

### **3.1 JerarquÃ­a de Objetos**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ARQUITECTURA DEPLOYMENT                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   DEPLOYMENT    â”‚
                    â”‚  replicas: 3    â”‚
                    â”‚  strategy:      â”‚
                    â”‚  RollingUpdate  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ gestiona
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                 â”‚
                    â–¼                 â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ ReplicaSet v1  â”‚  â”‚ ReplicaSet v2  â”‚
           â”‚  replicas: 0   â”‚  â”‚  replicas: 3   â”‚ â† ACTIVO
           â”‚  (histÃ³rico)   â”‚  â”‚  (actual)      â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                   â”‚
                    â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚          â”‚        â”‚        â”‚
                    â–¼          â–¼        â–¼        â–¼
               (ningÃºn Pod) â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”
                           â”‚Pod1â”‚  â”‚Pod2â”‚  â”‚Pod3â”‚
                           â”‚v2  â”‚  â”‚v2  â”‚  â”‚v2  â”‚
                           â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜

Flujo:
1. Usuario actualiza Deployment (nueva imagen)
2. Deployment crea nuevo ReplicaSet (v2)
3. Deployment escala v2 UP y v1 DOWN gradualmente
4. ReplicaSet v1 queda con 0 rÃ©plicas (historial)
5. ReplicaSet v2 tiene todas las rÃ©plicas (activo)
```

### **3.2 Componentes Clave**

**1. Deployment (Controller)**
- Gestiona todo el proceso de actualizaciÃ³n
- Decide cuÃ¡ndo crear/eliminar ReplicaSets
- Controla el ritmo del rolling update

**2. ReplicaSets (Versiones)**
- Deployment crea un ReplicaSet por cada versiÃ³n
- ReplicaSet activo: `replicas: N`
- ReplicaSets histÃ³ricos: `replicas: 0`

**3. Pods (Workload)**
- Gestionados por ReplicaSet activo
- Actualizados gradualmente durante rolling update

### **3.3 Owner References**

```yaml
# Deployment
metadata:
  name: my-deployment
  uid: abc-123-def

---
# ReplicaSet (creado por Deployment)
metadata:
  name: my-deployment-5d7f8c9b
  ownerReferences:
  - apiVersion: apps/v1

### **2.1 AnatomÃ­a de un Manifiesto Deployment**

```yaml
apiVersion: apps/v1          # â† API version (siempre apps/v1)
kind: Deployment             # â† Tipo de recurso

metadata:                    # â† Metadatos del Deployment
  name: webapp-deploy
  namespace: default         # â† Namespace (default si se omite)
  labels:
    app: webapp
    tier: frontend
    environment: production
  annotations:
    kubernetes.io/change-cause: "Initial deployment v1.0"

spec:                        # â† EspecificaciÃ³n del Deployment
  replicas: 3                # â† NÃºmero de rÃ©plicas deseadas
  
  selector:                  # â† Selector de Pods (DEBE coincidir con template.labels)
    matchLabels:
      app: webapp
      tier: frontend
  
  strategy:                  # â† Estrategia de actualizaciÃ³n
    type: RollingUpdate      # â† RollingUpdate o Recreate
    rollingUpdate:
      maxSurge: 1            # â† MÃ¡ximo de Pods extras durante update
      maxUnavailable: 0      # â† MÃ¡ximo de Pods no disponibles
  
  revisionHistoryLimit: 10   # â† NÃºmero de ReplicaSets histÃ³ricos a mantener
  progressDeadlineSeconds: 600  # â† Timeout para updates (default: 600s)
  
  template:                  # â† Template del Pod (IDENTICAL al spec de Pod)
    metadata:
      labels:
        app: webapp          # â† DEBE coincidir con selector
        tier: frontend
        version: "v1.0"
    
    spec:                    # â† EspecificaciÃ³n del Pod
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
          name: http
        resources:
          requests:
            memory: "128Mi"
            cpu: "250m"
          limits:
            memory: "256Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: http
          initialDelaySeconds: 5
          periodSeconds: 5
```

**4 Secciones Principales**:

| SecciÃ³n | PropÃ³sito | Obligatorio |
|---------|-----------|-------------|
| **apiVersion** | VersiÃ³n de API de Kubernetes | âœ… SÃ­ (`apps/v1`) |
| **kind** | Tipo de recurso (`Deployment`) | âœ… SÃ­ |
| **metadata** | Nombre, labels, annotations | âœ… SÃ­ |
| **spec** | ConfiguraciÃ³n: replicas, strategy, template | âœ… SÃ­ |

---

### **2.2 Campos Obligatorios vs Opcionales**

| Campo | Obligatorio | Default | DescripciÃ³n |
|-------|-------------|---------|-------------|
| `spec.replicas` | âŒ No | `1` | NÃºmero de rÃ©plicas |
| `spec.selector` | âœ… SÃ­ | - | Selector de Pods (DEBE coincidir) |
| `spec.template` | âœ… SÃ­ | - | Template del Pod |
| `spec.template.metadata.labels` | âœ… SÃ­ | - | Labels del Pod |
| `spec.strategy.type` | âŒ No | `RollingUpdate` | Estrategia de actualizaciÃ³n |
| `spec.strategy.rollingUpdate.maxSurge` | âŒ No | `25%` | Pods extras durante update |
| `spec.strategy.rollingUpdate.maxUnavailable` | âŒ No | `25%` | Pods no disponibles |
| `spec.revisionHistoryLimit` | âŒ No | `10` | Historial de ReplicaSets |
| `spec.progressDeadlineSeconds` | âŒ No | `600` | Timeout para updates |

---

### **2.3 Crear Tu Primer Deployment**

#### **Ejemplo 1: Deployment Simple**

ğŸ“„ **Archivo**: [`ejemplos/01-basico/01-deployment-simple.yaml`](./ejemplos/01-basico/01-deployment-simple.yaml)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-simple
  labels:
    app: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
```

**Aplicar**:

```bash
# Crear Deployment
kubectl apply -f ejemplos/01-basico/01-deployment-simple.yaml
# deployment.apps/webapp-simple created

# Ver Deployment
kubectl get deployments
# NAME            READY   UP-TO-DATE   AVAILABLE   AGE
# webapp-simple   3/3     3            3           10s

# Ver ReplicaSets creados por el Deployment
kubectl get rs
# NAME                      DESIRED   CURRENT   READY   AGE
# webapp-simple-5d7f8c9b    3         3         3       15s

# Ver Pods
kubectl get pods
# NAME                            READY   STATUS    RESTARTS   AGE
# webapp-simple-5d7f8c9b-abc12    1/1     Running   0          20s
# webapp-simple-5d7f8c9b-def34    1/1     Running   0          20s
# webapp-simple-5d7f8c9b-ghi56    1/1     Running   0          20s

# Ver detalles del Deployment
kubectl describe deployment webapp-simple
# Name:                   webapp-simple
# Namespace:              default
# Selector:               app=webapp
# Replicas:               3 desired | 3 updated | 3 total | 3 available
# StrategyType:           RollingUpdate
# RollingUpdateStrategy:  25% max unavailable, 25% max surge
# Pod Template:
#   Labels:  app=webapp
#   Containers:
#    nginx:
#     Image:        nginx:alpine
#     Port:         80/TCP
# Conditions:
#   Type           Status  Reason
#   ----           ------  ------
#   Available      True    MinimumReplicasAvailable
#   Progressing    True    NewReplicaSetAvailable
# Events:
#   Type    Reason             Age   Message
#   ----    ------             ----  -------
#   Normal  ScalingReplicaSet  30s   Scaled up replica set webapp-simple-5d7f8c9b to 3
```

---

#### **Ejemplo 2: Deployment Production-Ready**

ğŸ“„ **Archivo**: [`ejemplos/01-basico/02-deployment-production.yaml`](./ejemplos/01-basico/02-deployment-production.yaml)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-prod
  labels:
    app: webapp
    tier: frontend
    environment: production
    version: "v1.0"
  annotations:
    kubernetes.io/change-cause: "Initial production deployment"
spec:
  replicas: 5
  
  selector:
    matchLabels:
      app: webapp
      tier: frontend
      environment: production
  
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2           # â† Permite 2 Pods extras (total: 7 durante update)
      maxUnavailable: 0     # â† Siempre mantiene mÃ­nimo 5 disponibles
  
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
  
  template:
    metadata:
      labels:
        app: webapp
        tier: frontend
        environment: production
        version: "v1.0"
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        
        resources:
          requests:
            memory: "256Mi"
            cpu: "500m"
          limits:
            memory: "512Mi"
            cpu: "1000m"
        
        livenessProbe:
          httpGet:
            path: /
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /
            port: http
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
      
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - webapp
              topologyKey: kubernetes.io/hostname
```

**CaracterÃ­sticas production-ready**:
- âœ… **5 rÃ©plicas** para alta disponibilidad
- âœ… **maxUnavailable: 0** â†’ Zero downtime garantizado
- âœ… **Resources** definidos (requests & limits)
- âœ… **Health checks** (liveness & readiness)
- âœ… **Anti-affinity** para distribuir Pods en nodos
- âœ… **Change cause** para auditorÃ­a
- âœ… **Environment variables** con fieldRef

---

### **2.4 Comandos de GestiÃ³n de Deployments**

#### **CreaciÃ³n**

```bash
# Crear desde archivo YAML
kubectl apply -f deployment.yaml

# Crear imperativamente (no recomendado para producciÃ³n)
kubectl create deployment webapp --image=nginx:alpine --replicas=3

# Crear con comando completo
kubectl create deployment webapp \
  --image=nginx:alpine \
  --replicas=5 \
  --port=80
```

#### **Lectura (Get)**

```bash
# Listar Deployments
kubectl get deployments
kubectl get deploy              # Alias
kubectl get deploy -o wide      # MÃ¡s info (imÃ¡genes, selector)
kubectl get deploy --show-labels

# Listar en todos los namespaces
kubectl get deploy -A

# Ver como YAML/JSON
kubectl get deploy webapp -o yaml
kubectl get deploy webapp -o json

# Filtrar por labels
kubectl get deploy -l app=webapp
kubectl get deploy -l tier=frontend,environment=production
```

#### **InspecciÃ³n (Describe)**

```bash
# Ver detalles completos
kubectl describe deployment webapp

# Ver secciÃ³n especÃ­fica
kubectl describe deploy webapp | grep -A 10 "Pod Template"
kubectl describe deploy webapp | grep -A 5 "Events"

# Ver status
kubectl get deploy webapp -o jsonpath='{.status}'

# Ver condiciones
kubectl get deploy webapp -o jsonpath='{.status.conditions[*].type}'
# Available Progressing
```

#### **ActualizaciÃ³n (Edit)**

```bash
# Editar interactivamente (abre editor)
kubectl edit deployment webapp

# Actualizar imagen imperativamente
kubectl set image deployment/webapp nginx=nginx:1.21-alpine

# Actualizar mÃºltiples contenedores
kubectl set image deployment/webapp \
  nginx=nginx:1.21-alpine \
  sidecar=sidecar:v2.0

# Actualizar resources
kubectl set resources deployment webapp \
  -c=nginx \
  --requests=cpu=200m,memory=256Mi \
  --limits=cpu=500m,memory=512Mi

# Actualizar con patch
kubectl patch deployment webapp -p '{"spec":{"replicas":5}}'
```

#### **Escalado**

```bash
# Escalar imperativamente
kubectl scale deployment webapp --replicas=10

# Escalar declarativamente (editar YAML y aplicar)
kubectl apply -f deployment.yaml

# Autoscaling (HPA - tema avanzado)
kubectl autoscale deployment webapp --min=3 --max=10 --cpu-percent=80
```

#### **EliminaciÃ³n**

```bash
# Eliminar Deployment (y sus ReplicaSets y Pods)
kubectl delete deployment webapp
kubectl delete -f deployment.yaml

# Eliminar mÃºltiples
kubectl delete deployment webapp1 webapp2

# Eliminar todos del namespace
kubectl delete deployments --all

# Eliminar con grace period
kubectl delete deployment webapp --grace-period=30

# Eliminar sin esperar (force)
kubectl delete deployment webapp --force --grace-period=0
```

---

### **2.5 Inspeccionar ReplicaSets Gestionados**

```bash
# Listar ReplicaSets
kubectl get rs

# Ver ReplicaSets de un Deployment especÃ­fico
kubectl get rs -l app=webapp

# Ver ReplicaSets con owner references
kubectl get rs -o yaml | grep -A 5 ownerReferences

# Ver historial de ReplicaSets (versiones)
kubectl get rs --sort-by=.metadata.creationTimestamp

# Ver ReplicaSet activo vs histÃ³ricos
kubectl get rs
# NAME                  DESIRED   CURRENT   READY
# webapp-5d7f8c9b       0         0         0       â† HistÃ³rico (v1)
# webapp-7c8d9e0f       3         3         3       â† Activo (v2)
```

---

### **2.6 Ver Pods Gestionados por un Deployment**

```bash
# Listar Pods del Deployment
kubectl get pods -l app=webapp

# Ver Pods con mÃ¡s info
kubectl get pods -l app=webapp -o wide
# NAME                      NODE        IMAGE
# webapp-7c8d9e0f-abc       minikube    nginx:alpine

# Ver Pods con owner references
kubectl get pods -l app=webapp -o yaml | grep -A 10 ownerReferences

# Ver quÃ© ReplicaSet gestiona cada Pod
kubectl get pods -l app=webapp -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.metadata.ownerReferences[0].name}{"\n"}{end}'
# webapp-7c8d9e0f-abc12    webapp-7c8d9e0f
# webapp-7c8d9e0f-def34    webapp-7c8d9e0f

# Observar Pods en tiempo real
kubectl get pods -l app=webapp --watch
```

---

### **2.7 Ver Estado y Condiciones**

```bash
# Ver status del Deployment
kubectl get deployment webapp -o jsonpath='{.status}' | jq

# Ver condiciones
kubectl get deployment webapp -o jsonpath='{.status.conditions[*]}' | jq

# Tipos de condiciones:
# - Available:    Pods disponibles >= replicas
# - Progressing:  Update en progreso
# - ReplicaFailure: Fallo al crear Pods

# Ver si Deployment estÃ¡ Available
kubectl get deploy webapp -o jsonpath='{.status.conditions[?(@.type=="Available")].status}'
# True

# Ver rÃ©plicas
kubectl get deploy webapp -o jsonpath='{.status.replicas}'           # Total
kubectl get deploy webapp -o jsonpath='{.status.readyReplicas}'      # Listos
kubectl get deploy webapp -o jsonpath='{.status.availableReplicas}'  # Disponibles
kubectl get deploy webapp -o jsonpath='{.status.updatedReplicas}'    # Actualizados
```

---

### **2.8 Ver Eventos**

```bash
# Ver eventos del Deployment
kubectl get events --field-selector involvedObject.kind=Deployment,involvedObject.name=webapp

# Ver eventos de creaciÃ³n de ReplicaSets
kubectl get events --field-selector involvedObject.kind=ReplicaSet,reason=SuccessfulCreate

# Ver eventos recientes
kubectl get events --sort-by=.metadata.creationTimestamp

# Ver eventos con watch
kubectl get events -w
```

---

### **âœ… Checkpoint 02: CreaciÃ³n y GestiÃ³n**

Antes de continuar, asegÃºrate de poder:

- [ ] Crear un Deployment desde un manifiesto YAML
- [ ] Identificar las 4 secciones principales del manifiesto
- [ ] Explicar la diferencia entre `spec.replicas` y `spec.template`
- [ ] Listar Deployments, ReplicaSets y Pods relacionados
- [ ] Inspeccionar el estado de un Deployment con `describe`
- [ ] Ver eventos de creaciÃ³n y escalado
- [ ] Escalar un Deployment imperativamente
- [ ] Explicar quÃ© es `spec.selector` y por quÃ© debe coincidir con `template.labels`

ğŸ“ **Laboratorio**: [`laboratorios/lab-02-gestion-deployments.md`](./laboratorios/lab-02-gestion-deployments.md)
- DuraciÃ³n: 35 minutos
- Crea Deployments simple y production-ready
- Practica comandos de gestiÃ³n (get, describe, scale)
- Inspecciona ReplicaSets y Pods gestionados
- Observa owner references

---

## ğŸ”„ 3. Rolling Updates: Actualizaciones Sin Downtime

### **3.1 Â¿QuÃ© es un Rolling Update?**

**Rolling Update** = ActualizaciÃ³n **gradual** de Pods, reemplazando versiÃ³n vieja por nueva **sin downtime**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ROLLING UPDATE FLOW                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ESTADO INICIAL (VersiÃ³n v1.20):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ReplicaSet v1 (replicas: 3)                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚ â”‚ Pod v1  â”‚  â”‚ Pod v1  â”‚  â”‚ Pod v1  â”‚                â”‚
â”‚ â”‚ READY   â”‚  â”‚ READY   â”‚  â”‚ READY   â”‚                â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Usuario actualiza imagen a v1.21 â†’ kubectl apply

PASO 1: Crear nuevo ReplicaSet
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ReplicaSet v1 (replicas: 3)   ReplicaSet v2 (replicas: 0)â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚ â”‚ Pod v1  â”‚  â”‚ Pod v1  â”‚  â”‚ Pod v1  â”‚  (ningÃºn Pod)  â”‚
â”‚ â”‚ READY   â”‚  â”‚ READY   â”‚  â”‚ READY   â”‚                â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 2: Escalar v2 UP (1 Pod), v1 DOWN (1 Pod)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ReplicaSet v1 (replicas: 2)   ReplicaSet v2 (replicas: 1)â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚ â”‚ Pod v1  â”‚  â”‚ Pod v1  â”‚      â”‚ Pod v2  â”‚            â”‚
â”‚ â”‚ READY   â”‚  â”‚ READY   â”‚      â”‚ READY   â”‚            â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                               â†‘ NUEVO                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 3: Escalar v2 UP (1 Pod), v1 DOWN (1 Pod)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ReplicaSet v1 (replicas: 1)   ReplicaSet v2 (replicas: 2)â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ Pod v1  â”‚                  â”‚ Pod v2  â”‚  â”‚ Pod v2  â”‚â”‚
â”‚ â”‚ READY   â”‚                  â”‚ READY   â”‚  â”‚ READY   â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 4: Escalar v2 UP (1 Pod), v1 DOWN (0 Pods)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ReplicaSet v1 (replicas: 0)   ReplicaSet v2 (replicas: 3)â”‚
â”‚ (ningÃºn Pod)                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚                              â”‚ Pod v2  â”‚  â”‚ Pod v2  â”‚  â”‚ Pod v2  â”‚â”‚
â”‚                              â”‚ READY   â”‚  â”‚ READY   â”‚  â”‚ READY   â”‚â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    âœ… UPDATE COMPLETADO
```

**Ventajas**:
- âœ… **Zero downtime**: Siempre hay Pods disponibles
- âœ… **Gradual**: Detecta problemas antes de afectar todos los Pods
- âœ… **AutomÃ¡tico**: Kubernetes lo gestiona
- âœ… **Rollback automÃ¡tico**: Si falla, vuelve atrÃ¡s

---

### **3.2 Triggers de Rolling Update**

Un Rolling Update se **activa automÃ¡ticamente** cuando cambias:

| Campo modificado | Activa Rolling Update | Ejemplo |
|------------------|----------------------|---------|
| `spec.template.spec.containers[].image` | âœ… SÃ­ | Cambiar versiÃ³n de imagen |
| `spec.template.metadata.labels` | âœ… SÃ­ | Agregar/modificar labels del Pod |
| `spec.template.spec.containers[].env` | âœ… SÃ­ | Cambiar variables de entorno |
| `spec.template.spec.containers[].resources` | âœ… SÃ­ | Cambiar requests/limits |
| `spec.template.spec.containers[].ports` | âœ… SÃ­ | Cambiar puertos |
| `spec.template.spec.volumes` | âœ… SÃ­ | Cambiar volumes |
| `spec.replicas` | âŒ No | Solo escala (sin recrear Pods) |
| `spec.strategy` | âŒ No | Afecta prÃ³ximo rolling update |
| `metadata.labels` | âŒ No | Labels del Deployment, no del Pod |

**Regla**: Rolling Update se activa si cambias **`spec.template`** (el blueprint del Pod).

---

### **3.3 DemostraciÃ³n PrÃ¡ctica de Rolling Update**

ğŸ“„ **Archivo**: [`ejemplos/02-rolling-updates/01-rolling-update-demo.yaml`](./ejemplos/02-rolling-updates/01-rolling-update-demo.yaml)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rolling-demo
  annotations:
    kubernetes.io/change-cause: "Deployment inicial con nginx:1.20"
spec:
  replicas: 5
  selector:
    matchLabels:
      app: rolling-demo
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: rolling-demo
        version: "v1"
    spec:
      containers:
      - name: nginx
        image: nginx:1.20-alpine  # â† VersiÃ³n inicial
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
```

**Paso 1**: Aplicar versiÃ³n inicial
```bash
kubectl apply -f ejemplos/02-rolling-updates/01-rolling-update-demo.yaml

# Ver Pods creados
kubectl get pods -l app=rolling-demo
# NAME                          READY   STATUS    AGE
# rolling-demo-5d7f8c9b-abc     1/1     Running   10s
# rolling-demo-5d7f8c9b-def     1/1     Running   10s
# rolling-demo-5d7f8c9b-ghi     1/1     Running   10s
# rolling-demo-5d7f8c9b-jkl     1/1     Running   10s
# rolling-demo-5d7f8c9b-mno     1/1     Running   10s

# Ver imagen actual
kubectl get pods -l app=rolling-demo -o jsonpath='{.items[0].spec.containers[0].image}'
# nginx:1.20-alpine âœ…
```

**Paso 2**: Actualizar imagen a nginx:1.21

```yaml
# Modificar en el archivo
spec:
  template:
    metadata:
      labels:
        version: "v2"  # â† Cambiar versiÃ³n
    spec:
      containers:
      - name: nginx
        image: nginx:1.21-alpine  # â† NUEVA VERSIÃ“N
```

```bash
# Actualizar annotation change-cause
kubectl annotate deployment rolling-demo \
  kubernetes.io/change-cause="Actualizar a nginx:1.21" \
  --overwrite

# Aplicar cambios
kubectl apply -f ejemplos/02-rolling-updates/01-rolling-update-demo.yaml
# deployment.apps/rolling-demo configured
```

**Paso 3**: Observar Rolling Update en tiempo real

```bash
# Terminal 1: Observar Pods
kubectl get pods -l app=rolling-demo --watch

# Terminal 2: Observar ReplicaSets
kubectl get rs -l app=rolling-demo --watch

# Terminal 3: Ver progreso del rollout
kubectl rollout status deployment/rolling-demo
# Waiting for deployment "rolling-demo" rollout to finish: 1 out of 5 new replicas have been updated...
# Waiting for deployment "rolling-demo" rollout to finish: 2 out of 5 new replicas have been updated...
# Waiting for deployment "rolling-demo" rollout to finish: 3 out of 5 new replicas have been updated...
# Waiting for deployment "rolling-demo" rollout to finish: 4 out of 5 new replicas have been updated...
# Waiting for deployment "rolling-demo" rollout to finish: 1 old replicas are pending termination...
# deployment "rolling-demo" successfully rolled out âœ…
```

**Salida esperada en Terminal 1**:

```
NAME                          READY   STATUS              AGE
rolling-demo-5d7f8c9b-abc     1/1     Running             2m
rolling-demo-5d7f8c9b-def     1/1     Running             2m
rolling-demo-5d7f8c9b-ghi     1/1     Running             2m
rolling-demo-5d7f8c9b-jkl     1/1     Running             2m
rolling-demo-5d7f8c9b-mno     1/1     Running             2m
rolling-demo-7c8d9e0f-xyz     0/1     ContainerCreating   0s   â† NUEVO v1.21
rolling-demo-7c8d9e0f-xyz     1/1     Running             3s
rolling-demo-5d7f8c9b-abc     1/1     Terminating         2m   â† VIEJO eliminado
rolling-demo-7c8d9e0f-pqr     0/1     ContainerCreating   0s   â† NUEVO v1.21
rolling-demo-7c8d9e0f-pqr     1/1     Running             3s
rolling-demo-5d7f8c9b-def     1/1     Terminating         2m
rolling-demo-7c8d9e0f-rst     0/1     ContainerCreating   0s
rolling-demo-7c8d9e0f-rst     1/1     Running             3s
rolling-demo-5d7f8c9b-ghi     1/1     Terminating         2m
rolling-demo-7c8d9e0f-uvw     0/1     ContainerCreating   0s
rolling-demo-7c8d9e0f-uvw     1/1     Running             3s
rolling-demo-5d7f8c9b-jkl     1/1     Terminating         2m
rolling-demo-7c8d9e0f-xyz2    0/1     ContainerCreating   0s
rolling-demo-7c8d9e0f-xyz2    1/1     Running             3s
rolling-demo-5d7f8c9b-mno     1/1     Terminating         2m
# âœ… TODOS LOS PODS ACTUALIZADOS
```

**Paso 4**: Verificar actualizaciÃ³n completa

```bash
# Ver Pods con nueva imagen
kubectl get pods -l app=rolling-demo -o jsonpath='{.items[*].spec.containers[0].image}'
# nginx:1.21-alpine nginx:1.21-alpine nginx:1.21-alpine nginx:1.21-alpine nginx:1.21-alpine âœ…

# Ver ReplicaSets
kubectl get rs -l app=rolling-demo
# NAME                    DESIRED   CURRENT   READY   AGE
# rolling-demo-5d7f8c9b   0         0         0       5m   â† HistÃ³rico (v1.20)
# rolling-demo-7c8d9e0f   5         5         5       2m   â† Activo (v1.21)
```

---

### **3.4 ParÃ¡metros de Rolling Update: maxSurge y maxUnavailable**

```yaml
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1            # â† Pods EXTRAS permitidos
      maxUnavailable: 0      # â† Pods NO DISPONIBLES permitidos
```

#### **maxSurge**

**DefiniciÃ³n**: NÃºmero **mÃ¡ximo de Pods extras** que pueden existir **sobre** `spec.replicas` durante el rolling update.

```
replicas: 5
maxSurge: 2

Durante update:
- MÃ¡ximo permitido: 5 + 2 = 7 Pods
- Crea 2 nuevos antes de eliminar viejos
```

**Valores**:
- **NÃºmero entero**: `maxSurge: 2` â†’ 2 Pods extras
- **Porcentaje**: `maxSurge: 25%` â†’ 25% de replicas (redondeado arriba)
- **Default**: `25%`

**Ejemplo**:
```yaml
spec:
  replicas: 10
  strategy:
    rollingUpdate:
      maxSurge: 3  # â† Permite hasta 13 Pods durante update (10 + 3)
```

#### **maxUnavailable**

**DefiniciÃ³n**: NÃºmero **mÃ¡ximo de Pods no disponibles** (Terminating o Not Ready) durante el rolling update.

```
replicas: 5
maxUnavailable: 1

Durante update:
- MÃ­nimo disponible: 5 - 1 = 4 Pods
- Puede tener 1 Pod no disponible temporalmente
```

**Valores**:
- **NÃºmero entero**: `maxUnavailable: 1` â†’ 1 Pod no disponible
- **Porcentaje**: `maxUnavailable: 25%` â†’ 25% de replicas (redondeado abajo)
- **Default**: `25%`
- **âš ï¸ Importante**: `maxUnavailable: 0` â†’ **Zero downtime** garantizado

**Ejemplo**:
```yaml
spec:
  replicas: 10
  strategy:
    rollingUpdate:
      maxUnavailable: 0  # â† SIEMPRE 10 Pods disponibles (zero downtime)
      maxSurge: 1        # â† Crea 1 nuevo antes de eliminar viejo
```

---

### **3.5 Escenarios de ConfiguraciÃ³n**

#### **Escenario 1: Zero Downtime (ProducciÃ³n)**

```yaml
spec:
  replicas: 10
  strategy:
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 0  # â† Zero downtime
```

**Comportamiento**:
- Siempre 10 Pods disponibles
- Crea 2 nuevos (total: 12)
- Espera que estÃ©n Ready
- Elimina 2 viejos (vuelve a 10)
- Repite hasta completar

**Uso**: Aplicaciones crÃ­ticas en producciÃ³n

---

#### **Escenario 2: Update RÃ¡pido (Dev/Staging)**

```yaml
spec:
  replicas: 10
  strategy:
    rollingUpdate:
      maxSurge: 5
      maxUnavailable: 5
```

**Comportamiento**:
- Crea 5 nuevos (total: 15)
- Elimina 5 viejos simultÃ¡neamente
- Update mÃ¡s rÃ¡pido (menos iteraciones)
- âš ï¸ Puede tener downtime momentÃ¡neo

**Uso**: Ambientes no crÃ­ticos, prioridad en velocidad

---

#### **Escenario 3: Conservar Recursos**

```yaml
spec:
  replicas: 10
  strategy:
    rollingUpdate:
      maxSurge: 0        # â† Sin Pods extras
      maxUnavailable: 1
```

**Comportamiento**:
- Elimina 1 viejo primero (quedan 9)
- Crea 1 nuevo (vuelve a 10)
- Repite Pod por Pod
- âš ï¸ Update lento
- âš ï¸ Puede tener micro-downtimes

**Uso**: Clusters con recursos limitados

---

### **3.6 Ejemplo PrÃ¡ctico: maxSurge y maxUnavailable**

ğŸ“„ **Archivo**: [`ejemplos/02-rolling-updates/02-max-surge-unavailable.yaml`](./ejemplos/02-rolling-updates/02-max-surge-unavailable.yaml)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: surge-demo
spec:
  replicas: 5
  selector:
    matchLabels:
      app: surge-demo
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2          # â† Permite hasta 7 Pods (5+2)
      maxUnavailable: 0    # â† Siempre mÃ­nimo 5 disponibles
  template:
    metadata:
      labels:
        app: surge-demo
    spec:
      containers:
      - name: nginx
        image: nginx:1.20-alpine
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 3
```

**Probar**:

```bash
# Aplicar
kubectl apply -f ejemplos/02-rolling-updates/02-max-surge-unavailable.yaml

# Actualizar imagen
kubectl set image deployment/surge-demo nginx=nginx:1.21-alpine

# Observar en tiempo real
kubectl get pods -l app=surge-demo --watch

# Ver rollout status
kubectl rollout status deployment/surge-demo
```

**ObservarÃ¡s**:
1. Se crean 2 Pods nuevos primero (total: 7)
2. Esperan estar Ready
3. Eliminan 2 Pods viejos (vuelve a 5)
4. Repite hasta completar

---

### **âœ… Checkpoint 03: Rolling Updates**

Antes de continuar, asegÃºrate de poder:

- [ ] Explicar quÃ© es un rolling update
- [ ] Describir el flujo: crear ReplicaSet v2 â†’ escalar UP/DOWN gradualmente
- [ ] Mencionar 5 cambios que activan rolling update
- [ ] Explicar `maxSurge` y `maxUnavailable` con ejemplos
- [ ] Configurar zero downtime (maxUnavailable: 0)
- [ ] Usar `kubectl rollout status` para ver progreso
- [ ] Observar rolling update en tiempo real con `--watch`
- [ ] Identificar ReplicaSets histÃ³ricos vs activos

ğŸ“ **Laboratorio**: [`laboratorios/lab-03-rolling-updates.md`](./laboratorios/lab-03-rolling-updates.md)
- DuraciÃ³n: 45 minutos
- Practica rolling updates con diferentes configuraciones
- Experimenta con maxSurge y maxUnavailable
- Simula escenarios: zero downtime, update rÃ¡pido, recursos limitados
- Monitorea progreso del rollout

---

## âª 4. Rollback y GestiÃ³n de Versiones

### **4.1 Historial de Revisiones**

Kubernetes **guarda automÃ¡ticamente** las versiones previas de tu Deployment mediante ReplicaSets histÃ³ricos.

```bash
# Ver historial de rollout
kubectl rollout history deployment/webapp
# REVISION  CHANGE-CAUSE
# 1         Initial deployment v1.0
# 2         Actualizar a nginx:1.21
# 3         Aumentar replicas a 10
# 4         Actualizar resources (CPU/RAM)
```

**Conceptos**:
- **Revision**: NÃºmero secuencial de cada cambio en `spec.template`
- **Change-Cause**: AnotaciÃ³n `kubernetes.io/change-cause` (opcional pero recomendada)
- **ReplicaSet histÃ³rico**: ReplicaSet con `replicas: 0`

---

### **4.2 Configurar revisionHistoryLimit**

```yaml
spec:
  revisionHistoryLimit: 10  # â† Mantiene 10 ReplicaSets histÃ³ricos (default: 10)
```

**Valores**:
- `revisionHistoryLimit: 10` â†’ Mantiene Ãºltimas 10 revisiones
- `revisionHistoryLimit: 3` â†’ Mantiene Ãºltimas 3 (menos recursos)
- `revisionHistoryLimit: 0` â†’ NO mantiene historial (âš ï¸ no podrÃ¡s hacer rollback)

**Trade-off**:
- âœ… **MÃ¡s revisiones** = MÃ¡s opciones de rollback, mÃ¡s recursos consumidos
- âœ… **Menos revisiones** = Menos recursos, menos opciones de rollback

---

### **4.3 Ver Detalles de una RevisiÃ³n**

```bash
# Ver detalles de revisiÃ³n especÃ­fica
kubectl rollout history deployment/webapp --revision=2

# Salida esperada:
# deployment.apps/webapp with revision #2
# Pod Template:
#   Labels:       app=webapp
#                 pod-template-hash=7c8d9e0f
#                 version=v2
#   Annotations:  kubernetes.io/change-cause: Actualizar a nginx:1.21
#   Containers:
#    nginx:
#     Image:      nginx:1.21-alpine
#     Port:       80/TCP
#     Host Port:  0/TCP
#     Environment:        <none>
#     Mounts:     <none>
#   Volumes:      <none>
```

**Uso**: Comparar diferencias entre versiones antes de hacer rollback.

---

### **4.4 Rollback Manual**

#### **Rollback a RevisiÃ³n Anterior (Undo)**

```bash
# Rollback a la revisiÃ³n inmediatamente anterior
kubectl rollout undo deployment/webapp
# deployment.apps/webapp rolled back

# Ver progreso
kubectl rollout status deployment/webapp
# Waiting for deployment "webapp" rollout to finish: 1 out of 5 new replicas have been updated...
# deployment "webapp" successfully rolled out âœ…
```

#### **Rollback a RevisiÃ³n EspecÃ­fica**

```bash
# Rollback a revisiÃ³n #2
kubectl rollout undo deployment/webapp --to-revision=2
# deployment.apps/webapp rolled back

# Verificar
kubectl rollout history deployment/webapp
# REVISION  CHANGE-CAUSE
# 1         Initial deployment v1.0
# 3         Aumentar replicas a 10
# 4         Actualizar resources (CPU/RAM)
# 5         Actualizar a nginx:1.21  â† Ahora es la revisiÃ³n 5 (rollback crea nueva revisiÃ³n)
```

**âš ï¸ Importante**: Rollback **crea una nueva revisiÃ³n**, NO restaura el nÃºmero anterior.

---

### **4.5 Ejemplo PrÃ¡ctico de Rollback**

ğŸ“„ **Archivo**: [`ejemplos/03-rollback/01-rollback-demo.yaml`](./ejemplos/03-rollback/01-rollback-demo.yaml)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rollback-demo
  annotations:
    kubernetes.io/change-cause: "v1.0: Deployment inicial"
spec:
  replicas: 5
  revisionHistoryLimit: 5  # â† Mantiene Ãºltimas 5 revisiones
  selector:
    matchLabels:
      app: rollback-demo
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: rollback-demo
        version: "v1.0"
    spec:
      containers:
      - name: nginx
        image: nginx:1.20-alpine
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
```

**Paso 1**: Aplicar versiÃ³n inicial
```bash
kubectl apply -f ejemplos/03-rollback/01-rollback-demo.yaml

# Verificar
kubectl get deployment rollback-demo
kubectl rollout history deployment/rollback-demo
# REVISION  CHANGE-CAUSE
# 1         v1.0: Deployment inicial
```

**Paso 2**: Actualizar a v1.21 (OK)
```bash
# Actualizar change-cause
kubectl annotate deployment rollback-demo \
  kubernetes.io/change-cause="v1.1: Actualizar a nginx:1.21" \
  --overwrite

# Actualizar imagen
kubectl set image deployment/rollback-demo nginx=nginx:1.21-alpine

# Verificar
kubectl rollout status deployment/rollback-demo
# deployment "rollback-demo" successfully rolled out âœ…

kubectl rollout history deployment/rollback-demo
# REVISION  CHANGE-CAUSE
# 1         v1.0: Deployment inicial
# 2         v1.1: Actualizar a nginx:1.21
```

**Paso 3**: Actualizar a v1.22 (FALLA - simular error)
```bash
# Actualizar change-cause
kubectl annotate deployment rollback-demo \
  kubernetes.io/change-cause="v1.2: Actualizar a nginx:1.22 (BUGGY)" \
  --overwrite

# Actualizar a versiÃ³n problemÃ¡tica (simulamos con imagen incorrecta)
kubectl set image deployment/rollback-demo nginx=nginx:1.22-alpine-WRONG

# Observar FALLO
kubectl rollout status deployment/rollback-demo
# Waiting for deployment "rollback-demo" rollout to finish: 1 out of 5 new replicas have been updated...
# Waiting for deployment "rollback-demo" rollout to finish: 1 old replicas are pending termination...
# (âš ï¸ Se queda bloqueado porque la imagen no existe)

# Ver Pods con error
kubectl get pods -l app=rollback-demo
# NAME                             READY   STATUS             RESTARTS   AGE
# rollback-demo-5d7f8c9b-abc       1/1     Running            0          5m   â† v1.21 (viejo, todavÃ­a activo)
# rollback-demo-7c8d9e0f-xyz       0/1     ImagePullBackOff   0          30s  â† v1.22 (nuevo, FALLA)
```

**Paso 4**: Rollback a versiÃ³n anterior (v1.21)
```bash
# Rollback inmediato
kubectl rollout undo deployment/rollback-demo
# deployment.apps/rollback-demo rolled back

# Ver progreso
kubectl rollout status deployment/rollback-demo
# deployment "rollback-demo" successfully rolled out âœ…

# Verificar Pods
kubectl get pods -l app=rollback-demo
# NAME                             READY   STATUS    RESTARTS   AGE
# rollback-demo-5d7f8c9b-abc       1/1     Running   0          6m  â† v1.21 (restaurado)
# rollback-demo-5d7f8c9b-def       1/1     Running   0          6m
# (Todos los Pods vuelven a estar Running)

# Ver historial
kubectl rollout history deployment/rollback-demo
# REVISION  CHANGE-CAUSE
# 1         v1.0: Deployment inicial
# 3         v1.2: Actualizar a nginx:1.22 (BUGGY)  â† Fallo
# 4         v1.1: Actualizar a nginx:1.21          â† Rollback (nueva revisiÃ³n)
```

---

### **4.6 Rollback AutomÃ¡tico (progressDeadlineSeconds)**

```yaml
spec:
  progressDeadlineSeconds: 600  # â† Timeout de 600s (default: 600s = 10 min)
```

**Comportamiento**:
- Si el rolling update no completa en `progressDeadlineSeconds`, Kubernetes marca el Deployment como **Progressing=False**
- âš ï¸ **NO hace rollback automÃ¡tico**, solo marca como fallido
- TÃº decides si hacer rollback manual

**Ver condiciones**:
```bash
kubectl get deployment webapp -o jsonpath='{.status.conditions[?(@.type=="Progressing")]}'

# Si timeout excedido:
# {
#   "type": "Progressing",
#   "status": "False",
#   "reason": "ProgressDeadlineExceeded",
#   "message": "ReplicaSet \"webapp-7c8d9e0f\" has timed out progressing."
# }
```

**Ejemplo**: Update lento que excede timeout
```yaml
spec:
  progressDeadlineSeconds: 60  # â† Solo 60 segundos
  template:
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 90  # â† Tarda 90s en estar Ready (excede timeout)
```

---

### **4.7 Pausar y Reanudar Rolling Updates**

Permite **detener temporalmente** un rolling update en progreso.

#### **Pausar Deployment**

```bash
# Pausar rollout
kubectl rollout pause deployment/webapp
# deployment.apps/webapp paused

# Hacer mÃºltiples cambios sin activar rolling update
kubectl set image deployment/webapp nginx=nginx:1.22-alpine
kubectl set resources deployment/webapp -c=nginx --limits=cpu=1,memory=1Gi

# Reanudar rollout (aplica TODOS los cambios juntos)
kubectl rollout resume deployment/webapp
# deployment.apps/webapp resumed

# Ver progreso
kubectl rollout status deployment/webapp
```

**Uso**:
- Aplicar mÃºltiples cambios como una sola actualizaciÃ³n
- Reducir nÃºmero de rolling updates (menos interrupciones)
- Testing incremental en producciÃ³n (Canary)

---

### **4.8 Ejemplo Avanzado: Pause/Resume**

ğŸ“„ **Archivo**: [`ejemplos/03-rollback/02-pause-resume.yaml`](./ejemplos/03-rollback/02-pause-resume.yaml)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pause-demo
spec:
  replicas: 10
  selector:
    matchLabels:
      app: pause-demo
  template:
    metadata:
      labels:
        app: pause-demo
    spec:
      containers:
      - name: nginx
        image: nginx:1.20-alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
```

**Probar**:
```bash
# Aplicar
kubectl apply -f ejemplos/03-rollback/02-pause-resume.yaml

# Pausar
kubectl rollout pause deployment/pause-demo

# Hacer 3 cambios
kubectl set image deployment/pause-demo nginx=nginx:1.21-alpine
kubectl scale deployment pause-demo --replicas=15
kubectl set resources deployment pause-demo -c=nginx --limits=cpu=500m,memory=512Mi

# Ver que NO se activa rolling update (Deployment pausado)
kubectl get rs -l app=pause-demo
# (Solo 1 ReplicaSet activo, sin nuevos)

# Reanudar (aplica los 3 cambios juntos)
kubectl rollout resume deployment/pause-demo

# Ver rollout
kubectl rollout status deployment/pause-demo
# deployment "pause-demo" successfully rolled out âœ…

# Verificar cambios aplicados
kubectl get deployment pause-demo -o jsonpath='{.spec.replicas}'  # 15
kubectl get pods -l app=pause-demo -o jsonpath='{.items[0].spec.containers[0].image}'  # nginx:1.21-alpine
```

---

### **4.9 Troubleshooting: Rollout Bloqueado**

#### **SÃ­ntoma**: Rolling update se queda "stuck"

```bash
kubectl rollout status deployment/webapp
# Waiting for deployment "webapp" rollout to finish: 1 out of 5 new replicas have been updated...
# (Se queda aquÃ­ indefinidamente)
```

#### **Causas Comunes**:

| Causa | SÃ­ntoma | SoluciÃ³n |
|-------|---------|----------|
| **Imagen no existe** | `ImagePullBackOff` | Verificar nombre/tag imagen |
| **Readiness probe falla** | Pod nunca Ready | Revisar probe config |
| **Resources insuficientes** | `Pending` (no schedule) | Revisar requests/limits |
| **Node selector no match** | `Pending` | Revisar nodeSelector/affinity |
| **PVC no bound** | `Pending` | Verificar PersistentVolumeClaims |

#### **Debugging**:

```bash
# Ver Pods con problemas
kubectl get pods -l app=webapp

# Describir Pod con error
kubectl describe pod <pod-name>

# Ver logs del contenedor
kubectl logs <pod-name> -c <container-name>

# Ver eventos del Deployment
kubectl describe deployment webapp

# Ver ReplicaSets
kubectl get rs -l app=webapp

# Ver condiciones
kubectl get deployment webapp -o jsonpath='{.status.conditions[*]}'
```

#### **Soluciones**:

```bash
# OpciÃ³n 1: Rollback inmediato
kubectl rollout undo deployment/webapp

# OpciÃ³n 2: Corregir problema y re-aplicar
kubectl set image deployment/webapp nginx=nginx:1.21-alpine  # Imagen correcta

# OpciÃ³n 3: Eliminar y recrear
kubectl delete deployment webapp
kubectl apply -f deployment.yaml
```

---

### **âœ… Checkpoint 04: Rollback y Versiones**

Antes de continuar, asegÃºrate de poder:

- [ ] Ver historial de revisiones con `kubectl rollout history`
- [ ] Explicar quÃ© es `revisionHistoryLimit` y su impacto
- [ ] Ver detalles de una revisiÃ³n especÃ­fica
- [ ] Hacer rollback a la revisiÃ³n anterior con `undo`
- [ ] Hacer rollback a revisiÃ³n especÃ­fica con `--to-revision`
- [ ] Explicar que rollback crea una nueva revisiÃ³n
- [ ] Configurar `progressDeadlineSeconds` para timeout
- [ ] Pausar y reanudar rolling updates
- [ ] Diagnosticar rollout bloqueado (ImagePullBackOff, Pending, etc.)

ğŸ“ **Laboratorio**: [`laboratorios/lab-04-rollback-versiones.md`](./laboratorios/lab-04-rollback-versiones.md)
- DuraciÃ³n: 40 minutos
- Practica rollback manual y automÃ¡tico
- Simula fallos de despliegue (imagen incorrecta)
- Experimenta con pause/resume
- Troubleshooting de rollouts bloqueados

---

## ğŸš€ 5. Estrategias de Deployment Avanzadas

### **5.1 RollingUpdate vs Recreate: ComparaciÃ³n**

```yaml
# Estrategia 1: RollingUpdate (DEFAULT)
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0

# Estrategia 2: Recreate
spec:
  strategy:
    type: Recreate
```

| Aspecto | RollingUpdate | Recreate |
|---------|---------------|----------|
| **Downtime** | âœ… Zero downtime (si maxUnavailable: 0) | âŒ Downtime total (elimina todos los Pods) |
| **Velocidad** | ğŸ¢ MÃ¡s lento (gradual) | ğŸš€ MÃ¡s rÃ¡pido (instantÃ¡neo) |
| **Uso de recursos** | ğŸ“ˆ Requiere recursos extras (maxSurge) | ğŸ“‰ Usa solo recursos necesarios |
| **Rollback** | âœ… AutomÃ¡tico (parcial si falla) | âŒ Manual (todo o nada) |
| **Versiones simultÃ¡neas** | âœ… SÃ­ (v1 y v2 coexisten) | âŒ No (solo v2) |
| **Casos de uso** | Web apps, APIs stateless, microservicios | Bases de datos, apps con estado compartido |

---

### **5.2 Estrategia Recreate: CuÃ¡ndo Usarla**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database-deploy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: database
  strategy:
    type: Recreate  # â† Elimina TODOS los Pods antes de crear nuevos
  template:
    metadata:
      labels:
        app: database
    spec:
      containers:
      - name: postgres
        image: postgres:14-alpine
        env:
        - name: POSTGRES_PASSWORD
          value: "secretpassword"
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: postgres-pvc
```

**Comportamiento**:
1. Escala ReplicaSet viejo a 0 (elimina todos los Pods)
2. Espera que todos estÃ©n Terminated
3. Crea ReplicaSet nuevo
4. Escala a `replicas` deseadas

**CuÃ¡ndo usar Recreate**:
- âœ… **Aplicaciones con estado compartido** (bases de datos, Kafka)
- âœ… **Incompatibilidad entre versiones** (v1 y v2 no pueden coexistir)
- âœ… **Recursos limitados** (no hay espacio para Pods extras)
- âœ… **Single replica** (replicas: 1)

**âš ï¸ Downtime**: Durante 10-30 segundos (tiempo de terminar Pods + crear nuevos).

---

### **5.3 Blue-Green Deployment**

**Concepto**: Mantener 2 entornos completos (**Blue** = actual, **Green** = nuevo), cambiar trÃ¡fico instantÃ¡neamente.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BLUE-GREEN DEPLOYMENT                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 1: Entorno Blue (v1) activo
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service (app=webapp, version=blue)        â”‚  â† Apunta a Blue
â”‚        â†“                                   â”‚
â”‚ Deployment Blue (v1)                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Pod v1  â”‚  â”‚ Pod v1  â”‚  â”‚ Pod v1  â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 2: Crear entorno Green (v2) en paralelo
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service (app=webapp, version=blue)        â”‚  â† TodavÃ­a apunta a Blue
â”‚        â†“                                   â”‚
â”‚ Deployment Blue (v1)                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Pod v1  â”‚  â”‚ Pod v1  â”‚  â”‚ Pod v1  â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                            â”‚
â”‚ Deployment Green (v2)                     â”‚  â† Nuevo (testing)
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Pod v2  â”‚  â”‚ Pod v2  â”‚  â”‚ Pod v2  â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 3: Cambiar Service a Green (switch instantÃ¡neo)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service (app=webapp, version=green)       â”‚  â† CambiÃ³ a Green
â”‚        â†“                                   â”‚
â”‚ Deployment Green (v2)                     â”‚  â† ACTIVO
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Pod v2  â”‚  â”‚ Pod v2  â”‚  â”‚ Pod v2  â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                            â”‚
â”‚ Deployment Blue (v1)                      â”‚  â† Standby (rollback rÃ¡pido)
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Pod v1  â”‚  â”‚ Pod v1  â”‚  â”‚ Pod v1  â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 4 (Opcional): Eliminar Blue despuÃ©s de validaciÃ³n
```

**ImplementaciÃ³n en Kubernetes**:

ğŸ“„ **Archivos**:
- [`ejemplos/04-estrategias/01-blue-deployment.yaml`](./ejemplos/04-estrategias/01-blue-deployment.yaml)
- [`ejemplos/04-estrategias/02-green-deployment.yaml`](./ejemplos/04-estrategias/02-green-deployment.yaml)
- [`ejemplos/04-estrategias/03-service.yaml`](./ejemplos/04-estrategias/03-service.yaml)

**Blue Deployment (v1)**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-blue
  labels:
    app: webapp
    version: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
      version: blue
  template:
    metadata:
      labels:
        app: webapp
        version: blue
    spec:
      containers:
      - name: nginx
        image: nginx:1.20-alpine
        ports:
        - containerPort: 80
```

**Green Deployment (v2)**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-green
  labels:
    app: webapp
    version: green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
      version: green
  template:
    metadata:
      labels:
        app: webapp
        version: green
    spec:
      containers:
      - name: nginx
        image: nginx:1.21-alpine  # â† NUEVA VERSIÃ“N
        ports:
        - containerPort: 80
```

**Service** (controla trÃ¡fico):
```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp
    version: blue  # â† Cambia a 'green' para switch
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
```

**Proceso**:
```bash
# 1. Crear Blue (v1)
kubectl apply -f ejemplos/04-estrategias/01-blue-deployment.yaml

# 2. Crear Service apuntando a Blue
kubectl apply -f ejemplos/04-estrategias/03-service.yaml

# 3. Verificar trÃ¡fico a Blue
kubectl get svc webapp-service
kubectl get endpoints webapp-service

# 4. Crear Green (v2) en paralelo
kubectl apply -f ejemplos/04-estrategias/02-green-deployment.yaml

# 5. Testing en Green (sin trÃ¡fico pÃºblico)
kubectl port-forward deployment/webapp-green 8080:80
# curl localhost:8080  (verificar que funciona)

# 6. SWITCH: Cambiar Service a Green (editar YAML)
# Cambiar selector de 'version: blue' a 'version: green'
kubectl apply -f ejemplos/04-estrategias/03-service.yaml

# 7. Verificar trÃ¡fico a Green
kubectl get endpoints webapp-service

# 8. Rollback inmediato (si hay problemas)
# Cambiar selector a 'version: blue'
kubectl apply -f ejemplos/04-estrategias/03-service.yaml

# 9. Eliminar Blue (despuÃ©s de validaciÃ³n)
kubectl delete deployment webapp-blue
```

**Ventajas**:
- âœ… **Rollback instantÃ¡neo** (cambiar selector del Service)
- âœ… **Zero downtime**
- âœ… **Testing completo** antes de switch

**Desventajas**:
- âŒ **Requiere 2x recursos** (Blue + Green simultÃ¡neos)
- âŒ **Complejidad**: Gestionar 2 Deployments

---

### **5.4 Canary Deployment**

**Concepto**: Enviar un **porcentaje pequeÃ±o** de trÃ¡fico a la nueva versiÃ³n (canary) antes de rollout completo.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CANARY DEPLOYMENT                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 1: VersiÃ³n Stable (v1) con 100% trÃ¡fico
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service (app=webapp)                      â”‚  â† 100% trÃ¡fico
â”‚        â†“                                   â”‚
â”‚ Deployment Stable (v1) - replicas: 10    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ...            â”‚
â”‚ â”‚ Pod v1  â”‚  â”‚ Pod v1  â”‚  (10 Pods)      â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 2: Desplegar Canary (v2) con 10% trÃ¡fico
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service (app=webapp)                      â”‚  â† 90% v1 + 10% v2
â”‚        â†“                                   â”‚
â”‚ Deployment Stable (v1) - replicas: 9     â”‚  â† Reducido a 9
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ...            â”‚
â”‚ â”‚ Pod v1  â”‚  â”‚ Pod v1  â”‚  (9 Pods)       â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                            â”‚
â”‚ Deployment Canary (v2) - replicas: 1     â”‚  â† 1 Pod nuevo
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚ â”‚ Pod v2  â”‚                               â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 3: Aumentar trÃ¡fico a Canary (50%)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service (app=webapp)                      â”‚  â† 50% v1 + 50% v2
â”‚        â†“                                   â”‚
â”‚ Deployment Stable (v1) - replicas: 5     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ...                          â”‚
â”‚ â”‚ Pod v1  â”‚  (5 Pods)                     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                            â”‚
â”‚ Deployment Canary (v2) - replicas: 5     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ...                          â”‚
â”‚ â”‚ Pod v2  â”‚  (5 Pods)                     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 4: Promover Canary a 100% (eliminar Stable)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service (app=webapp)                      â”‚  â† 100% v2
â”‚        â†“                                   â”‚
â”‚ Deployment Canary (v2) - replicas: 10    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ...            â”‚
â”‚ â”‚ Pod v2  â”‚  â”‚ Pod v2  â”‚  (10 Pods)      â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ImplementaciÃ³n**:

ğŸ“„ **Archivos**:
- [`ejemplos/04-estrategias/04-stable-deployment.yaml`](./ejemplos/04-estrategias/04-stable-deployment.yaml)
- [`ejemplos/04-estrategias/05-canary-deployment.yaml`](./ejemplos/04-estrategias/05-canary-deployment.yaml)
- [`ejemplos/04-estrategias/06-service-canary.yaml`](./ejemplos/04-estrategias/06-service-canary.yaml)

**Stable Deployment**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-stable
spec:
  replicas: 10  # â† 100% inicialmente
  selector:
    matchLabels:
      app: webapp
      track: stable
  template:
    metadata:
      labels:
        app: webapp
        track: stable
        version: "v1"
    spec:
      containers:
      - name: nginx
        image: nginx:1.20-alpine
        ports:
        - containerPort: 80
```

**Canary Deployment**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-canary
spec:
  replicas: 1  # â† 10% (1 de 10 Pods totales)
  selector:
    matchLabels:
      app: webapp
      track: canary
  template:
    metadata:
      labels:
        app: webapp
        track: canary
        version: "v2"
    spec:
      containers:
      - name: nginx
        image: nginx:1.21-alpine  # â† NUEVA VERSIÃ“N
        ports:
        - containerPort: 80
```

**Service** (balancea entre Stable y Canary):
```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp  # â† Matchea AMBOS (stable y canary)
  ports:
  - port: 80
    targetPort: 80
```

**Proceso**:
```bash
# 1. Desplegar Stable (v1)
kubectl apply -f ejemplos/04-estrategias/04-stable-deployment.yaml
kubectl apply -f ejemplos/04-estrategias/06-service-canary.yaml

# 2. Verificar 100% trÃ¡fico a v1
kubectl get endpoints webapp-service
# (10 Pods de stable)

# 3. Desplegar Canary (v2) con 10% trÃ¡fico
kubectl apply -f ejemplos/04-estrategias/05-canary-deployment.yaml

# 4. Reducir Stable a 9 Pods (mantener total: 10)
kubectl scale deployment webapp-stable --replicas=9

# 5. Verificar distribuciÃ³n 90/10
kubectl get endpoints webapp-service
# (9 Pods stable + 1 Pod canary)

# 6. Monitorear mÃ©tricas de Canary
# (Errores, latencia, trÃ¡fico, etc.)

# 7a. Si Canary OK â†’ Aumentar a 50%
kubectl scale deployment webapp-stable --replicas=5
kubectl scale deployment webapp-canary --replicas=5

# 7b. Si Canary OK â†’ Promover a 100%
kubectl scale deployment webapp-canary --replicas=10
kubectl delete deployment webapp-stable

# 8. Rollback si falla
kubectl delete deployment webapp-canary
kubectl scale deployment webapp-stable --replicas=10
```

**Ventajas**:
- âœ… **Riesgo reducido** (solo 10% usuarios afectados)
- âœ… **Testing en producciÃ³n** con trÃ¡fico real
- âœ… **Rollback rÃ¡pido** (delete canary)

**Desventajas**:
- âŒ **Complejidad**: Gestionar 2 Deployments + mÃ©tricas
- âŒ **Requiere balanceo manual** (scaling)

---

### **5.5 Progressive Delivery con Flagger (Avanzado)**

**Flagger** = Herramienta para automatizar Canary deployments con mÃ©tricas.

```yaml
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: webapp
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  service:
    port: 80
  analysis:
    interval: 1m
    threshold: 5
    maxWeight: 50
    stepWeight: 10
    metrics:
    - name: request-success-rate
      thresholdRange:
        min: 99
    - name: request-duration
      thresholdRange:
        max: 500
```

**Comportamiento**:
1. Detecta cambio en Deployment
2. Crea Canary con 10% trÃ¡fico
3. Monitorea mÃ©tricas cada 1 minuto
4. Si mÃ©tricas OK â†’ Aumenta 10% (stepWeight)
5. Si mÃ©tricas fallan â†’ Rollback automÃ¡tico
6. Repite hasta 50% (maxWeight)

**âš ï¸ Requiere**: Service mesh (Istio, Linkerd) o Ingress Controller (NGINX, Traefik).

---

### **âœ… Checkpoint 05: Estrategias Avanzadas**

Antes de continuar, asegÃºrate de poder:

- [ ] Comparar RollingUpdate vs Recreate (downtime, velocidad, recursos)
- [ ] Explicar cuÃ¡ndo usar Recreate (bases de datos, incompatibilidad)
- [ ] Describir Blue-Green deployment (2 entornos, switch instantÃ¡neo)
- [ ] Implementar Blue-Green con 2 Deployments + Service selector
- [ ] Describir Canary deployment (porcentaje gradual)
- [ ] Implementar Canary con scaling manual de replicas
- [ ] Calcular porcentajes: 1 canary + 9 stable = 10% canary
- [ ] Explicar ventajas/desventajas de cada estrategia

ğŸ“ **Laboratorio**: [`laboratorios/lab-05-estrategias-avanzadas.md`](./laboratorios/lab-05-estrategias-avanzadas.md)
- DuraciÃ³n: 60 minutos
- Implementa Blue-Green deployment
- Practica Canary con diferentes porcentajes (10%, 50%, 100%)
- Simula rollback de Canary
- Compara tiempos y recursos de cada estrategia

---

## âœ¨ 6. Best Practices para Deployments

### **6.1 Naming Conventions**

```yaml
metadata:
  name: webapp-frontend-prod  # â† Descriptivo: app-component-environment
  labels:
    app: webapp              # â† Nombre aplicaciÃ³n
    component: frontend      # â† Componente especÃ­fico
    tier: web                # â† Capa arquitectÃ³nica
    environment: production  # â† Ambiente
    version: "v2.1.0"        # â† VersiÃ³n semÃ¡ntica
    managed-by: helm         # â† Herramienta de gestiÃ³n (opcional)
```

**Convenciones recomendadas**:
- **Lowercase**: Siempre minÃºsculas (obligatorio en Kubernetes)
- **Separadores**: Usar `-` (no `_`)
- **MÃ¡ximo 63 caracteres**
- **Labels estÃ¡ndar**: `app`, `component`, `version`, `environment`

---

### **6.2 Resources: Requests y Limits**

**âš ï¸ SIEMPRE define resources en producciÃ³n**:

```yaml
spec:
  template:
    spec:
      containers:
      - name: nginx
        resources:
          requests:          # â† Reserva garantizada
            memory: "256Mi"
            cpu: "500m"      # â† 0.5 CPU
          limits:            # â† MÃ¡ximo permitido
            memory: "512Mi"
            cpu: "1000m"     # â† 1 CPU
```

**CÃ¡lculo de requests**:
1. **Medir consumo real** en staging/producciÃ³n
2. **Requests** = Promedio + 20% margen
3. **Limits** = Requests Ã— 2 (permite picos)

**Ejemplo**:
- Promedio: 200Mi RAM, 300m CPU
- Requests: 250Mi RAM, 400m CPU (20% margen)
- Limits: 500Mi RAM, 800m CPU (2x)

**âš ï¸ Consecuencias sin resources**:
- Sin requests â†’ Pods compiten por recursos (problemas de rendimiento)
- Sin limits â†’ Un Pod puede consumir todos los recursos del nodo (OOMKilled)

---

### **6.3 Health Checks: Liveness y Readiness**

```yaml
spec:
  template:
    spec:
      containers:
      - name: webapp
        image: webapp:v2
        ports:
        - containerPort: 8080
        
        # Â¿EstÃ¡ VIVO el proceso?
        livenessProbe:
          httpGet:
            path: /healthz      # â† Endpoint simple (debe responder rÃ¡pido)
            port: 8080
          initialDelaySeconds: 30  # â† Espera inicial (startup time)
          periodSeconds: 10        # â† Cada 10s
          timeoutSeconds: 5        # â† Timeout por request
          failureThreshold: 3      # â† Reinicia despuÃ©s de 3 fallos
        
        # Â¿EstÃ¡ LISTO para recibir trÃ¡fico?
        readinessProbe:
          httpGet:
            path: /ready        # â† Endpoint mÃ¡s complejo (DB, cache, etc.)
            port: 8080
          initialDelaySeconds: 5   # â† MÃ¡s corto que liveness
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3      # â† Saca del Service despuÃ©s de 3 fallos
```

**Diferencias clave**:

| Aspecto | Liveness | Readiness |
|---------|----------|-----------|
| **PropÃ³sito** | Detectar procesos muertos/bloqueados | Detectar si estÃ¡ listo para trÃ¡fico |
| **AcciÃ³n** | **Reinicia** el Pod | **Saca** del Service (sin reiniciar) |
| **Endpoint** | Simple (`/healthz` â†’ 200 OK) | Complejo (`/ready` â†’ verifica DB, cache) |
| **initialDelaySeconds** | MÃ¡s largo (30-60s) | MÃ¡s corto (5-10s) |
| **CuÃ¡ndo falla** | Proceso bloqueado, deadlock | DB desconectada, cache lleno |

**Ejemplo endpoints**:

```go
// Liveness: Solo verifica que el servidor responde
func healthz(w http.ResponseWriter, r *http.Request) {
    w.WriteHeader(http.StatusOK)
    w.Write([]byte("OK"))
}

// Readiness: Verifica dependencias
func ready(w http.ResponseWriter, r *http.Request) {
    if dbConnected() && cacheAvailable() {
        w.WriteHeader(http.StatusOK)
        w.Write([]byte("Ready"))
    } else {
        w.WriteHeader(http.StatusServiceUnavailable)
        w.Write([]byte("Not Ready"))
    }
}
```

---

### **6.4 Security Contexts**

```yaml
spec:
  template:
    spec:
      securityContext:
        runAsNonRoot: true           # â† NO ejecutar como root
        runAsUser: 1000              # â† UID especÃ­fico
        fsGroup: 2000                # â† GID para volumes
        seccompProfile:
          type: RuntimeDefault       # â† Seccomp profile
      
      containers:
      - name: nginx
        image: nginx:alpine
        securityContext:
          allowPrivilegeEscalation: false  # â† No permitir escalada
          capabilities:
            drop:
            - ALL                    # â† Eliminar todas las capabilities
            add:
            - NET_BIND_SERVICE       # â† Solo agregar las necesarias
          readOnlyRootFilesystem: true     # â† Filesystem de solo lectura
        
        volumeMounts:
        - name: tmp
          mountPath: /tmp            # â† Directorio writable
        - name: cache
          mountPath: /var/cache/nginx
      
      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir: {}
```

**Principios de seguridad**:
- âœ… **No root**: Ejecutar con usuario no privilegiado
- âœ… **Read-only filesystem**: Previene modificaciÃ³n de binarios
- âœ… **Drop capabilities**: Eliminar permisos innecesarios
- âœ… **Seccomp profile**: Filtrar syscalls peligrosas

---

### **6.5 Anti-Patterns: QuÃ© NO Hacer**

#### **âŒ Anti-Pattern 1: Omitir readinessProbe**

```yaml
# âŒ MAL: Sin readiness probe
spec:
  template:
    spec:
      containers:
      - name: webapp
        image: webapp:v2
# Problema: Pod recibe trÃ¡fico ANTES de estar listo (errores 500)
```

```yaml
# âœ… BIEN: Con readiness probe
spec:
  template:
    spec:
      containers:
      - name: webapp
        image: webapp:v2
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
```

---

#### **âŒ Anti-Pattern 2: maxUnavailable alto sin testing**

```yaml
# âŒ MAL: Permite 50% downtime
spec:
  strategy:
    rollingUpdate:
      maxUnavailable: 50%
# Problema: Si hay error, 50% de Pods caen simultÃ¡neamente
```

```yaml
# âœ… BIEN: Zero downtime
spec:
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # â† Siempre mÃ­nimo replicas disponibles
```

---

#### **âŒ Anti-Pattern 3: No usar change-cause**

```yaml
# âŒ MAL: Sin annotation
metadata:
  name: webapp
# Problema: No sabes quÃ© cambiÃ³ en cada revisiÃ³n
```

```yaml
# âœ… BIEN: Con change-cause
metadata:
  name: webapp
  annotations:
    kubernetes.io/change-cause: "v2.1.0: Actualizar nginx + agregar health checks"
```

---

#### **âŒ Anti-Pattern 4: Usar :latest**

```yaml
# âŒ MAL: Tag latest (no determinÃ­stico)
spec:
  template:
    spec:
      containers:
      - name: webapp
        image: webapp:latest  # â† Â¿QuÃ© versiÃ³n es?
```

```yaml
# âœ… BIEN: Tag especÃ­fico (semantic versioning)
spec:
  template:
    spec:
      containers:
      - name: webapp
        image: webapp:v2.1.0  # â† VersiÃ³n exacta
```

---

#### **âŒ Anti-Pattern 5: replicas: 1 en producciÃ³n**

```yaml
# âŒ MAL: Single replica
spec:
  replicas: 1  # â† Single point of failure
```

```yaml
# âœ… BIEN: MÃºltiples replicas + anti-affinity
spec:
  replicas: 3
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - webapp
              topologyKey: kubernetes.io/hostname
```

---

### **6.6 Production-Ready Deployment Template**

ğŸ“„ **Archivo**: [`ejemplos/05-best-practices/production-template.yaml`](./ejemplos/05-best-practices/production-template.yaml)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-frontend-prod
  labels:
    app: webapp
    component: frontend
    tier: web
    environment: production
    version: "v2.1.0"
  annotations:
    kubernetes.io/change-cause: "v2.1.0: Production deployment with security hardening"
spec:
  replicas: 5
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
  
  selector:
    matchLabels:
      app: webapp
      component: frontend
      environment: production
  
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 0  # â† Zero downtime
  
  template:
    metadata:
      labels:
        app: webapp
        component: frontend
        tier: web
        environment: production
        version: "v2.1.0"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    
    spec:
      # Security context
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault
      
      # Anti-affinity (distribuir en nodos diferentes)
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - webapp
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: webapp
        image: webapp:v2.1.0  # â† Tag especÃ­fico (NO latest)
        imagePullPolicy: IfNotPresent
        
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        
        # Resources
        resources:
          requests:
            memory: "256Mi"
            cpu: "500m"
          limits:
            memory: "512Mi"
            cpu: "1000m"
        
        # Health checks
        livenessProbe:
          httpGet:
            path: /healthz
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        
        # Security context
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE
          readOnlyRootFilesystem: true
        
        # Environment variables
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: LOG_LEVEL
          value: "info"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        
        # Volumes
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /var/cache
        - name: config
          mountPath: /etc/config
          readOnly: true
      
      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir: {}
      - name: config
        configMap:
          name: webapp-config
```

**CaracterÃ­sticas production-ready**:
- âœ… **5 replicas** + anti-affinity (alta disponibilidad)
- âœ… **Zero downtime** (maxUnavailable: 0)
- âœ… **Resources** definidos (requests + limits)
- âœ… **Health checks** (liveness + readiness)
- âœ… **Security hardening** (runAsNonRoot, readOnlyRootFilesystem, drop capabilities)
- âœ… **Semantic versioning** (v2.1.0)
- âœ… **Change cause** para auditorÃ­a
- âœ… **Prometheus annotations** para monitoreo
- âœ… **ConfigMap** para configuraciÃ³n externa

---

### **âœ… Checkpoint 06: Best Practices**

Antes de continuar, asegÃºrate de poder:

- [ ] Aplicar naming conventions (app-component-environment)
- [ ] Definir resources (requests + limits) con cÃ¡lculos apropiados
- [ ] Configurar liveness y readiness probes correctamente
- [ ] Explicar diferencia entre liveness y readiness
- [ ] Implementar security contexts (runAsNonRoot, readOnlyRootFilesystem, capabilities)
- [ ] Identificar 5 anti-patterns comunes
- [ ] Usar semantic versioning (NO :latest)
- [ ] Configurar zero downtime (maxUnavailable: 0)
- [ ] Agregar change-cause annotations

ğŸ“ **Laboratorio**: [`laboratorios/lab-06-best-practices.md`](./laboratorios/lab-06-best-practices.md)
- DuraciÃ³n: 50 minutos
- Transforma un Deployment bÃ¡sico a production-ready
- Implementa todos los best practices
- Testing de health checks (simula fallos)
- Valida security contexts

---

## ğŸ“Š 7. Monitoreo y Troubleshooting

### **7.1 Comandos de DiagnÃ³stico RÃ¡pido**

```bash
# Ver estado general
kubectl get deployment webapp -o wide

# Ver condiciones del Deployment
kubectl get deployment webapp -o jsonpath='{.status.conditions[*]}'

# Ver eventos recientes (Ãºltimos 10 minutos)
kubectl get events --field-selector involvedObject.kind=Deployment,involvedObject.name=webapp --sort-by='.metadata.creationTimestamp'

# Ver Pods con problemas
kubectl get pods -l app=webapp --field-selector status.phase!=Running

# Describir Deployment completo
kubectl describe deployment webapp

# Ver logs de todos los Pods
kubectl logs -l app=webapp --tail=50 --prefix=true

# Ver recursos consumidos (requiere Metrics Server)
kubectl top pods -l app=webapp
```

---

### **7.2 Debugging Common Issues**

#### **Issue 1: ImagePullBackOff**

```bash
# SÃ­ntoma
kubectl get pods -l app=webapp
# NAME                     READY   STATUS             RESTARTS   AGE
# webapp-7c8d9e0f-abc      0/1     ImagePullBackOff   0          2m

# DiagnÃ³stico
kubectl describe pod webapp-7c8d9e0f-abc | grep -A 10 "Events"
# Events:
#   Type     Reason     Message
#   ----     ------     -------
#   Normal   Pulling    Pulling image "webapp:v2.1.0-WRONG"
#   Warning  Failed     Failed to pull image "webapp:v2.1.0-WRONG": rpc error: ...
#   Warning  Failed     Error: ErrImagePull

# SoluciÃ³n
# 1. Verificar nombre/tag de imagen
kubectl get deployment webapp -o jsonpath='{.spec.template.spec.containers[0].image}'
# 2. Corregir imagen
kubectl set image deployment/webapp webapp=webapp:v2.1.0  # Tag correcto
```

---

#### **Issue 2: CrashLoopBackOff**

```bash
# SÃ­ntoma
kubectl get pods -l app=webapp
# NAME                     READY   STATUS             RESTARTS   AGE
# webapp-7c8d9e0f-abc      0/1     CrashLoopBackOff   5          5m

# DiagnÃ³stico
# Ver logs del contenedor
kubectl logs webapp-7c8d9e0f-abc
# Error: Cannot connect to database at db:5432

# Ver logs del contenedor anterior (si reiniciÃ³)
kubectl logs webapp-7c8d9e0f-abc --previous

# Ver eventos
kubectl describe pod webapp-7c8d9e0f-abc | grep -A 10 "Events"

# SoluciÃ³n
# 1. Verificar variables de entorno
kubectl get deployment webapp -o jsonpath='{.spec.template.spec.containers[0].env[*]}'
# 2. Verificar dependencias (DB, cache, etc.)
# 3. Revisar health checks (Â¿demasiado agresivos?)
```

---

#### **Issue 3: Pods Pending (No Schedule)**

```bash
# SÃ­ntoma
kubectl get pods -l app=webapp
# NAME                     READY   STATUS    RESTARTS   AGE
# webapp-7c8d9e0f-abc      0/1     Pending   0          5m

# DiagnÃ³stico
kubectl describe pod webapp-7c8d9e0f-abc | grep -A 10 "Events"
# Events:
#   Type     Reason            Message
#   ----     ------            -------
#   Warning  FailedScheduling  0/1 nodes are available: 1 Insufficient cpu.

# SoluciÃ³n
# Caso 1: Resources insuficientes
kubectl top nodes  # Ver recursos disponibles
# Reducir requests o agregar nodos

# Caso 2: Node selector no match
kubectl get nodes --show-labels
kubectl get deployment webapp -o jsonpath='{.spec.template.spec.nodeSelector}'
```

---

#### **Issue 4: Readiness Probe Failing**

```bash
# SÃ­ntoma
kubectl get pods -l app=webapp
# NAME                     READY   STATUS    RESTARTS   AGE
# webapp-7c8d9e0f-abc      0/1     Running   0          2m

# DiagnÃ³stico
kubectl describe pod webapp-7c8d9e0f-abc | grep -A 10 "Readiness"
# Readiness probe failed: Get "http://10.244.0.5:8080/ready": dial tcp 10.244.0.5:8080: connect: connection refused

# Ver logs
kubectl logs webapp-7c8d9e0f-abc

# SoluciÃ³n
# 1. Verificar endpoint de readiness
kubectl exec webapp-7c8d9e0f-abc -- curl -v localhost:8080/ready
# 2. Aumentar initialDelaySeconds
# 3. Revisar lÃ³gica de /ready (Â¿falla dependencia?)
```

---

### **7.3 MÃ©tricas Clave a Monitorear**

**Con Prometheus + Grafana**:

```promql
# Disponibilidad de Pods
sum(kube_deployment_status_replicas_available{deployment="webapp"}) 
  / 
sum(kube_deployment_spec_replicas{deployment="webapp"}) * 100

# Tasa de reintentos (rolling update fallido)
rate(kube_pod_container_status_restarts_total{namespace="default", pod=~"webapp-.*"}[5m])

# Latencia de rolling update (tiempo desde start hasta available)
histogram_quantile(0.99, 
  rate(kube_deployment_status_condition_progressing_duration_seconds_bucket[5m])
)

# Pods no listos (readiness probe failing)
sum(kube_pod_status_ready{condition="false", namespace="default", pod=~"webapp-.*"})
```

**Alertas recomendadas**:
- âœ… **Deployment not available**: `replicas_available < replicas_desired` por > 5 minutos
- âœ… **High restart rate**: > 5 restarts en 5 minutos
- âœ… **Rollout stuck**: Progressing=False por > 10 minutos
- âœ… **Pod not ready**: > 20% Pods con readiness=false

---

### **âœ… Checkpoint 07: Monitoreo y Troubleshooting**

Antes de continuar, asegÃºrate de poder:

- [ ] Usar comandos de diagnÃ³stico rÃ¡pido (get, describe, events, logs)
- [ ] Diagnosticar ImagePullBackOff (imagen incorrecta)
- [ ] Diagnosticar CrashLoopBackOff (logs, previous logs)
- [ ] Diagnosticar Pods Pending (resources, node selector)
- [ ] Diagnosticar readiness probe failing
- [ ] Identificar mÃ©tricas clave de disponibilidad
- [ ] Configurar alertas bÃ¡sicas (Deployment not available)

ğŸ“ **Laboratorio**: [`laboratorios/lab-07-troubleshooting.md`](./laboratorios/lab-07-troubleshooting.md)
- DuraciÃ³n: 45 minutos
- Simula 5 problemas comunes y resuÃ©lvelos
- Practica debugging con kubectl logs/describe/events
- Configura alertas bÃ¡sicas

---

## ğŸ¯ Resumen del MÃ³dulo

### **Conceptos Clave Aprendidos**

1. **Deployments** = Controlador que gestiona ReplicaSets y rolling updates automÃ¡ticos
2. **Rolling Update** = ActualizaciÃ³n gradual (v1 â†’ v2) sin downtime
3. **maxSurge** = Pods extras permitidos durante update
4. **maxUnavailable** = Pods no disponibles permitidos (0 = zero downtime)
5. **Rollback** = Volver a versiÃ³n anterior con `kubectl rollout undo`
6. **Estrategias avanzadas**: Blue-Green (switch instantÃ¡neo), Canary (% gradual)
7. **Best practices**: Resources, health checks, security contexts, semantic versioning

---

### **Comandos Esenciales**

```bash
# GestiÃ³n bÃ¡sica
kubectl apply -f deployment.yaml
kubectl get deployments
kubectl describe deployment webapp
kubectl delete deployment webapp

# Rolling updates
kubectl set image deployment/webapp nginx=nginx:1.21
kubectl rollout status deployment/webapp
kubectl rollout pause deployment/webapp
kubectl rollout resume deployment/webapp

# Rollback
kubectl rollout history deployment/webapp
kubectl rollout undo deployment/webapp
kubectl rollout undo deployment/webapp --to-revision=2

# Escalado
kubectl scale deployment webapp --replicas=10

# Troubleshooting
kubectl get pods -l app=webapp
kubectl logs <pod-name>
kubectl describe pod <pod-name>
kubectl get events --field-selector involvedObject.kind=Deployment
```

---

### **Flujo de Trabajo Completo**

```
1. DiseÃ±o
   â†“
2. Crear manifiesto YAML (con best practices)
   â†“
3. Aplicar: kubectl apply -f deployment.yaml
   â†“
4. Verificar: kubectl get deploy, kubectl rollout status
   â†“
5. Monitorear: Prometheus/Grafana, kubectl top pods
   â†“
6. Actualizar: kubectl set image o editar YAML
   â†“
7. Rolling update automÃ¡tico (gradual)
   â†“
8a. Si OK â†’ Continuar
8b. Si falla â†’ Rollback: kubectl rollout undo
   â†“
9. Iterar (CI/CD pipeline)
```

---

### **Decisiones Clave**

| DecisiÃ³n | OpciÃ³n A | OpciÃ³n B | CuÃ¡ndo usar A | CuÃ¡ndo usar B |
|----------|----------|----------|---------------|---------------|
| **Estrategia** | RollingUpdate | Recreate | Apps stateless | Apps stateful, incompatibilidad |
| **maxUnavailable** | 0 | > 0 | ProducciÃ³n (zero downtime) | Dev/Staging (velocidad) |
| **maxSurge** | Alto (2-5) | Bajo (1) | Recursos abundantes | Recursos limitados |
| **Deployment avanzado** | Blue-Green | Canary | Rollback instantÃ¡neo | Testing gradual |
| **revisionHistoryLimit** | 10 | 3 | Muchas opciones de rollback | Conservar recursos |

---

### **Checklist Production-Ready**

- [ ] **Replicas**: â‰¥ 3 para alta disponibilidad
- [ ] **Resources**: requests + limits definidos
- [ ] **Health checks**: liveness + readiness probes
- [ ] **Strategy**: maxUnavailable: 0 (zero downtime)
- [ ] **Security**: runAsNonRoot, readOnlyRootFilesystem, drop capabilities
- [ ] **Versioning**: Semantic versioning (NO :latest)
- [ ] **Annotations**: kubernetes.io/change-cause
- [ ] **Anti-affinity**: Distribuir Pods en nodos diferentes
- [ ] **Monitoring**: MÃ©tricas + alertas configuradas
- [ ] **Rollback**: revisionHistoryLimit > 0

---

### **PrÃ³ximos Pasos**

ğŸ“ **Has completado el MÃ³dulo 07: Deployments y Rolling Updates**

**Siguientes mÃ³dulos**:
- **MÃ³dulo 08**: [Services y Endpoints](../modulo-08-services-endpoints/README.md) â†’ Exponer Deployments
- **MÃ³dulo 09**: [Ingress](../modulo-09-ingress-external-access/README.md) â†’ Acceso externo HTTP/HTTPS
- **MÃ³dulo 10**: [Namespaces](../modulo-10-namespaces-organizacion/README.md) â†’ OrganizaciÃ³n multi-tenant

**Recursos adicionales**:
- ğŸ“– [DocumentaciÃ³n oficial de Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
- ğŸ“– [Best practices de Google](https://cloud.google.com/architecture/best-practices-for-operating-containers)
- ğŸ“– [Flagger (Progressive Delivery)](https://flagger.app/)
- ğŸ¥ [KubeCon talks sobre Deployments](https://www.youtube.com/kubecon)

---

### **âœ… Checkpoint Final**

AutoevaluaciÃ³n completa del mÃ³dulo:

**Conceptos (SecciÃ³n 1)**:
- [ ] Explicar el problema que resuelven los Deployments
- [ ] Describir la arquitectura: Deployment â†’ ReplicaSet â†’ Pods
- [ ] Comparar Deployment vs ReplicaSet (cuÃ¡ndo usar cada uno)

**GestiÃ³n (SecciÃ³n 2)**:
- [ ] Crear Deployment desde manifiesto YAML
- [ ] Usar kubectl para gestionar Deployments (get, describe, scale, delete)
- [ ] Inspeccionar ReplicaSets y Pods gestionados

**Rolling Updates (SecciÃ³n 3)**:
- [ ] Explicar flujo de rolling update (crear RS v2, escalar gradualmente)
- [ ] Configurar maxSurge y maxUnavailable apropiadamente
- [ ] Observar rolling update en tiempo real con --watch

**Rollback (SecciÃ³n 4)**:
- [ ] Ver historial de revisiones
- [ ] Hacer rollback a versiÃ³n anterior o especÃ­fica
- [ ] Pausar/reanudar rolling updates
- [ ] Troubleshoot rollouts bloqueados

**Estrategias (SecciÃ³n 5)**:
- [ ] Implementar Blue-Green deployment
- [ ] Implementar Canary deployment
- [ ] Elegir estrategia apropiada segÃºn caso de uso

**Best Practices (SecciÃ³n 6)**:
- [ ] Aplicar naming conventions
- [ ] Definir resources apropiadamente
- [ ] Configurar health checks (liveness + readiness)
- [ ] Implementar security contexts
- [ ] Evitar anti-patterns comunes

**Troubleshooting (SecciÃ³n 7)**:
- [ ] Diagnosticar problemas comunes (ImagePullBackOff, CrashLoopBackOff, Pending)
- [ ] Usar comandos de debugging efectivamente
- [ ] Configurar monitoreo y alertas

---

## ğŸ“š Recursos del MÃ³dulo

### **Ejemplos Disponibles**

```
ejemplos/
â”œâ”€â”€ 01-basico/
â”‚   â”œâ”€â”€ 01-deployment-simple.yaml          # Deployment bÃ¡sico
â”‚   â””â”€â”€ 02-deployment-production.yaml      # Production-ready
â”œâ”€â”€ 02-rolling-updates/
â”‚   â”œâ”€â”€ 01-rolling-update-demo.yaml        # Demo de rolling update
â”‚   â””â”€â”€ 02-max-surge-unavailable.yaml      # ConfiguraciÃ³n maxSurge/maxUnavailable
â”œâ”€â”€ 03-rollback/
â”‚   â”œâ”€â”€ 01-rollback-demo.yaml              # Demo de rollback
â”‚   â””â”€â”€ 02-pause-resume.yaml               # Pause/resume
â”œâ”€â”€ 04-estrategias/
â”‚   â”œâ”€â”€ 01-blue-deployment.yaml            # Blue-Green: Blue
â”‚   â”œâ”€â”€ 02-green-deployment.yaml           # Blue-Green: Green
â”‚   â”œâ”€â”€ 03-service.yaml                    # Blue-Green: Service
â”‚   â”œâ”€â”€ 04-stable-deployment.yaml          # Canary: Stable
â”‚   â”œâ”€â”€ 05-canary-deployment.yaml          # Canary: Canary
â”‚   â””â”€â”€ 06-service-canary.yaml             # Canary: Service
â””â”€â”€ 05-best-practices/
    â””â”€â”€ production-template.yaml           # Template completo
```

### **Laboratorios Disponibles**

```
laboratorios/
â”œâ”€â”€ lab-01-introduccion-deployments.md     # 30 min
â”œâ”€â”€ lab-02-gestion-deployments.md          # 35 min
â”œâ”€â”€ lab-03-rolling-updates.md              # 45 min
â”œâ”€â”€ lab-04-rollback-versiones.md           # 40 min
â”œâ”€â”€ lab-05-estrategias-avanzadas.md        # 60 min
â”œâ”€â”€ lab-06-best-practices.md               # 50 min
â”œâ”€â”€ lab-07-troubleshooting.md              # 45 min
â””â”€â”€ lab-08-proyecto-integrador.md          # 90 min (FINAL)
```

**Tiempo total de laboratorios**: ~6 horas prÃ¡cticas

---

## ğŸ“ CertificaciÃ³n de Conocimientos

**Has completado exitosamente el MÃ³dulo 07** si puedes:

1. âœ… Crear y gestionar Deployments con kubectl
2. âœ… Configurar rolling updates con zero downtime (maxUnavailable: 0)
3. âœ… Hacer rollback a versiones anteriores
4. âœ… Implementar estrategias avanzadas (Blue-Green, Canary)
5. âœ… Aplicar best practices de producciÃ³n
6. âœ… Troubleshoot problemas comunes de Deployments
7. âœ… Configurar health checks y security contexts
8. âœ… DiseÃ±ar Deployments production-ready siguiendo el template

**Tiempo de dominio estimado**: 4-5 horas de estudio + 6 horas de labs = **10-11 horas totales**

---

### **ğŸ“– ContinÃºa tu aprendizaje**

â¡ï¸ **Siguiente mÃ³dulo**: [MÃ³dulo 08 - Services y Endpoints](../modulo-08-services-endpoints/README.md)

ğŸ’¬ **Â¿Dudas o feedback?**: Consulta con tu instructor o en los canales de Slack del curso.

ğŸ‰ **Â¡Felicitaciones por completar este mÃ³dulo!**

---

**Ãšltima actualizaciÃ³n**: 2024  
**VersiÃ³n del documento**: 2.0  
**Autor**: Curso Kubernetes Completo
